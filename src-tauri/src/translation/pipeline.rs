use futures_util::StreamExt;
use serde_json::json;
/// ç¿»è¯‘ç®¡çº¿ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
use std::sync::Arc;

use crate::database::Database;
use crate::llm_manager::{ApiConfig, LLMManager};
use crate::models::AppError;
use crate::providers::ProviderAdapter;
// â˜… VFS ç»Ÿä¸€å­˜å‚¨ï¼ˆ2025-12-07ï¼‰
use crate::vfs::database::VfsDatabase;

use super::events::TranslationEventEmitter;
use super::types::{TranslationRequest, TranslationResponse};

/// ç¿»è¯‘ç®¡çº¿ä¾èµ–
pub struct TranslationDeps {
    pub llm: Arc<LLMManager>,
    pub db: Arc<Database>, // ä¸»æ•°æ®åº“ï¼ˆé…ç½®/è®¾ç½®è¯»å–ï¼‰
    pub emitter: TranslationEventEmitter,
    pub vfs_db: Arc<VfsDatabase>, // â˜… VFS æ•°æ®åº“ï¼ˆå¿…éœ€ï¼Œå”¯ä¸€å­˜å‚¨ï¼‰
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum StreamStatus {
    Completed,
    Cancelled,
}

/// è¿è¡Œç¿»è¯‘ç®¡çº¿
pub async fn run_translation(
    request: TranslationRequest,
    deps: TranslationDeps,
) -> Result<Option<TranslationResponse>, AppError> {
    // 0. è¾“å…¥éªŒè¯ï¼šæ£€æŸ¥ç©ºæ–‡æœ¬
    if request.text.trim().is_empty() {
        return Err(AppError::validation("ç¿»è¯‘æ–‡æœ¬ä¸èƒ½ä¸ºç©º".to_string()));
    }

    // 0.1 è¾“å…¥éªŒè¯ï¼šæ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆé˜²æ­¢è¶…å¤§æ–‡æœ¬å¯¼è‡´ API è¶…æ—¶æˆ– OOMï¼‰
    const MAX_TEXT_CHARS: usize = 100_000; // 100K å­—ç¬¦ä¸Šé™
    let text_char_count = request.text.chars().count();
    if text_char_count > MAX_TEXT_CHARS {
        return Err(AppError::validation(format!(
            "ç¿»è¯‘æ–‡æœ¬è¿‡é•¿ï¼ˆå½“å‰ {} å­—ç¬¦ï¼Œæœ€å¤§ {} å­—ç¬¦ï¼‰",
            text_char_count, MAX_TEXT_CHARS
        )));
    }

    // 1. æ„é€ ç¿»è¯‘ Prompt
    let (system_prompt, user_prompt) = build_translation_prompts(&request)?;

    // 2. è·å–ç¿»è¯‘æ¨¡å‹é…ç½®å¹¶è§£å¯† API Key
    let config = deps.llm.get_translation_model_config().await?;
    let api_key = deps.llm.decrypt_api_key(&config.api_key)?;

    // 3. æµå¼è°ƒç”¨ LLM
    let mut accumulated = String::new();
    let stream_event = format!("translation_stream_{}", request.session_id);

    let stream_status = stream_translate(
        &config,
        &api_key,
        &system_prompt,
        &user_prompt,
        &stream_event,
        deps.llm.clone(),
        |chunk| {
            accumulated.push_str(&chunk);
            deps.emitter
                .emit_data(&request.session_id, chunk, accumulated.clone());
        },
    )
    .await?;

    if matches!(stream_status, StreamStatus::Cancelled) {
        deps.emitter.emit_cancelled(&request.session_id);
        return Ok(None);
    }

    // ğŸ”§ P0-06 ä¿®å¤ï¼šç§»é™¤åç«¯çš„ VFS è®°å½•åˆ›å»ºï¼Œç”±å‰ç«¯ç»Ÿä¸€ç®¡ç†
    // åŸå› ï¼šå‰ç«¯é€šè¿‡ Learning Hub åˆ›å»ºç©ºç¿»è¯‘æ–‡ä»¶åï¼Œåç«¯å†åˆ›å»ºä¼šå¯¼è‡´åŒå†™ï¼ˆå­¤å„¿è®°å½•ï¼‰
    // ç°åœ¨åªè¿”å›ç¿»è¯‘ç»“æœï¼Œå‰ç«¯é€šè¿‡ DSTU adapter çš„ updateTranslation æ›´æ–°è®°å½•
    let now = chrono::Utc::now().to_rfc3339();

    println!("âœ… [Translation] ç¿»è¯‘å®Œæˆï¼Œç”±å‰ç«¯ç®¡ç†å­˜å‚¨");

    // 5. å‘é€å®Œæˆäº‹ä»¶ï¼ˆä¸å†åˆ›å»ºæ–°è®°å½•ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼‰
    deps.emitter.emit_complete(
        &request.session_id,
        request.session_id.clone(), // ä½¿ç”¨ session_id ä½œä¸ºä¸´æ—¶ IDï¼Œå‰ç«¯ä¼šç”¨å®é™… node ID
        accumulated.clone(),
        now.clone(),
    );

    Ok(Some(TranslationResponse {
        id: request.session_id.clone(), // ä½¿ç”¨ session_idï¼Œå‰ç«¯ä¼šå¿½ç•¥æ­¤å€¼
        translated_text: accumulated,
        created_at: now,
        session_id: request.session_id,
    }))
}

/// è¯­è¨€ code â†’ å…¨åæ˜ å°„ï¼Œç¡®ä¿ LLM ç²¾ç¡®ç†è§£ç›®æ ‡è¯­è¨€
fn lang_full_name(code: &str) -> &str {
    match code {
        "zh-CN" | "zh" => "Simplified Chinese (ç®€ä½“ä¸­æ–‡)",
        "zh-TW" => "Traditional Chinese (ç¹é«”ä¸­æ–‡)",
        "en" => "English",
        "ja" => "Japanese (æ—¥æœ¬èª)",
        "ko" => "Korean (í•œêµ­ì–´)",
        "fr" => "French (franÃ§ais)",
        "de" => "German (Deutsch)",
        "es" => "Spanish (espaÃ±ol)",
        "ru" => "Russian (Ñ€ÑƒÑÑĞºĞ¸Ğ¹)",
        "ar" => "Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)",
        "pt" => "Portuguese (portuguÃªs)",
        "pt-BR" => "Brazilian Portuguese (portuguÃªs brasileiro)",
        "it" => "Italian (italiano)",
        "vi" => "Vietnamese (tiáº¿ng Viá»‡t)",
        "th" => "Thai (à¹„à¸—à¸¢)",
        "hi" => "Hindi (à¤¹à¤¿à¤¨à¥à¤¦à¥€)",
        "tr" => "Turkish (TÃ¼rkÃ§e)",
        "pl" => "Polish (polski)",
        "nl" => "Dutch (Nederlands)",
        "sv" => "Swedish (svenska)",
        "la" => "Latin (Latina)",
        "el" => "Greek (Î•Î»Î»Î·Î½Î¹ÎºÎ¬)",
        "uk" => "Ukrainian (ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°)",
        "id" => "Indonesian (Bahasa Indonesia)",
        "ms" => "Malay (Bahasa Melayu)",
        "auto" => "auto-detected language",
        other => other,
    }
}

/// é¢†åŸŸé¢„è®¾ prompt æ¨¡æ¿
fn domain_system_prompt(domain: &str) -> &str {
    match domain {
        "academic" => 
            "You are an expert academic translator specializing in scholarly papers, theses, and research articles. \
             Translate with precision, maintaining academic register and discipline-specific terminology. \
             Preserve citation formats (e.g. [1], (Author, Year)), mathematical notation, and abbreviations. \
             Ensure terminological consistency throughout. Only output the translated text.",
        "technical" => 
            "You are a professional technical translator specializing in software documentation, engineering, and IT content. \
             Keep code snippets, variable names, command-line examples, and API references untranslated. \
             Preserve markdown/HTML formatting. Translate technical terms accurately using industry-standard vocabulary. \
             Only output the translated text.",
        "literary" => 
            "You are a literary translator with expertise in creative writing. \
             Prioritize natural fluency and emotional resonance over literal accuracy. \
             Preserve rhetorical devices, metaphors, rhythm, and the author's unique voice. \
             Adapt cultural references when necessary for the target audience. Only output the translated text.",
        "legal" => 
            "You are a certified legal translator. \
             Translate with absolute precision using standard legal terminology in the target language. \
             Preserve the exact structure of clauses, articles, and numbered sections. \
             Do not paraphrase or simplify legal language. Only output the translated text.",
        "medical" => 
            "You are a medical translator with expertise in clinical and biomedical texts. \
             Use standard medical terminology (ICD/MeSH terms where applicable). \
             Preserve drug names, dosages, anatomical terms, and abbreviations accurately. \
             Only output the translated text.",
        "casual" | "conversation" => 
            "You are a friendly translator for everyday conversations and social media content. \
             Use natural, colloquial language that sounds native. \
             Adapt idioms, slang, and cultural expressions appropriately. Only output the translated text.",
        _ => 
            "You are a professional translator. Translate the given text accurately while preserving its tone, style, and formatting. Do not add explanations or notes. Only output the translated text.",
    }
}

/// æ„é€ ç¿»è¯‘ Prompt
pub fn build_translation_prompts(request: &TranslationRequest) -> Result<(String, String), AppError> {
    // System Prompt: ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ï¼Œå¦åˆ™æ ¹æ®é¢†åŸŸé€‰æ‹©é¢„è®¾
    let mut system_prompt = if let Some(override_prompt) = &request.prompt_override {
        if !override_prompt.trim().is_empty() {
            override_prompt.clone()
        } else {
            domain_system_prompt(
                request.domain.as_deref().unwrap_or("general")
            ).to_string()
        }
    } else {
        domain_system_prompt(
            request.domain.as_deref().unwrap_or("general")
        ).to_string()
    };

    // æ³¨å…¥é£æ ¼æ§åˆ¶ï¼ˆå½“é¢†åŸŸå·²æ˜¯ casual æ—¶è·³è¿‡ï¼Œé¿å…é‡å¤æŒ‡ä»¤ï¼‰
    let domain_str = request.domain.as_deref().unwrap_or("general");
    if domain_str != "casual" && domain_str != "conversation" {
        if let Some(formality) = &request.formality {
            let style_instruction = match formality.as_str() {
                "formal" => {
                    "\n\nUse formal, polite language suitable for business or academic contexts."
                }
                "casual" => "\n\nUse casual, conversational language.",
                _ => "",
            };
            system_prompt.push_str(style_instruction);
        }
    }

    // æ³¨å…¥æœ¯è¯­è¡¨
    if let Some(glossary) = &request.glossary {
        if !glossary.is_empty() {
            system_prompt.push_str("\n\nGlossary (you MUST use these exact translations for the specified terms):");
            for (src, tgt) in glossary {
                system_prompt.push_str(&format!("\n- \"{}\" â†’ \"{}\"", src, tgt));
            }
        }
    }

    // User Prompt: ä½¿ç”¨å…¨è¯­è¨€åç§°
    let src_name = lang_full_name(&request.src_lang);
    let tgt_name = lang_full_name(&request.tgt_lang);

    let user_prompt = if request.src_lang == "auto" {
        format!(
            "Please translate the following text to {}:\n\n{}",
            tgt_name, request.text
        )
    } else {
        format!(
            "Please translate the following text from {} to {}:\n\n{}",
            src_name, tgt_name, request.text
        )
    };

    Ok((system_prompt, user_prompt))
}

/// æµå¼ç¿»è¯‘ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
async fn stream_translate<F>(
    config: &ApiConfig,
    api_key: &str,
    system_prompt: &str,
    user_prompt: &str,
    stream_event: &str,
    llm: Arc<LLMManager>,
    mut on_chunk: F,
) -> Result<StreamStatus, AppError>
where
    F: FnMut(String),
{
    let result = async {
        // æ„é€ æ¶ˆæ¯
        let messages = vec![
            json!({
                "role": "system",
                "content": system_prompt
            }),
            json!({
                "role": "user",
                "content": user_prompt
            }),
        ];

        // æ„é€ è¯·æ±‚ä½“
        let request_body = json!({
            "model": config.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": config.max_output_tokens,
            "stream": true, // å…³é”®ï¼šå¯ç”¨æµå¼
        });

        // é€‰æ‹©é€‚é…å™¨
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };

        // æ„é€  HTTP è¯·æ±‚
        let preq = adapter
            .build_request(&config.base_url, api_key, &config.model, &request_body)
            .map_err(|e| AppError::llm(format!("ç¿»è¯‘è¯·æ±‚æ„å»ºå¤±è´¥: {}", e)))?;

        let mut header_map = reqwest::header::HeaderMap::new();
        for (k, v) in preq.headers.iter() {
            if let (Ok(name), Ok(val)) = (
                reqwest::header::HeaderName::from_bytes(k.as_bytes()),
                reqwest::header::HeaderValue::from_str(v),
            ) {
                header_map.insert(name, val);
            }
        }

        // å¤ç”¨ LLMManager é…ç½®å¥½çš„ HTTP å®¢æˆ·ç«¯
        let client = llm.get_http_client();

        // æ³¨å†Œå–æ¶ˆç›‘å¬
        llm.consume_pending_cancel(stream_event).await;
        let mut cancel_rx = llm.subscribe_cancel_stream(stream_event).await;

        // å‘é€æµå¼è¯·æ±‚
        let response = client
            .post(&preq.url)
            .headers(header_map)
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::llm(format!("ç¿»è¯‘è¯·æ±‚å¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            // è®°å½•å®Œæ•´é”™è¯¯åˆ°æ—¥å¿—ï¼ˆä»…å¼€å‘è°ƒè¯•ç”¨ï¼‰
            eprintln!(
                "âŒ [Translation] API error {}: {}",
                status, error_text
            );
            // è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼Œä¸æš´éœ²æ•æ„Ÿä¿¡æ¯
            let user_message = match status.as_u16() {
                401 => "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥è®¾ç½®",
                403 => "API è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥è´¦æˆ·æƒé™",
                429 => "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•",
                500..=599 => "ç¿»è¯‘æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                _ => "ç¿»è¯‘è¯·æ±‚å¤±è´¥ï¼Œè¯·é‡è¯•",
            };
            return Err(AppError::llm(user_message.to_string()));
        }

        // è§£æ SSE æµ
        let mut stream = response.bytes_stream();
        let mut buffer = String::new();
        let mut stream_ended = false;
        let mut cancelled = false;

        while !stream_ended && !cancelled {
            if llm.consume_pending_cancel(stream_event).await {
                cancelled = true;
                break;
            }

            tokio::select! {
                changed = cancel_rx.changed() => {
                    if changed.is_ok() && *cancel_rx.borrow() {
                        cancelled = true;
                    }
                }
                chunk_result = stream.next() => {
                    match chunk_result {
                        Some(chunk) => {
                            let bytes = chunk.map_err(|e| AppError::llm(format!("è¯»å–æµå¤±è´¥: {}", e)))?;
                            buffer.push_str(&String::from_utf8_lossy(&bytes));

                            while let Some(pos) = buffer.find("\n\n") {
                                let line = buffer[..pos].trim().to_string();
                                buffer = buffer[pos + 2..].to_string();

                                if line.is_empty() {
                                    continue;
                                }

                                if line == "data: [DONE]" {
                                    stream_ended = true;
                                    break;
                                }

                                let events = adapter.parse_stream(&line);
                                for event in events {
                                    match event {
                                        crate::providers::StreamEvent::ContentChunk(content) => {
                                            on_chunk(content);
                                        }
                                        crate::providers::StreamEvent::Done => {
                                            stream_ended = true;
                                            break;
                                        }
                                        _ => {}
                                    }
                                }

                                if stream_ended {
                                    break;
                                }
                            }
                        }
                        None => {
                            break;
                        }
                    }
                }
            }
        }

        if cancelled {
            return Ok(StreamStatus::Cancelled);
        }

        Ok(StreamStatus::Completed)
    }.await;

    llm.clear_cancel_stream(stream_event).await;

    result
}
