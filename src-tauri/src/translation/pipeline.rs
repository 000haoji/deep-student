use futures_util::StreamExt;
use serde_json::json;
/// ç¿»è¯‘ç®¡çº¿ - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
use std::sync::Arc;

use crate::database::Database;
use crate::llm_manager::{ApiConfig, LLMManager};
use crate::models::AppError;
use crate::providers::ProviderAdapter;
// â˜… VFS ç»Ÿä¸€å­˜å‚¨ï¼ˆ2025-12-07ï¼‰
use crate::vfs::database::VfsDatabase;

use super::events::TranslationEventEmitter;
use super::types::{TranslationRequest, TranslationResponse};

/// ç¿»è¯‘ç®¡çº¿ä¾èµ–
pub struct TranslationDeps {
    pub llm: Arc<LLMManager>,
    pub db: Arc<Database>, // ä¸»æ•°æ®åº“ï¼ˆé…ç½®/è®¾ç½®è¯»å–ï¼‰
    pub emitter: TranslationEventEmitter,
    pub vfs_db: Arc<VfsDatabase>, // â˜… VFS æ•°æ®åº“ï¼ˆå¿…éœ€ï¼Œå”¯ä¸€å­˜å‚¨ï¼‰
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum StreamStatus {
    Completed,
    Cancelled,
}

/// è¿è¡Œç¿»è¯‘ç®¡çº¿
pub async fn run_translation(
    request: TranslationRequest,
    deps: TranslationDeps,
) -> Result<Option<TranslationResponse>, AppError> {
    // 0. è¾“å…¥éªŒè¯ï¼šæ£€æŸ¥ç©ºæ–‡æœ¬
    if request.text.trim().is_empty() {
        return Err(AppError::validation("ç¿»è¯‘æ–‡æœ¬ä¸èƒ½ä¸ºç©º".to_string()));
    }

    // 0.1 è¾“å…¥éªŒè¯ï¼šæ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆé˜²æ­¢è¶…å¤§æ–‡æœ¬å¯¼è‡´ API è¶…æ—¶æˆ– OOMï¼‰
    const MAX_TEXT_CHARS: usize = 100_000; // 100K å­—ç¬¦ä¸Šé™
    let text_char_count = request.text.chars().count();
    if text_char_count > MAX_TEXT_CHARS {
        return Err(AppError::validation(format!(
            "ç¿»è¯‘æ–‡æœ¬è¿‡é•¿ï¼ˆå½“å‰ {} å­—ç¬¦ï¼Œæœ€å¤§ {} å­—ç¬¦ï¼‰",
            text_char_count, MAX_TEXT_CHARS
        )));
    }

    // 1. æ„é€ ç¿»è¯‘ Prompt
    let (system_prompt, user_prompt) = build_translation_prompts(&request)?;

    // 2. è·å–ç¿»è¯‘æ¨¡å‹é…ç½®å¹¶è§£å¯† API Key
    let config = deps.llm.get_translation_model_config().await?;
    let api_key = deps.llm.decrypt_api_key(&config.api_key)?;

    // 3. æµå¼è°ƒç”¨ LLM
    let mut accumulated = String::new();
    let stream_event = format!("translation_stream_{}", request.session_id);

    let stream_status = stream_translate(
        &config,
        &api_key,
        &system_prompt,
        &user_prompt,
        &stream_event,
        deps.llm.clone(),
        |chunk| {
            accumulated.push_str(&chunk);
            deps.emitter
                .emit_data(&request.session_id, chunk, accumulated.clone());
        },
    )
    .await?;

    if matches!(stream_status, StreamStatus::Cancelled) {
        deps.emitter.emit_cancelled(&request.session_id);
        return Ok(None);
    }

    // ğŸ”§ P0-06 ä¿®å¤ï¼šç§»é™¤åç«¯çš„ VFS è®°å½•åˆ›å»ºï¼Œç”±å‰ç«¯ç»Ÿä¸€ç®¡ç†
    // åŸå› ï¼šå‰ç«¯é€šè¿‡ Learning Hub åˆ›å»ºç©ºç¿»è¯‘æ–‡ä»¶åï¼Œåç«¯å†åˆ›å»ºä¼šå¯¼è‡´åŒå†™ï¼ˆå­¤å„¿è®°å½•ï¼‰
    // ç°åœ¨åªè¿”å›ç¿»è¯‘ç»“æœï¼Œå‰ç«¯é€šè¿‡ DSTU adapter çš„ updateTranslation æ›´æ–°è®°å½•
    let now = chrono::Utc::now().to_rfc3339();

    println!("âœ… [Translation] ç¿»è¯‘å®Œæˆï¼Œç”±å‰ç«¯ç®¡ç†å­˜å‚¨");

    // 5. å‘é€å®Œæˆäº‹ä»¶ï¼ˆä¸å†åˆ›å»ºæ–°è®°å½•ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼‰
    deps.emitter.emit_complete(
        &request.session_id,
        request.session_id.clone(), // ä½¿ç”¨ session_id ä½œä¸ºä¸´æ—¶ IDï¼Œå‰ç«¯ä¼šç”¨å®é™… node ID
        accumulated.clone(),
        now.clone(),
    );

    Ok(Some(TranslationResponse {
        id: request.session_id.clone(), // ä½¿ç”¨ session_idï¼Œå‰ç«¯ä¼šå¿½ç•¥æ­¤å€¼
        translated_text: accumulated,
        created_at: now,
        session_id: request.session_id,
    }))
}

/// æ„é€ ç¿»è¯‘ Prompt
fn build_translation_prompts(request: &TranslationRequest) -> Result<(String, String), AppError> {
    // System Prompt
    let mut system_prompt = request.prompt_override.clone().unwrap_or_else(|| {
        "You are a professional translator. Translate the given text accurately while preserving its tone, style, and formatting. Do not add explanations or notes. Only output the translated text.".to_string()
    });

    // æ³¨å…¥é£æ ¼æ§åˆ¶
    if let Some(formality) = &request.formality {
        let style_instruction = match formality.as_str() {
            "formal" => {
                "\n\nUse formal, polite language suitable for business or academic contexts."
            }
            "casual" => "\n\nUse casual, conversational language.",
            _ => "",
        };
        system_prompt.push_str(style_instruction);
    }

    // æ³¨å…¥æœ¯è¯­è¡¨
    if let Some(glossary) = &request.glossary {
        if !glossary.is_empty() {
            system_prompt.push_str("\n\nGlossary (must use these translations):");
            for (src, tgt) in glossary {
                system_prompt.push_str(&format!("\n- {} â†’ {}", src, tgt));
            }
        }
    }

    // User Prompt
    let user_prompt = if request.src_lang == "auto" {
        format!(
            "Please translate the following text to {}:\n\n{}",
            request.tgt_lang, request.text
        )
    } else {
        format!(
            "Please translate the following text from {} to {}:\n\n{}",
            request.src_lang, request.tgt_lang, request.text
        )
    };

    Ok((system_prompt, user_prompt))
}

/// æµå¼ç¿»è¯‘ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
async fn stream_translate<F>(
    config: &ApiConfig,
    api_key: &str,
    system_prompt: &str,
    user_prompt: &str,
    stream_event: &str,
    llm: Arc<LLMManager>,
    mut on_chunk: F,
) -> Result<StreamStatus, AppError>
where
    F: FnMut(String),
{
    let result = async {
        // æ„é€ æ¶ˆæ¯
        let messages = vec![
            json!({
                "role": "system",
                "content": system_prompt
            }),
            json!({
                "role": "user",
                "content": user_prompt
            }),
        ];

        // æ„é€ è¯·æ±‚ä½“
        let request_body = json!({
            "model": config.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": config.max_output_tokens,
            "stream": true, // å…³é”®ï¼šå¯ç”¨æµå¼
        });

        // é€‰æ‹©é€‚é…å™¨
        let adapter: Box<dyn ProviderAdapter> = match config.model_adapter.as_str() {
            "google" | "gemini" => Box::new(crate::providers::GeminiAdapter::new()),
            "anthropic" | "claude" => Box::new(crate::providers::AnthropicAdapter::new()),
            _ => Box::new(crate::providers::OpenAIAdapter),
        };

        // æ„é€  HTTP è¯·æ±‚
        let preq = adapter
            .build_request(&config.base_url, api_key, &config.model, &request_body)
            .map_err(|e| AppError::llm(format!("ç¿»è¯‘è¯·æ±‚æ„å»ºå¤±è´¥: {}", e)))?;

        let mut header_map = reqwest::header::HeaderMap::new();
        for (k, v) in preq.headers.iter() {
            if let (Ok(name), Ok(val)) = (
                reqwest::header::HeaderName::from_bytes(k.as_bytes()),
                reqwest::header::HeaderValue::from_str(v),
            ) {
                header_map.insert(name, val);
            }
        }

        // å¤ç”¨ LLMManager é…ç½®å¥½çš„ HTTP å®¢æˆ·ç«¯
        let client = llm.get_http_client();

        // æ³¨å†Œå–æ¶ˆç›‘å¬
        llm.consume_pending_cancel(stream_event).await;
        let mut cancel_rx = llm.subscribe_cancel_stream(stream_event).await;

        // å‘é€æµå¼è¯·æ±‚
        let response = client
            .post(&preq.url)
            .headers(header_map)
            .json(&preq.body)
            .send()
            .await
            .map_err(|e| AppError::llm(format!("ç¿»è¯‘è¯·æ±‚å¤±è´¥: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            // è®°å½•å®Œæ•´é”™è¯¯åˆ°æ—¥å¿—ï¼ˆä»…å¼€å‘è°ƒè¯•ç”¨ï¼‰
            eprintln!(
                "âŒ [Translation] API error {}: {}",
                status, error_text
            );
            // è¿”å›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼Œä¸æš´éœ²æ•æ„Ÿä¿¡æ¯
            let user_message = match status.as_u16() {
                401 => "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥è®¾ç½®",
                403 => "API è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥è´¦æˆ·æƒé™",
                429 => "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•",
                500..=599 => "ç¿»è¯‘æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                _ => "ç¿»è¯‘è¯·æ±‚å¤±è´¥ï¼Œè¯·é‡è¯•",
            };
            return Err(AppError::llm(user_message.to_string()));
        }

        // è§£æ SSE æµ
        let mut stream = response.bytes_stream();
        let mut buffer = String::new();
        let mut stream_ended = false;
        let mut cancelled = false;

        while !stream_ended && !cancelled {
            if llm.consume_pending_cancel(stream_event).await {
                cancelled = true;
                break;
            }

            tokio::select! {
                changed = cancel_rx.changed() => {
                    if changed.is_ok() && *cancel_rx.borrow() {
                        cancelled = true;
                    }
                }
                chunk_result = stream.next() => {
                    match chunk_result {
                        Some(chunk) => {
                            let bytes = chunk.map_err(|e| AppError::llm(format!("è¯»å–æµå¤±è´¥: {}", e)))?;
                            buffer.push_str(&String::from_utf8_lossy(&bytes));

                            while let Some(pos) = buffer.find("\n\n") {
                                let line = buffer[..pos].trim().to_string();
                                buffer = buffer[pos + 2..].to_string();

                                if line.is_empty() {
                                    continue;
                                }

                                if line == "data: [DONE]" {
                                    stream_ended = true;
                                    break;
                                }

                                let events = adapter.parse_stream(&line);
                                for event in events {
                                    match event {
                                        crate::providers::StreamEvent::ContentChunk(content) => {
                                            on_chunk(content);
                                        }
                                        crate::providers::StreamEvent::Done => {
                                            stream_ended = true;
                                            break;
                                        }
                                        _ => {}
                                    }
                                }

                                if stream_ended {
                                    break;
                                }
                            }
                        }
                        None => {
                            break;
                        }
                    }
                }
            }
        }

        if cancelled {
            return Ok(StreamStatus::Cancelled);
        }

        Ok(StreamStatus::Completed)
    }.await;

    llm.clear_cancel_stream(stream_event).await;

    result
}
