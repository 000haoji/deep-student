//! è®ºæ–‡ä¿å­˜ä¸å¼•ç”¨æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œå™¨
//!
//! ## å·¥å…·
//! - `builtin-paper_save` â€” ä¸‹è½½å­¦æœ¯è®ºæ–‡ PDF å¹¶ä¿å­˜åˆ° VFSï¼ˆæ”¯æŒæ‰¹é‡ â‰¤5ï¼‰
//!   - æ”¯æŒ arXiv IDã€DOIã€ç›´æ¥ PDF URL
//!   - DOI è‡ªåŠ¨é€šè¿‡ Unpaywall API è§£æå¼€æ”¾è·å– PDF
//!   - SHA256 å»é‡ï¼šå·²å­˜åœ¨çš„è®ºæ–‡ç›´æ¥è¿”å› VFS æ–‡ä»¶ ID
//! - `builtin-cite_format` â€” å°†è®ºæ–‡å…ƒæ•°æ®æ ¼å¼åŒ–ä¸ºæ ‡å‡†å¼•ç”¨æ ¼å¼
//!   - æ”¯æŒ BibTeXã€GB/T 7714ã€APA ä¸‰ç§æ ¼å¼
//!
//! ## è®¾è®¡è¯´æ˜
//! - PDF ä¸‹è½½åèµ° VFS å®Œæ•´é“¾è·¯ï¼šblob å­˜å‚¨ â†’ æ–‡ä»¶åˆ›å»º â†’ æ–‡æœ¬æå– â†’ ç´¢å¼• â†’ å¼‚æ­¥ OCR
//! - ä½¿ç”¨ ExecutionContext.vfs_db æ“ä½œ VFS æ•°æ®åº“
//! - ä½¿ç”¨ ExecutionContext.pdf_processing_service è§¦å‘å¼‚æ­¥ OCR/å‹ç¼© Pipeline

use std::sync::Arc;
use std::time::Instant;

use async_trait::async_trait;
use reqwest::header::{HeaderMap, HeaderValue, USER_AGENT};
use serde_json::{json, Value};
use std::time::Duration;

use super::executor::{ExecutionContext, ToolExecutor, ToolSensitivity};
use super::strip_tool_namespace;
use crate::chat_v2::events::event_types;
use crate::chat_v2::types::{ToolCall, ToolResultInfo};

// ============================================================================
// å¸¸é‡
// ============================================================================

/// å•æ¬¡æ‰¹é‡ä¸‹è½½ä¸Šé™
const MAX_BATCH_SIZE: usize = 5;

/// PDF ä¸‹è½½è¶…æ—¶ï¼ˆç§’ï¼‰
const PDF_DOWNLOAD_TIMEOUT_SECS: u64 = 60;

/// PDF æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆ50MBï¼Œä¸ VFS ä¸€è‡´ï¼‰
const MAX_PDF_SIZE: usize = 50 * 1024 * 1024;

/// Unpaywall API ç«¯ç‚¹ï¼ˆé€šè¿‡ DOI æŸ¥æ‰¾å¼€æ”¾è·å– PDFï¼‰
const UNPAYWALL_API_URL: &str = "https://api.unpaywall.org/v2";

/// Unpaywall è¯·æ±‚è¶…æ—¶
const UNPAYWALL_TIMEOUT_SECS: u64 = 15;

/// User-Agent
const UA: &str = "DeepStudent/1.0 (Academic Paper Save; mailto:support@deepstudent.app)";

/// ä¸‹è½½è¿›åº¦å‘å°„æœ€å°é—´éš”ï¼ˆæ¯ 500KB æˆ–æ¯ 5%ï¼‰
const PROGRESS_BYTES_INTERVAL: usize = 512 * 1024; // 512KB

// ============================================================================
// è¿›åº¦çŠ¶æ€
// ============================================================================

/// å•ç¯‡è®ºæ–‡çš„å¤„ç†é˜¶æ®µ
#[derive(Clone, Copy, serde::Serialize)]
#[serde(rename_all = "snake_case")]
enum PaperStage {
    Resolving,
    Downloading,
    Deduplicating,
    Storing,
    Processing,
    Indexing,
    Done,
    Error,
}

/// å•ç¯‡è®ºæ–‡çš„è¿›åº¦çŠ¶æ€
#[derive(Clone, serde::Serialize)]
struct PaperProgressItem {
    /// ç´¢å¼•
    #[serde(rename = "i")]
    index: usize,
    /// æ ‡é¢˜
    #[serde(rename = "t")]
    title: String,
    /// å½“å‰é˜¶æ®µ
    #[serde(rename = "s")]
    stage: PaperStage,
    /// ä¸‹è½½è¿›åº¦ 0-100
    #[serde(rename = "pct")]
    percent: u8,
    /// å·²ä¸‹è½½å­—èŠ‚
    #[serde(rename = "dl", skip_serializing_if = "Option::is_none")]
    downloaded: Option<u64>,
    /// æ€»å­—èŠ‚
    #[serde(rename = "total", skip_serializing_if = "Option::is_none")]
    total_bytes: Option<u64>,
    /// æ–‡ä»¶ IDï¼ˆå®Œæˆåï¼‰
    #[serde(rename = "fid", skip_serializing_if = "Option::is_none")]
    file_id: Option<String>,
    /// æ˜¯å¦å»é‡
    #[serde(rename = "dedup", skip_serializing_if = "std::ops::Not::not")]
    deduplicated: bool,
    /// é”™è¯¯ä¿¡æ¯
    #[serde(rename = "err", skip_serializing_if = "Option::is_none")]
    error: Option<String>,
    /// å½“å‰ä¸‹è½½æºæ ‡ç­¾
    #[serde(rename = "src", skip_serializing_if = "Option::is_none")]
    source: Option<String>,
    /// å¯ç”¨çš„ä¸‹è½½æºåˆ—è¡¨ï¼ˆä¾›å‰ç«¯æ‰‹åŠ¨åˆ‡æ¢ï¼‰
    #[serde(rename = "srcs", skip_serializing_if = "Option::is_none")]
    sources: Option<Vec<SourceCandidate>>,
}

/// ä¸‹è½½æºå€™é€‰
#[derive(Clone, serde::Serialize)]
struct SourceCandidate {
    /// æºæ ‡ç­¾ï¼ˆå¦‚ "arXiv", "arXiv Mirror", "Unpaywall"ï¼‰
    label: String,
    /// ä¸‹è½½ URL
    url: String,
}

impl PaperProgressItem {
    fn new(index: usize, title: &str) -> Self {
        Self {
            index,
            title: title.to_string(),
            stage: PaperStage::Resolving,
            percent: 0,
            downloaded: None,
            total_bytes: None,
            file_id: None,
            deduplicated: false,
            error: None,
            source: None,
            sources: None,
        }
    }
}

/// é€šè¿‡ emit_chunk å‘å°„å½“å‰è¿›åº¦å¿«ç…§ï¼ˆNDJSONæ ¼å¼ï¼‰
fn emit_progress(ctx: &ExecutionContext, papers: &[PaperProgressItem]) {
    if let Ok(json_line) = serde_json::to_string(&json!({ "papers": papers })) {
        ctx.emitter.emit_chunk(
            event_types::TOOL_CALL,
            &ctx.block_id,
            &format!("{}\n", json_line),
            None,
        );
    }
}

// ============================================================================
// è®ºæ–‡ä¿å­˜æ‰§è¡Œå™¨
// ============================================================================

pub struct PaperSaveExecutor {
    /// HTTP å®¢æˆ·ç«¯ï¼ˆPDF ä¸‹è½½ç”¨ï¼Œé•¿è¶…æ—¶ï¼‰
    download_client: reqwest::Client,
    /// HTTP å®¢æˆ·ç«¯ï¼ˆUnpaywall API ç”¨ï¼ŒçŸ­è¶…æ—¶ï¼‰
    unpaywall_client: reqwest::Client,
}

impl PaperSaveExecutor {
    pub fn new() -> Self {
        let mut headers = HeaderMap::new();
        headers.insert(USER_AGENT, HeaderValue::from_static(UA));

        let download_client = reqwest::Client::builder()
            .timeout(Duration::from_secs(PDF_DOWNLOAD_TIMEOUT_SECS))
            .default_headers(headers.clone())
            .build()
            .expect("Failed to create PDF download HTTP client");

        let unpaywall_client = reqwest::Client::builder()
            .timeout(Duration::from_secs(UNPAYWALL_TIMEOUT_SECS))
            .default_headers(headers)
            .build()
            .expect("Failed to create Unpaywall HTTP client");

        Self {
            download_client,
            unpaywall_client,
        }
    }


    // ========================================================================
    // paper_save â€” æ‰¹é‡ä¸‹è½½è®ºæ–‡åˆ° VFS
    // ========================================================================

    async fn execute_paper_save(
        &self,
        call: &ToolCall,
        ctx: &ExecutionContext,
    ) -> Result<Value, String> {
        // ğŸ”§ è¯Šæ–­æ—¥å¿—ï¼šåœ¨å…¥å£å¤„è®°å½• arguments çš„åŸå§‹ç±»å‹å’Œå†…å®¹é¢„è§ˆ
        {
            let raw = call.arguments.to_string();
            log::info!(
                "[PaperSave] execute_paper_save called. args_type={}, args_len={}, preview={}",
                if call.arguments.is_object() { "object" }
                else if call.arguments.is_array() { "array" }
                else if call.arguments.is_string() { "string" }
                else if call.arguments.is_null() { "null" }
                else { "other" },
                raw.len(),
                &raw[..raw.len().min(300)]
            );
        }

        // å¥å£®åŒ–å‚æ•°æå–ï¼šå¤„ç†å¤šç§ arguments æ ¼å¼
        let papers_owned: Vec<Value>;

        // æˆªæ–­é”™è¯¯æ£€æŸ¥ï¼ˆä¼˜å…ˆï¼‰
        if call.arguments.get("_truncation_error").is_some() {
            return Err(
                "Tool call arguments were truncated by LLM max_tokens limit. Please retry with a shorter prompt."
                    .to_string(),
            );
        }

        // è¾…åŠ©å‡½æ•°ï¼šä» Value ä¸­æå– papers æ•°ç»„ï¼ˆå¤„ç† "papers" å€¼å¯èƒ½æ˜¯ array / string / object ç­‰å„ç§ç±»å‹ï¼‰
        fn extract_papers_from_value(val: &Value) -> Option<Vec<Value>> {
            // å°è¯•ä» "papers" key æå–
            if let Some(papers_val) = val.get("papers") {
                let val_type = if papers_val.is_array() { "array" }
                    else if papers_val.is_string() { "string" }
                    else if papers_val.is_object() { "object" }
                    else if papers_val.is_null() { "null" }
                    else { "other" };
                log::info!("[PaperSave] Found 'papers' key, value type={}, preview={}", val_type, {
                    let s = papers_val.to_string();
                    s[..s.len().min(200)].to_string()
                });
                if let Some(arr) = papers_val.as_array() {
                    // æ­£å¸¸ï¼š{"papers": [...]}
                    return Some(arr.clone());
                }
                if let Some(s) = papers_val.as_str() {
                    // åŒé‡ç¼–ç ï¼š{"papers": "[{...}]"} â€” papers å€¼æ˜¯ JSON å­—ç¬¦ä¸²
                    log::warn!("[PaperSave] 'papers' value is a JSON string (len={}), double-decoding", s.len());
                    if let Ok(inner) = serde_json::from_str::<Value>(s) {
                        if let Some(arr) = inner.as_array() {
                            return Some(arr.clone());
                        }
                        // è§£æå‡ºæ¥æ˜¯å•ä¸ªå¯¹è±¡
                        if inner.is_object() {
                            return Some(vec![inner]);
                        }
                    }
                }
                if papers_val.is_object() {
                    // å•ç¯‡è®ºæ–‡ç›´æ¥æ”¾åœ¨ papers key ä¸‹ï¼š{"papers": {"title": "..."}}
                    log::warn!("[PaperSave] 'papers' value is a single object, wrapping in array");
                    return Some(vec![papers_val.clone()]);
                }
            }
            // å°è¯•ä» "paper" (å•æ•°) key æå–
            if let Some(paper_val) = val.get("paper") {
                log::warn!("[PaperSave] LLM used 'paper' (singular) instead of 'papers', auto-correcting");
                if let Some(arr) = paper_val.as_array() {
                    return Some(arr.clone());
                }
                if let Some(s) = paper_val.as_str() {
                    if let Ok(inner) = serde_json::from_str::<Value>(s) {
                        if let Some(arr) = inner.as_array() {
                            return Some(arr.clone());
                        }
                        if inner.is_object() {
                            return Some(vec![inner]);
                        }
                    }
                }
                if paper_val.is_object() {
                    return Some(vec![paper_val.clone()]);
                }
            }
            // å¦‚æœå¯¹è±¡è‡ªèº«æ˜¯å•ç¯‡è®ºæ–‡ {"title": "...", "doi": "..."}
            if val.is_object() && val.get("title").is_some() {
                log::warn!("[PaperSave] LLM sent a single paper object without 'papers' wrapper, auto-wrapping");
                return Some(vec![val.clone()]);
            }
            None
        }

        if let Some(arr) = call.arguments.as_array() {
            // LLM ç›´æ¥ä¼ äº†è£¸æ•°ç»„è€Œé {"papers": [...]}
            log::warn!("[PaperSave] arguments is a bare array, wrapping as papers");
            papers_owned = arr.clone();
        } else if let Some(extracted) = extract_papers_from_value(&call.arguments) {
            papers_owned = extracted;
        } else if let Some(s) = call.arguments.as_str() {
            // æ•´ä¸ª arguments æ˜¯ JSON å­—ç¬¦ä¸²ï¼ˆåŒé‡ç¼–ç ï¼‰
            log::warn!(
                "[PaperSave] arguments is a JSON string (len={}), attempting double-decode",
                s.len()
            );
            let parsed: Value =
                serde_json::from_str(s).map_err(|e| format!("Failed to parse arguments string: {}", e))?;
            papers_owned = extract_papers_from_value(&parsed).ok_or_else(|| {
                format!(
                    "After double-decode, still missing 'papers' (array). Parsed keys: {:?}",
                    parsed.as_object().map(|o| o.keys().collect::<Vec<_>>())
                )
            })?;
        } else {
            // è¯Šæ–­æ—¥å¿—ï¼šæ‰“å°å®é™… arguments ç»“æ„
            let keys: Vec<String> = call
                .arguments
                .as_object()
                .map(|o| o.keys().cloned().collect())
                .unwrap_or_default();
            let type_name = if call.arguments.is_object() {
                "object"
            } else if call.arguments.is_string() {
                "string"
            } else if call.arguments.is_array() {
                "array"
            } else if call.arguments.is_null() {
                "null"
            } else {
                "other"
            };
            let raw = call.arguments.to_string();
            log::error!(
                "[PaperSave] Cannot extract 'papers' from arguments. type={}, keys={:?}, raw_preview={}",
                type_name,
                keys,
                &raw[..raw.len().min(500)]
            );
            return Err(format!(
                "Missing required parameter 'papers' (array). Got arguments type={}, keys={:?}",
                type_name, keys
            ));
        }

        let papers = &papers_owned;

        if papers.is_empty() {
            return Err("'papers' array is empty".to_string());
        }
        if papers.len() > MAX_BATCH_SIZE {
            return Err(format!(
                "Batch size {} exceeds limit {}. Please split into multiple calls.",
                papers.len(),
                MAX_BATCH_SIZE
            ));
        }

        let folder_id = call
            .arguments
            .get("folder_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());

        let vfs_db = ctx
            .vfs_db
            .as_ref()
            .ok_or("VFS database not available")?;

        // åˆå§‹åŒ–è¿›åº¦çŠ¶æ€æ•°ç»„
        let mut progress: Vec<PaperProgressItem> = papers
            .iter()
            .enumerate()
            .map(|(i, p)| {
                let title = p
                    .get("title")
                    .and_then(|v| v.as_str())
                    .unwrap_or("Untitled Paper");
                PaperProgressItem::new(i, title)
            })
            .collect();

        // å‘å°„åˆå§‹è¿›åº¦
        emit_progress(ctx, &progress);

        let mut results = Vec::new();

        for (i, paper) in papers.iter().enumerate() {
            if ctx.is_cancelled() {
                progress[i].stage = PaperStage::Error;
                progress[i].error = Some("cancelled".to_string());
                emit_progress(ctx, &progress);
                results.push(json!({
                    "index": i,
                    "success": false,
                    "error": "cancelled",
                }));
                break;
            }

            let result = self
                .save_single_paper(paper, folder_id.as_deref(), vfs_db, ctx, &mut progress, i)
                .await;

            match result {
                Ok(mut info) => {
                    if let Some(obj) = info.as_object_mut() {
                        obj.insert("index".to_string(), json!(i));
                    }
                    results.push(info);
                }
                Err(e) => {
                    let title = progress[i].title.clone();
                    log::warn!(
                        "[PaperSave] Failed to save paper '{}': {}",
                        title,
                        e
                    );
                    progress[i].stage = PaperStage::Error;
                    progress[i].error = Some(e.clone());
                    emit_progress(ctx, &progress);
                    results.push(json!({
                        "index": i,
                        "success": false,
                        "title": title,
                        "error": e,
                    }));
                }
            }
        }

        let success_count = results.iter().filter(|r| r.get("success").and_then(|v| v.as_bool()).unwrap_or(false)).count();

        Ok(json!({
            "total": papers.len(),
            "success_count": success_count,
            "failed_count": papers.len() - success_count,
            "results": results,
        }))
    }

    /// ä¿å­˜å•ç¯‡è®ºæ–‡ï¼ˆå¸¦è¿›åº¦å‘å°„ + å¤šæºè‡ªåŠ¨å›é€€ï¼‰
    async fn save_single_paper(
        &self,
        paper: &Value,
        folder_id: Option<&str>,
        vfs_db: &Arc<crate::vfs::database::VfsDatabase>,
        ctx: &ExecutionContext,
        progress: &mut Vec<PaperProgressItem>,
        idx: usize,
    ) -> Result<Value, String> {
        let url = paper.get("url").and_then(|v| v.as_str());
        let doi = paper.get("doi").and_then(|v| v.as_str());
        let arxiv_id = paper.get("arxiv_id").and_then(|v| v.as_str());
        let title = paper
            .get("title")
            .and_then(|v| v.as_str())
            .unwrap_or("Untitled Paper");

        // â”€â”€ Stage: Resolving â”€â”€
        progress[idx].stage = PaperStage::Resolving;
        emit_progress(ctx, progress);

        let candidates = self.resolve_all_pdf_urls(url, doi, arxiv_id).await;
        if candidates.is_empty() {
            return Err("No URL, arXiv ID, or DOI provided. At least one is required.".to_string());
        }

        // å°†å¯ç”¨æºåˆ—è¡¨å†™å…¥è¿›åº¦ï¼ˆä¾›å‰ç«¯æ‰‹åŠ¨åˆ‡æ¢ï¼‰
        progress[idx].sources = Some(
            candidates
                .iter()
                .map(|(u, label)| SourceCandidate {
                    label: label.clone(),
                    url: u.clone(),
                })
                .collect(),
        );

        // â”€â”€ å¤šæºè‡ªåŠ¨å›é€€ä¸‹è½½ â”€â”€
        let mut pdf_bytes: Option<Vec<u8>> = None;
        let mut last_error = String::new();

        for (candidate_url, source_label) in &candidates {
            if ctx.is_cancelled() {
                return Err("Download cancelled".to_string());
            }

            log::info!(
                "[PaperSave] Trying '{}' source={} url={}",
                title,
                source_label,
                candidate_url
            );

            progress[idx].stage = PaperStage::Downloading;
            progress[idx].source = Some(source_label.clone());
            progress[idx].percent = 0;
            progress[idx].downloaded = None;
            progress[idx].total_bytes = None;
            emit_progress(ctx, progress);

            match self
                .download_pdf_with_progress(candidate_url, ctx, progress, idx)
                .await
            {
                Ok(bytes) => {
                    pdf_bytes = Some(bytes);
                    break;
                }
                Err(e) => {
                    log::warn!(
                        "[PaperSave] Source '{}' failed for '{}': {}",
                        source_label,
                        title,
                        e
                    );
                    last_error = format!("[{}] {}", source_label, e);
                    // ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæº
                }
            }
        }

        let pdf_bytes = match pdf_bytes {
            Some(b) => b,
            None => {
                return Err(format!(
                    "All {} sources failed. Last error: {}",
                    candidates.len(),
                    last_error
                ));
            }
        };

        log::info!(
            "[PaperSave] Downloaded {} bytes for '{}'",
            pdf_bytes.len(),
            title
        );

        // â”€â”€ Stage: Deduplicating â”€â”€
        progress[idx].stage = PaperStage::Deduplicating;
        progress[idx].percent = 100;
        emit_progress(ctx, progress);

        use sha2::{Digest, Sha256};
        let mut hasher = Sha256::new();
        hasher.update(&pdf_bytes);
        let sha256 = format!("{:x}", hasher.finalize());

        let conn = vfs_db.get_conn_safe().map_err(|e| e.to_string())?;

        use crate::vfs::VfsFileRepo;
        if let Ok(Some(existing)) = VfsFileRepo::get_by_sha256_with_conn(&conn, &sha256) {
            if existing.status == "active" {
                log::info!(
                    "[PaperSave] Paper '{}' already exists: {}",
                    title,
                    existing.id
                );
                progress[idx].stage = PaperStage::Done;
                progress[idx].deduplicated = true;
                progress[idx].file_id = Some(existing.id.clone());
                emit_progress(ctx, progress);
                return Ok(json!({
                    "success": true,
                    "deduplicated": true,
                    "file_id": existing.id,
                    "title": title,
                    "message": format!("è®ºæ–‡å·²å­˜åœ¨äºèµ„æ–™åº“ä¸­ï¼ˆæ–‡ä»¶ID: {}ï¼‰", existing.id),
                }));
            }
        }

        // â”€â”€ Stage: Storing â”€â”€
        progress[idx].stage = PaperStage::Storing;
        emit_progress(ctx, progress);

        use crate::vfs::VfsBlobRepo;
        let blobs_dir = vfs_db.blobs_dir();
        let blob_hash = VfsBlobRepo::store_blob_with_conn(
            &conn,
            &blobs_dir,
            &pdf_bytes,
            Some("application/pdf"),
            None,
        )
        .map_err(|e| format!("Blob storage failed: {}", e))?
        .hash;

        // â”€â”€ Stage: Processing â”€â”€
        progress[idx].stage = PaperStage::Processing;
        emit_progress(ctx, progress);

        use crate::vfs::repos::pdf_preview::{render_pdf_preview, PdfPreviewConfig};
        let (preview_json, extracted_text, page_count) =
            match render_pdf_preview(&conn, &blobs_dir, &pdf_bytes, &PdfPreviewConfig::default()) {
                Ok(result) => {
                    let preview_str = result
                        .preview_json
                        .as_ref()
                        .and_then(|p| serde_json::to_string(p).ok());
                    (
                        preview_str,
                        result.extracted_text,
                        Some(result.page_count as i32),
                    )
                }
                Err(e) => {
                    log::warn!("[PaperSave] PDF preview failed for '{}': {}", title, e);
                    (None, None, None)
                }
            };

        let safe_title = sanitize_filename(title);
        let file_name = if safe_title.to_lowercase().ends_with(".pdf") {
            safe_title
        } else {
            format!("{}.pdf", safe_title)
        };

        // ğŸ”§ ä¿®å¤ï¼šä¸æŒ‡å®š folder_id æ—¶å­˜åˆ°æ ¹ç›®å½•ï¼ˆNoneï¼‰ï¼Œ
        // ä½¿è®ºæ–‡ç›´æ¥å‡ºç°åœ¨å­¦ä¹ èµ„æº"å…¨éƒ¨æ–‡ä»¶"è§†å›¾ä¸­ã€‚
        // ä¹‹å‰é”™è¯¯åœ°ä½¿ç”¨ AttachmentConfig::get_or_create_root_folder()
        // å¯¼è‡´è®ºæ–‡è¢«å­˜åˆ°"é™„ä»¶"éšè—æ–‡ä»¶å¤¹ä¸­ã€‚
        let target_folder_id = match folder_id {
            Some(id) if !id.is_empty() => Some(id.to_string()),
            _ => None, // æ ¹ç›®å½•
        };

        let file = VfsFileRepo::create_file_with_doc_data_in_folder(
            &conn,
            &sha256,
            &file_name,
            pdf_bytes.len() as i64,
            "pdf",
            Some("application/pdf"),
            Some(&blob_hash),
            None,
            target_folder_id.as_deref(),
            preview_json.as_deref(),
            extracted_text.as_deref(),
            page_count,
        )
        .map_err(|e| format!("File creation failed: {}", e))?;

        log::info!(
            "[PaperSave] File created: {} (name={}, pages={:?})",
            file.id,
            file_name,
            page_count
        );

        // â”€â”€ Stage: Indexing â”€â”€
        progress[idx].stage = PaperStage::Indexing;
        emit_progress(ctx, progress);

        if let Some(ref resource_id) = file.resource_id {
            use crate::vfs::index_service::VfsIndexService;
            use crate::vfs::unit_builder::UnitBuildInput;
            let index_service = VfsIndexService::new(vfs_db.clone());
            let input = UnitBuildInput {
                resource_id: resource_id.clone(),
                resource_type: "file".to_string(),
                data: None,
                ocr_text: None,
                ocr_pages_json: None,
                blob_hash: Some(blob_hash.clone()),
                page_count: file.page_count,
                extracted_text: file.extracted_text.clone(),
                preview_json: file.preview_json.clone(),
            };
            match index_service.sync_resource_units(input) {
                Ok(units) => {
                    log::info!(
                        "[PaperSave] Indexed {} units for file {}",
                        units.len(),
                        file.id
                    );
                }
                Err(e) => {
                    log::warn!(
                        "[PaperSave] Index sync failed for file {}: {}",
                        file.id,
                        e
                    );
                }
            }
        }

        // è§¦å‘å¼‚æ­¥ PDF å¤„ç† Pipeline
        if let Some(ref pdf_service) = ctx.pdf_processing_service {
            use crate::vfs::pdf_processing_service::ProcessingStage;
            let file_id = file.id.clone();
            let service = pdf_service.clone();
            tokio::spawn(async move {
                log::info!("[PaperSave] Starting media pipeline for file: {}", file_id);
                if let Err(e) = service
                    .start_pipeline(&file_id, Some(ProcessingStage::OcrProcessing))
                    .await
                {
                    log::error!(
                        "[PaperSave] Media pipeline failed for file {}: {}",
                        file_id,
                        e
                    );
                }
            });
        }

        // â”€â”€ Stage: Done â”€â”€
        progress[idx].stage = PaperStage::Done;
        progress[idx].file_id = Some(file.id.clone());
        emit_progress(ctx, progress);

        Ok(json!({
            "success": true,
            "deduplicated": false,
            "file_id": file.id,
            "title": title,
            "file_name": file_name,
            "size_bytes": pdf_bytes.len(),
            "page_count": page_count,
            "has_text": extracted_text.is_some(),
            "message": format!("è®ºæ–‡å·²ä¿å­˜åˆ°èµ„æ–™åº“ï¼ˆ{}é¡µï¼Œæ–‡ä»¶ID: {}ï¼‰", page_count.unwrap_or(0), file.id),
        }))
    }

    /// è§£ææ‰€æœ‰å¯ç”¨çš„ PDF ä¸‹è½½æºï¼ˆæ”¯æŒå¤šæºè‡ªåŠ¨å›é€€ï¼‰
    ///
    /// è¿”å› `Vec<(url, source_label)>`ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼š
    /// - ç›´æ¥ URLï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    /// - arXiv ä¸»ç«™ + é•œåƒç«™
    /// - DOI â†’ Unpaywall å¼€æ”¾è·å–
    async fn resolve_all_pdf_urls(
        &self,
        url: Option<&str>,
        doi: Option<&str>,
        arxiv_id: Option<&str>,
    ) -> Vec<(String, String)> {
        let mut candidates: Vec<(String, String)> = Vec::new();

        // 1. ç›´æ¥ URL
        if let Some(u) = url {
            if !u.is_empty() {
                candidates.push((u.to_string(), "Direct".to_string()));
            }
        }

        // 2. arXiv ä¸»ç«™ + é•œåƒ
        if let Some(id) = arxiv_id {
            if !id.is_empty() {
                let clean_id = id
                    .trim()
                    .strip_prefix("arXiv:")
                    .or_else(|| id.strip_prefix("arxiv:"))
                    .unwrap_or(id);
                // ä¸»ç«™
                let main_url = format!("https://arxiv.org/pdf/{}", clean_id);
                if !candidates.iter().any(|(u, _)| u == &main_url) {
                    candidates.push((main_url, "arXiv".to_string()));
                }
                // é•œåƒç«™ï¼ˆexport å­åŸŸï¼Œä¸åŒ CDNï¼‰
                candidates.push((
                    format!("https://export.arxiv.org/pdf/{}", clean_id),
                    "arXiv Export".to_string(),
                ));
            }
        }

        // 3. DOI â†’ Unpaywall å¼€æ”¾è·å–
        if let Some(d) = doi {
            if !d.is_empty() {
                let clean_doi = d
                    .trim()
                    .strip_prefix("https://doi.org/")
                    .unwrap_or(d);
                match self.resolve_doi_to_pdf(clean_doi).await {
                    Ok(pdf_url) => {
                        if !candidates.iter().any(|(u, _)| u == &pdf_url) {
                            candidates.push((pdf_url, "Unpaywall".to_string()));
                        }
                    }
                    Err(e) => {
                        log::debug!("[PaperSave] Unpaywall resolve failed for DOI '{}': {}", d, e);
                    }
                }
            }
        }

        // 4. å¦‚æœç›´æ¥ URL çœ‹èµ·æ¥åƒ arXivï¼Œè¡¥å……é•œåƒæº
        if let Some(u) = url {
            if u.contains("arxiv.org/pdf/") {
                if let Some(id_part) = u.split("arxiv.org/pdf/").nth(1) {
                    let clean = id_part.trim_end_matches(".pdf").trim_end_matches('/');
                    let export_url = format!("https://export.arxiv.org/pdf/{}", clean);
                    if !candidates.iter().any(|(existing, _)| existing == &export_url) {
                        candidates.push((export_url, "arXiv Export".to_string()));
                    }
                }
            }
        }

        candidates
    }

    /// é€šè¿‡ Unpaywall API å°† DOI è§£æä¸ºå¼€æ”¾è·å– PDF URL
    async fn resolve_doi_to_pdf(&self, doi: &str) -> Result<String, String> {
        let url = format!(
            "{}/{}?email=support@deepstudent.app",
            UNPAYWALL_API_URL, doi
        );

        log::debug!("[PaperSave] Unpaywall lookup: {}", url);

        let response = self
            .unpaywall_client
            .get(&url)
            .send()
            .await
            .map_err(|e| format!("Unpaywall request failed: {}", e))?;

        if !response.status().is_success() {
            return Err(format!(
                "Unpaywall returned HTTP {} for DOI '{}'. The paper may not have an open access version.",
                response.status().as_u16(),
                doi
            ));
        }

        let body: Value = response
            .json()
            .await
            .map_err(|e| format!("Unpaywall parse failed: {}", e))?;

        // å°è¯• best_oa_location.url_for_pdf
        if let Some(pdf_url) = body
            .get("best_oa_location")
            .and_then(|loc| loc.get("url_for_pdf"))
            .and_then(|v| v.as_str())
        {
            if !pdf_url.is_empty() {
                return Ok(pdf_url.to_string());
            }
        }

        // å›é€€ï¼šéå†æ‰€æœ‰ oa_locations
        if let Some(locations) = body.get("oa_locations").and_then(|v| v.as_array()) {
            for loc in locations {
                if let Some(pdf_url) = loc.get("url_for_pdf").and_then(|v| v.as_str()) {
                    if !pdf_url.is_empty() {
                        return Ok(pdf_url.to_string());
                    }
                }
            }
        }

        Err(format!(
            "No open access PDF found for DOI '{}'. The paper may be behind a paywall.",
            doi
        ))
    }

    /// ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆå¸¦æµå¼è¿›åº¦ä¸ŠæŠ¥ï¼‰
    async fn download_pdf_with_progress(
        &self,
        url: &str,
        ctx: &ExecutionContext,
        progress: &mut Vec<PaperProgressItem>,
        idx: usize,
    ) -> Result<Vec<u8>, String> {
        // å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸ HTTPSï¼ˆé™¤ localhostï¼‰
        if !url.starts_with("https://") && !url.starts_with("http://localhost/") && !url.starts_with("http://localhost:") && url != "http://localhost" {
            return Err(format!("Only HTTPS URLs are allowed: {}", url));
        }

        let response = if let Some(cancel_token) = ctx.cancellation_token() {
            tokio::select! {
                result = self.download_client.get(url).send() => {
                    result.map_err(|e| format!("PDF download failed: {}", e))?
                }
                _ = cancel_token.cancelled() => {
                    return Err("Download cancelled".to_string());
                }
            }
        } else {
            self.download_client
                .get(url)
                .send()
                .await
                .map_err(|e| format!("PDF download failed: {}", e))?
        };

        let status = response.status();
        if !status.is_success() {
            return Err(format!(
                "PDF download returned HTTP {} from {}",
                status.as_u16(),
                url
            ));
        }

        // Content-Length é¢„é˜² OOM + ç”¨äºè¿›åº¦è®¡ç®—
        let total_size = response.content_length();
        if let Some(cl) = total_size {
            if cl as usize > MAX_PDF_SIZE {
                return Err(format!(
                    "PDF too large: {} MB (limit: {} MB)",
                    cl / (1024 * 1024),
                    MAX_PDF_SIZE / (1024 * 1024)
                ));
            }
            progress[idx].total_bytes = Some(cl);
        }

        // æµå¼è¯»å– response bodyï¼Œæ¯ PROGRESS_BYTES_INTERVAL å‘å°„ä¸€æ¬¡è¿›åº¦
        let mut buffer = Vec::with_capacity(total_size.unwrap_or(1024 * 1024) as usize);
        let mut downloaded: u64 = 0;
        let mut last_emit_at: u64 = 0;
        let mut response = response;

        loop {
            let chunk_result = if let Some(cancel_token) = ctx.cancellation_token() {
                tokio::select! {
                    result = response.chunk() => result,
                    _ = cancel_token.cancelled() => {
                        return Err("Download cancelled".to_string());
                    }
                }
            } else {
                response.chunk().await
            };

            match chunk_result {
                Ok(Some(chunk)) => {
                    downloaded += chunk.len() as u64;

                    if buffer.len() + chunk.len() > MAX_PDF_SIZE {
                        return Err(format!(
                            "PDF too large: >{} MB (limit: {} MB)",
                            MAX_PDF_SIZE / (1024 * 1024),
                            MAX_PDF_SIZE / (1024 * 1024)
                        ));
                    }

                    buffer.extend_from_slice(&chunk);

                    // èŠ‚æµè¿›åº¦å‘å°„
                    if downloaded - last_emit_at >= PROGRESS_BYTES_INTERVAL as u64 {
                        last_emit_at = downloaded;
                        progress[idx].downloaded = Some(downloaded);
                        progress[idx].percent = if let Some(total) = total_size {
                            ((downloaded as f64 / total as f64) * 100.0).min(99.0) as u8
                        } else {
                            0 // æœªçŸ¥å¤§å°æ—¶ä¸æ˜¾ç¤ºç™¾åˆ†æ¯”
                        };
                        emit_progress(ctx, progress);
                    }
                }
                Ok(None) => break, // ä¸‹è½½å®Œæˆ
                Err(e) => {
                    return Err(format!("PDF read failed: {}", e));
                }
            }
        }

        // æœ€ç»ˆè¿›åº¦
        progress[idx].downloaded = Some(downloaded);
        progress[idx].percent = 100;
        emit_progress(ctx, progress);

        // PDF ç­¾åéªŒè¯
        if buffer.len() < 4 || &buffer[..4] != b"%PDF" {
            return Err("Downloaded file is not a valid PDF (missing %PDF header)".to_string());
        }

        Ok(buffer)
    }

    // ========================================================================
    // cite_format â€” å¼•ç”¨æ ¼å¼åŒ–
    // ========================================================================

    fn execute_cite_format(&self, call: &ToolCall) -> Result<Value, String> {
        let papers = call
            .arguments
            .get("papers")
            .and_then(|v| v.as_array())
            .ok_or("Missing required parameter 'papers' (array)")?;

        let format = call
            .arguments
            .get("format")
            .and_then(|v| v.as_str())
            .unwrap_or("bibtex");

        let mut citations = Vec::new();

        for paper in papers {
            let citation = match format {
                "bibtex" => Self::format_bibtex(paper),
                "gbt7714" => Self::format_gbt7714(paper),
                "apa" => Self::format_apa(paper),
                _ => Err(format!("Unsupported format: '{}'. Use 'bibtex', 'gbt7714', or 'apa'.", format)),
            }?;
            citations.push(json!({
                "title": paper.get("title").and_then(|v| v.as_str()).unwrap_or(""),
                "citation": citation,
            }));
        }

        Ok(json!({
            "format": format,
            "count": citations.len(),
            "citations": citations,
        }))
    }

    /// æ ¼å¼åŒ–ä¸º BibTeX
    fn format_bibtex(paper: &Value) -> Result<String, String> {
        let title = paper.get("title").and_then(|v| v.as_str()).unwrap_or("Untitled");
        let year = paper.get("year").and_then(|v| v.as_u64()).unwrap_or(0);
        let doi = paper.get("doi").and_then(|v| v.as_str()).unwrap_or("");
        let venue = paper.get("venue").and_then(|v| v.as_str()).unwrap_or("");

        let authors = Self::extract_authors_list(paper);
        let author_str = authors.join(" and ");

        // ç”Ÿæˆ cite key: ç¬¬ä¸€ä½œè€…å§“ + å¹´ä»½
        let cite_key = {
            let first_author = authors.first().map(|s| s.as_str()).unwrap_or("unknown");
            let last_name = first_author.split_whitespace().last().unwrap_or("unknown");
            let clean = last_name
                .chars()
                .filter(|c| c.is_alphanumeric())
                .collect::<String>()
                .to_lowercase();
            format!("{}{}", clean, year)
        };

        let mut bib = format!("@article{{{},\n", cite_key);
        bib.push_str(&format!("  title = {{{}}},\n", title));
        if !author_str.is_empty() {
            bib.push_str(&format!("  author = {{{}}},\n", author_str));
        }
        if year > 0 {
            bib.push_str(&format!("  year = {{{}}},\n", year));
        }
        if !venue.is_empty() {
            bib.push_str(&format!("  journal = {{{}}},\n", venue));
        }
        if !doi.is_empty() {
            bib.push_str(&format!("  doi = {{{}}},\n", doi));
        }
        bib.push('}');

        Ok(bib)
    }

    /// æ ¼å¼åŒ–ä¸º GB/T 7714
    fn format_gbt7714(paper: &Value) -> Result<String, String> {
        let title = paper.get("title").and_then(|v| v.as_str()).unwrap_or("Untitled");
        let year = paper.get("year").and_then(|v| v.as_u64()).unwrap_or(0);
        let doi = paper.get("doi").and_then(|v| v.as_str()).unwrap_or("");
        let venue = paper.get("venue").and_then(|v| v.as_str()).unwrap_or("");

        let authors = Self::extract_authors_list(paper);

        // GB/T 7714 æ ¼å¼ï¼šä½œè€…. æ ‡é¢˜[J]. æœŸåˆŠ, å¹´ä»½.
        let author_str = if authors.len() > 3 {
            format!("{}, ç­‰", authors[..3].join(", "))
        } else {
            authors.join(", ")
        };

        let mut citation = String::new();
        if !author_str.is_empty() {
            citation.push_str(&author_str);
            citation.push_str(". ");
        }
        citation.push_str(title);
        citation.push_str("[J]. ");
        if !venue.is_empty() {
            citation.push_str(venue);
            citation.push_str(", ");
        }
        if year > 0 {
            citation.push_str(&format!("{}", year));
        }
        citation.push('.');
        if !doi.is_empty() {
            citation.push_str(&format!(" DOI: {}.", doi));
        }

        Ok(citation)
    }

    /// æ ¼å¼åŒ–ä¸º APA æ ¼å¼
    fn format_apa(paper: &Value) -> Result<String, String> {
        let title = paper.get("title").and_then(|v| v.as_str()).unwrap_or("Untitled");
        let year = paper.get("year").and_then(|v| v.as_u64()).unwrap_or(0);
        let doi = paper.get("doi").and_then(|v| v.as_str()).unwrap_or("");
        let venue = paper.get("venue").and_then(|v| v.as_str()).unwrap_or("");

        let authors = Self::extract_authors_list(paper);

        // APA: Last, F. M., & Last, F. M. (Year). Title. Journal. DOI
        let apa_authors: Vec<String> = authors
            .iter()
            .map(|name| {
                let parts: Vec<&str> = name.split_whitespace().collect();
                if parts.len() >= 2 {
                    let last = parts.last().unwrap();
                    let initials: String = parts[..parts.len() - 1]
                        .iter()
                        .map(|p| format!("{}.", p.chars().next().unwrap_or('?')))
                        .collect::<Vec<_>>()
                        .join(" ");
                    format!("{}, {}", last, initials)
                } else {
                    name.clone()
                }
            })
            .collect();

        let author_str = if apa_authors.len() > 7 {
            format!(
                "{}, ... {}",
                apa_authors[..6].join(", "),
                apa_authors.last().unwrap()
            )
        } else if apa_authors.len() == 2 {
            format!("{}, & {}", apa_authors[0], apa_authors[1])
        } else if apa_authors.len() > 2 {
            let last = apa_authors.last().unwrap().clone();
            let rest = apa_authors[..apa_authors.len() - 1].join(", ");
            format!("{}, & {}", rest, last)
        } else {
            apa_authors.join(", ")
        };

        let mut citation = String::new();
        if !author_str.is_empty() {
            citation.push_str(&author_str);
            citation.push(' ');
        }
        if year > 0 {
            citation.push_str(&format!("({}). ", year));
        }
        citation.push_str(title);
        citation.push('.');
        if !venue.is_empty() {
            citation.push_str(&format!(" *{}*.", venue));
        }
        if !doi.is_empty() {
            let clean_doi = doi.strip_prefix("https://doi.org/").unwrap_or(doi);
            citation.push_str(&format!(" https://doi.org/{}", clean_doi));
        }

        Ok(citation)
    }

    /// æå–ä½œè€…åˆ—è¡¨
    fn extract_authors_list(paper: &Value) -> Vec<String> {
        if let Some(arr) = paper.get("authors").and_then(|v| v.as_array()) {
            arr.iter()
                .filter_map(|a| a.as_str().map(|s| s.to_string()))
                .collect()
        } else if let Some(s) = paper.get("authors").and_then(|v| v.as_str()) {
            s.split(", ").map(|s| s.to_string()).collect()
        } else {
            vec![]
        }
    }
}

impl Default for PaperSaveExecutor {
    fn default() -> Self {
        Self::new()
    }
}

/// æ–‡ä»¶åå‡€åŒ–ï¼šç§»é™¤éæ³•å­—ç¬¦
fn sanitize_filename(name: &str) -> String {
    let sanitized: String = name
        .chars()
        .map(|c| match c {
            '/' | '\\' | ':' | '*' | '?' | '"' | '<' | '>' | '|' => '_',
            _ if c.is_control() => '_',
            _ => c,
        })
        .collect();

    // æŒ‰å­—ç¬¦æ•°æˆªæ–­ï¼ˆéå­—èŠ‚æ•°ï¼‰ï¼Œé¿å…å¤šå­—èŠ‚å­—ç¬¦è¾¹ç•Œ panic
    let max_chars = 100;
    if sanitized.chars().count() > max_chars {
        sanitized.chars().take(max_chars).collect()
    } else {
        sanitized
    }
}

#[async_trait]
impl ToolExecutor for PaperSaveExecutor {
    fn can_handle(&self, tool_name: &str) -> bool {
        let stripped = strip_tool_namespace(tool_name);
        matches!(stripped, "paper_save" | "cite_format")
    }

    async fn execute(
        &self,
        call: &ToolCall,
        ctx: &ExecutionContext,
    ) -> Result<ToolResultInfo, String> {
        let start_time = Instant::now();
        let tool_name = strip_tool_namespace(&call.name);

        log::debug!(
            "[PaperSave] Executing: {} (full: {})",
            tool_name,
            call.name
        );

        ctx.emitter.emit_tool_call_start(
            &ctx.message_id,
            &ctx.block_id,
            &call.name,
            call.arguments.clone(),
            Some(&call.id),
            None,
        );

        let result = match tool_name {
            "paper_save" => self.execute_paper_save(call, ctx).await,
            "cite_format" => self.execute_cite_format(call),
            _ => Err(format!("Unknown paper tool: {}", tool_name)),
        };

        let duration = start_time.elapsed().as_millis() as u64;

        match result {
            Ok(output) => {
                ctx.emitter.emit_end(
                    event_types::TOOL_CALL,
                    &ctx.block_id,
                    Some(json!({
                        "result": output,
                        "durationMs": duration,
                    })),
                    None,
                );

                let result = ToolResultInfo::success(
                    Some(call.id.clone()),
                    Some(ctx.block_id.clone()),
                    call.name.clone(),
                    call.arguments.clone(),
                    output,
                    duration,
                );

                if let Err(e) = ctx.save_tool_block(&result) {
                    log::warn!("[PaperSave] Failed to save tool block: {}", e);
                }

                Ok(result)
            }
            Err(e) => {
                ctx.emitter
                    .emit_error(event_types::TOOL_CALL, &ctx.block_id, &e, None);

                log::warn!(
                    "[PaperSave] Tool {} failed: {} ({}ms)",
                    call.name,
                    e,
                    duration
                );

                let result = ToolResultInfo::failure(
                    Some(call.id.clone()),
                    Some(ctx.block_id.clone()),
                    call.name.clone(),
                    call.arguments.clone(),
                    e,
                    duration,
                );

                if let Err(e) = ctx.save_tool_block(&result) {
                    log::warn!("[PaperSave] Failed to save tool block: {}", e);
                }

                Ok(result)
            }
        }
    }

    fn name(&self) -> &'static str {
        "PaperSaveExecutor"
    }

    fn sensitivity_level(&self, tool_name: &str) -> ToolSensitivity {
        let stripped = strip_tool_namespace(tool_name);
        match stripped {
            // paper_save ä¸‹è½½å¤–éƒ¨æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œæ•æ„Ÿç­‰çº§ Medium
            "paper_save" => ToolSensitivity::Medium,
            // cite_format çº¯æ ¼å¼åŒ–ï¼Œæ— å‰¯ä½œç”¨
            "cite_format" => ToolSensitivity::Low,
            _ => ToolSensitivity::Medium,
        }
    }
}

// ============================================================================
// æµ‹è¯•
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_can_handle() {
        let executor = PaperSaveExecutor::new();
        assert!(executor.can_handle("builtin-paper_save"));
        assert!(executor.can_handle("builtin-cite_format"));
        assert!(!executor.can_handle("builtin-arxiv_search"));
        assert!(!executor.can_handle("builtin-web_search"));
    }

    #[test]
    fn test_sensitivity() {
        let executor = PaperSaveExecutor::new();
        assert!(matches!(
            executor.sensitivity_level("builtin-paper_save"),
            ToolSensitivity::Medium
        ));
        assert!(matches!(
            executor.sensitivity_level("builtin-cite_format"),
            ToolSensitivity::Low
        ));
    }

    #[test]
    fn test_sanitize_filename() {
        assert_eq!(
            sanitize_filename("A Survey of Transformers"),
            "A Survey of Transformers"
        );
        assert_eq!(
            sanitize_filename("What/Why:How?"),
            "What_Why_How_"
        );
        assert_eq!(
            sanitize_filename("Test <file> \"name\""),
            "Test _file_ _name_"
        );
    }

    #[test]
    fn test_sanitize_filename_chinese_truncation() {
        // 101 ä¸ªä¸­æ–‡å­—ç¬¦ â†’ æˆªæ–­ä¸º 100 ä¸ªå­—ç¬¦ï¼ˆä¸ä¼šåœ¨å¤šå­—èŠ‚è¾¹ç•Œ panicï¼‰
        let long_chinese = "åŸº".repeat(101);
        let result = sanitize_filename(&long_chinese);
        assert_eq!(result.chars().count(), 100);
        // ç¡®ä¿ç»“æœä»æ˜¯æœ‰æ•ˆ UTF-8ï¼ˆä¸ä¼šæˆªæ–­åˆ°å­—èŠ‚è¾¹ç•Œï¼‰
        assert!(result.is_ascii() || result.len() > 100);
    }

    #[test]
    fn test_format_bibtex() {
        let paper = json!({
            "title": "Attention Is All You Need",
            "authors": ["Ashish Vaswani", "Noam Shazeer"],
            "year": 2017,
            "venue": "NeurIPS",
            "doi": "10.5555/3295222.3295349",
        });
        let bib = PaperSaveExecutor::format_bibtex(&paper).unwrap();
        assert!(bib.contains("@article{vaswani2017"));
        assert!(bib.contains("Attention Is All You Need"));
        assert!(bib.contains("Ashish Vaswani and Noam Shazeer"));
    }

    #[test]
    fn test_format_gbt7714() {
        let paper = json!({
            "title": "Attention Is All You Need",
            "authors": ["Ashish Vaswani", "Noam Shazeer"],
            "year": 2017,
            "venue": "NeurIPS",
        });
        let citation = PaperSaveExecutor::format_gbt7714(&paper).unwrap();
        assert!(citation.contains("Ashish Vaswani, Noam Shazeer."));
        assert!(citation.contains("Attention Is All You Need[J]."));
        assert!(citation.contains("NeurIPS, 2017."));
    }

    #[test]
    fn test_format_apa() {
        let paper = json!({
            "title": "Attention Is All You Need",
            "authors": ["Ashish Vaswani", "Noam Shazeer"],
            "year": 2017,
            "venue": "NeurIPS",
            "doi": "10.5555/3295222.3295349",
        });
        let citation = PaperSaveExecutor::format_apa(&paper).unwrap();
        assert!(citation.contains("Vaswani, A."));
        assert!(citation.contains("(2017)"));
        assert!(citation.contains("*NeurIPS*"));
    }

    #[test]
    fn test_format_gbt7714_many_authors() {
        let paper = json!({
            "title": "Test Paper",
            "authors": ["Author A", "Author B", "Author C", "Author D", "Author E"],
            "year": 2024,
        });
        let citation = PaperSaveExecutor::format_gbt7714(&paper).unwrap();
        // GB/T 7714: >3 authors â†’ å‰3 + "ç­‰"
        assert!(citation.contains("Author A, Author B, Author C, ç­‰."));
    }
}
