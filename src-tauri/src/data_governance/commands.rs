//! # æ•°æ®æ²»ç† Tauri å‘½ä»¤
//!
//! å®šä¹‰æ•°æ®æ²»ç†ç³»ç»Ÿæš´éœ²ç»™å‰ç«¯çš„ Tauri å‘½ä»¤ã€‚
//!
//! ## å‘½ä»¤åˆ—è¡¨
//!
//! - `data_governance_get_schema_registry`: è·å– Schema æ³¨å†Œè¡¨
//! - `data_governance_get_audit_logs`: è·å–å®¡è®¡æ—¥å¿—
//! - `data_governance_get_migration_status`: è·å–è¿ç§»çŠ¶æ€
//! - `data_governance_run_health_check`: è¿è¡Œå¥åº·æ£€æŸ¥
//! - `data_governance_run_backup`: å¼‚æ­¥åå°å¤‡ä»½ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
//! - `data_governance_backup_tiered`: å¼‚æ­¥åˆ†å±‚å¤‡ä»½ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
//! - `data_governance_backup_and_export_zip`: ä¸€æ­¥å®Œæˆå¤‡ä»½å¹¶å¯¼å‡º ZIP
//! - `data_governance_export_zip`: å¼‚æ­¥ ZIP å¯¼å‡ºï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
//! - `data_governance_import_zip`: å¼‚æ­¥ ZIP å¯¼å…¥ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
//! - `data_governance_restore_backup`: å¼‚æ­¥å¤‡ä»½æ¢å¤ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
//! - `data_governance_cancel_backup`: å–æ¶ˆå¤‡ä»½ä»»åŠ¡
//! - `data_governance_get_backup_job`: è·å–å¤‡ä»½ä»»åŠ¡çŠ¶æ€
//! - `data_governance_list_backup_jobs`: è·å–æ‰€æœ‰å¤‡ä»½ä»»åŠ¡åˆ—è¡¨

use std::path::Path;
use std::sync::{Arc, RwLock};
use tauri::{AppHandle, Manager, State};

#[cfg(feature = "data_governance")]
use super::audit::{AuditFilter, AuditLog, AuditOperation, AuditRepository, AuditStatus};
use super::migration::{get_migration_set, MigrationCoordinator};
use super::schema_registry::{DatabaseId, DatabaseStatus, SchemaRegistry};
use crate::backup_common::{log_and_skip_entry_err, BACKUP_GLOBAL_LIMITER};
use crate::backup_job_manager::{
    BackupJobContext, BackupJobKind, BackupJobManager, BackupJobManagerState, BackupJobParams,
    BackupJobPhase, BackupJobResultPayload, BackupJobStatus, BackupJobSummary, PersistedJob,
};
use crate::utils::text::safe_truncate_chars;

fn resolve_target_and_pending(
    id: &DatabaseId,
    current_version: u32,
    status: Option<&DatabaseStatus>,
) -> (u32, usize) {
    let migration_set = get_migration_set(id.as_str());
    let target_version = status
        .map(|s| s.max_compatible_version)
        .or_else(|| migration_set.map(|set| set.latest_version() as u32))
        .unwrap_or(0);
    let pending_count = migration_set
        .map(|set| set.pending(current_version as i32).count())
        .unwrap_or(0);
    (target_version, pending_count)
}

/// æŒä¹…åŒ–è¿ç§»é”™è¯¯æ–‡ä»¶å
const MIGRATION_ERROR_FILE: &str = ".last_migration_error";

/// å°†çœŸå®çš„è¿ç§»é”™è¯¯æŒä¹…åŒ–åˆ°æ–‡ä»¶
///
/// è¿ç§»å¤±è´¥æ—¶ç”± lib.rs è°ƒç”¨ï¼Œå°†å®é™…çš„ SQL é”™è¯¯ä¿¡æ¯å†™å…¥æ–‡ä»¶ï¼Œ
/// ä¾›åç»­ `get_migration_status` å’Œè¯Šæ–­æŠ¥å‘Šè¯»å–ã€‚
pub fn persist_migration_error(app_data_dir: &Path, error: &str) {
    let error_file = app_data_dir.join(MIGRATION_ERROR_FILE);
    let payload = serde_json::json!({
        "error": error,
        "timestamp": chrono::Utc::now().to_rfc3339(),
    });
    if let Err(e) = std::fs::write(&error_file, payload.to_string()) {
        tracing::warn!(
            path = %error_file.display(),
            error = %e,
            "Failed to persist migration error to file"
        );
    }
}

/// è¿ç§»æˆåŠŸæ—¶æ¸…é™¤æŒä¹…åŒ–çš„é”™è¯¯æ–‡ä»¶
pub fn clear_migration_error(app_data_dir: &Path) {
    let error_file = app_data_dir.join(MIGRATION_ERROR_FILE);
    if error_file.exists() {
        let _ = std::fs::remove_file(&error_file);
    }
}

/// è¯»å–æŒä¹…åŒ–çš„è¿ç§»é”™è¯¯
fn read_persisted_migration_error(app_data_dir: &Path) -> Option<(String, String)> {
    let error_file = app_data_dir.join(MIGRATION_ERROR_FILE);
    let content = std::fs::read_to_string(&error_file).ok()?;
    let parsed: serde_json::Value = serde_json::from_str(&content).ok()?;
    let error = parsed.get("error")?.as_str()?.to_string();
    let timestamp = parsed.get("timestamp")?.as_str()?.to_string();
    Some((error, timestamp))
}

fn get_live_app_data_dir(app: &tauri::AppHandle) -> Result<std::path::PathBuf, String> {
    if let Some(state) = app.try_state::<crate::commands::AppState>() {
        return Ok(state.file_manager.get_writable_app_data_dir());
    }

    get_app_data_dir(app)
}

/// æ£€æŸ¥ä¸»æ•°æ®åº“æ˜¯å¦å¤„äºç»´æŠ¤æ¨¡å¼ã€‚
///
/// å½“å¤‡ä»½/æ¢å¤/æ•°æ®è¿ç§»ç­‰æ•°æ®æ²»ç†æ“ä½œæ­£åœ¨è¿›è¡Œæ—¶ï¼Œ
/// åŒæ­¥å‘½ä»¤ä¸åº”è®¿é—®æ•°æ®åº“æ–‡ä»¶ï¼Œå¦åˆ™ä¼šç»•è¿‡ç»´æŠ¤æ¨¡å¼é€ æˆæ•°æ®ä¸ä¸€è‡´ã€‚
fn check_maintenance_mode(app: &tauri::AppHandle) -> Result<(), String> {
    if let Some(state) = app.try_state::<crate::commands::AppState>() {
        if state.database.is_in_maintenance_mode() {
            return Err("æ•°æ®æ²»ç†æ“ä½œæ­£åœ¨è¿›è¡Œï¼ˆç»´æŠ¤æ¨¡å¼ï¼‰ï¼Œè¯·ç¨åå†è¯•ã€‚".to_string());
        }
    }
    Ok(())
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct AuditHealthSnapshot {
    pub is_healthy: bool,
    pub last_error: Option<String>,
    pub last_error_at: Option<String>,
}

#[derive(Debug, Clone)]
struct AuditHealthError {
    message: String,
    occurred_at: String,
}

#[derive(Default)]
pub struct AuditHealthState {
    last_error: std::sync::Mutex<Option<AuditHealthError>>,
}

impl AuditHealthSnapshot {
    fn healthy() -> Self {
        Self {
            is_healthy: true,
            last_error: None,
            last_error_at: None,
        }
    }
}

impl AuditHealthState {
    pub fn record_success(&self) {
        let mut guard = self.last_error.lock().ok();
        if let Some(ref mut slot) = guard {
            **slot = None;
        }
    }

    pub fn record_failure(&self, message: impl Into<String>) {
        let mut guard = self.last_error.lock().ok();
        let Some(ref mut slot) = guard else {
            return;
        };
        **slot = Some(AuditHealthError {
            message: message.into(),
            occurred_at: chrono::Utc::now().to_rfc3339(),
        });
    }

    pub fn snapshot(&self) -> AuditHealthSnapshot {
        let guard = self.last_error.lock().ok();
        match guard.as_deref() {
            Some(Some(err)) => AuditHealthSnapshot {
                is_healthy: false,
                last_error: Some(err.message.clone()),
                last_error_at: Some(err.occurred_at.clone()),
            },
            _ => AuditHealthSnapshot::healthy(),
        }
    }
}

/// åŒæ­¥å‘½ä»¤è·å–å…¨å±€é”çš„é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆ60 ç§’ï¼‰
const SYNC_LOCK_TIMEOUT_SECS: u64 = 60;

fn refresh_schema_registry_from_dir(
    app_data_dir: &Path,
    registry_state: &Arc<RwLock<SchemaRegistry>>,
) -> Result<SchemaRegistry, String> {
    let latest_registry = super::init::get_current_schema_state(app_data_dir).map_err(|e| {
        tracing::error!(
            "[data_governance] åˆ·æ–° SchemaRegistry å¤±è´¥ ({}): {}",
            app_data_dir.display(),
            e
        );
        format!(
            "åˆ·æ–° SchemaRegistry å¤±è´¥ ({}): {}",
            sanitize_path_for_user(app_data_dir),
            e
        )
    })?;

    let mut guard = registry_state
        .write()
        .map_err(|e| format!("å†™å…¥ SchemaRegistry çŠ¶æ€å¤±è´¥: {}", e))?;
    *guard = latest_registry.clone();

    Ok(latest_registry)
}

fn refresh_schema_registry_from_live_state(
    app: &tauri::AppHandle,
    registry_state: &Arc<RwLock<SchemaRegistry>>,
) -> Result<SchemaRegistry, String> {
    let app_data_dir = get_live_app_data_dir(app)?;
    refresh_schema_registry_from_dir(&app_data_dir, registry_state)
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct SlotMigrationTestResponse {
    pub success: bool,
    pub report: String,
}

fn slot_c_test_dir(app_data_dir: &Path) -> std::path::PathBuf {
    app_data_dir.parent().unwrap_or(app_data_dir).join("slotC")
}

fn slot_d_test_dir(app_data_dir: &Path) -> std::path::PathBuf {
    app_data_dir.parent().unwrap_or(app_data_dir).join("slotD")
}

fn run_slot_c_empty_db_test(app_data_dir: &Path) -> SlotMigrationTestResponse {
    use std::fmt::Write;

    let slot_c_dir = slot_c_test_dir(app_data_dir);
    let mut report = String::new();
    let mut success = false;

    if slot_c_dir.exists() {
        let _ = std::fs::remove_dir_all(&slot_c_dir);
    }
    let _ = std::fs::create_dir_all(&slot_c_dir);

    let mut coordinator = MigrationCoordinator::new(slot_c_dir.clone()).with_audit_db(None);

    match coordinator.run_all() {
        Ok(migration_report) => {
            success = true;
            let _ = writeln!(
                report,
                "ç»“æœ: æˆåŠŸ ({}ms)",
                migration_report.total_duration_ms
            );
            for db_report in &migration_report.databases {
                let _ = writeln!(
                    report,
                    "  [{}] v{} -> v{}, åº”ç”¨ {} ä¸ªè¿ç§», {}ms",
                    db_report.id.as_str(),
                    db_report.from_version,
                    db_report.to_version,
                    db_report.applied_count,
                    db_report.duration_ms
                );
            }
        }
        Err(e) => {
            let _ = writeln!(report, "ç»“æœ: å¤±è´¥!");
            let _ = writeln!(report, "  ROOT CAUSE: {}", e);
        }
    }

    let _ = std::fs::remove_dir_all(&slot_c_dir);
    let _ = std::fs::create_dir_all(&slot_c_dir);

    SlotMigrationTestResponse { success, report }
}

fn run_slot_d_clone_db_test(app_data_dir: &Path) -> SlotMigrationTestResponse {
    use std::fmt::Write;

    let slot_d_dir = slot_d_test_dir(app_data_dir);
    let mut report = String::new();
    let mut success = false;

    if slot_d_dir.exists() {
        let _ = std::fs::remove_dir_all(&slot_d_dir);
    }
    let _ = std::fs::create_dir_all(&slot_d_dir);

    // å¤åˆ¶å½“å‰æ´»è·ƒæ’æ§½çš„æ•°æ®åº“æ–‡ä»¶ï¼ˆåªå¤åˆ¶ .db å’Œ .db-walï¼Œä¸å¤åˆ¶å¤§æ–‡ä»¶ï¼‰
    let db_files: &[&str] = &[
        "chat_v2.db",
        "chat_v2.db-wal",
        "mistakes.db",
        "mistakes.db-wal",
        "llm_usage.db",
        "llm_usage.db-wal",
    ];
    let db_subdir_files: &[(&str, &str)] = &[("databases", "vfs.db"), ("databases", "vfs.db-wal")];

    let mut copy_errors: Vec<String> = Vec::new();

    for file_name in db_files {
        let src = app_data_dir.join(file_name);
        if src.exists() {
            let dst = slot_d_dir.join(file_name);
            if let Err(e) = std::fs::copy(&src, &dst) {
                copy_errors.push(format!("{}: {}", file_name, e));
            }
        }
    }

    for (subdir, file_name) in db_subdir_files {
        let src = app_data_dir.join(subdir).join(file_name);
        if src.exists() {
            let dst_dir = slot_d_dir.join(subdir);
            let _ = std::fs::create_dir_all(&dst_dir);
            let dst = dst_dir.join(file_name);
            if let Err(e) = std::fs::copy(&src, &dst) {
                copy_errors.push(format!("{}/{}: {}", subdir, file_name, e));
            }
        }
    }

    if !copy_errors.is_empty() {
        let _ = writeln!(report, "å¤åˆ¶æ–‡ä»¶æ—¶å‡ºé”™: {}", copy_errors.join("; "));
    }

    let mut coordinator = MigrationCoordinator::new(slot_d_dir.clone()).with_audit_db(None);

    match coordinator.run_all() {
        Ok(migration_report) => {
            success = true;
            let _ = writeln!(
                report,
                "ç»“æœ: æˆåŠŸ ({}ms)",
                migration_report.total_duration_ms
            );
            for db_report in &migration_report.databases {
                if db_report.applied_count > 0 {
                    let _ = writeln!(
                        report,
                        "  [{}] v{} -> v{}, åº”ç”¨ {} ä¸ªè¿ç§», {}ms",
                        db_report.id.as_str(),
                        db_report.from_version,
                        db_report.to_version,
                        db_report.applied_count,
                        db_report.duration_ms
                    );
                } else {
                    let _ = writeln!(
                        report,
                        "  [{}] v{} (å·²æ˜¯æœ€æ–°)",
                        db_report.id.as_str(),
                        db_report.to_version
                    );
                }
            }
        }
        Err(e) => {
            let _ = writeln!(report, "ç»“æœ: å¤±è´¥!");
            let _ = writeln!(report, "  ROOT CAUSE: {}", e);
        }
    }

    let _ = std::fs::remove_dir_all(&slot_d_dir);
    let _ = std::fs::create_dir_all(&slot_d_dir);

    SlotMigrationTestResponse { success, report }
}

#[cfg(feature = "data_governance")]
fn try_save_audit_log(app: &tauri::AppHandle, log: AuditLog) {
    // å®¡è®¡å¤±è´¥ä¸åº”é˜»æ–­ä¸»æµç¨‹ï¼šè¿™é‡Œåªåš best-effort è®°å½•ï¼Œå¹¶å†™å…¥ tracing warnã€‚
    let audit_health = app.try_state::<Arc<AuditHealthState>>();
    let Some(audit_db) = app.try_state::<Arc<super::audit::AuditDatabase>>() else {
        if let Some(state) = audit_health {
            state.record_failure("å®¡è®¡æ•°æ®åº“æœªåˆå§‹åŒ–");
        }
        return;
    };

    let conn = match audit_db.get_conn() {
        Ok(c) => c,
        Err(e) => {
            tracing::warn!("[data_governance] è·å–å®¡è®¡æ•°æ®åº“è¿æ¥å¤±è´¥: {}", e);
            if let Some(state) = audit_health {
                state.record_failure(format!("è·å–å®¡è®¡æ•°æ®åº“è¿æ¥å¤±è´¥: {}", e));
            }
            return;
        }
    };

    if let Err(e) = AuditRepository::init(&conn) {
        tracing::warn!("[data_governance] åˆå§‹åŒ–å®¡è®¡è¡¨å¤±è´¥ï¼Œè·³è¿‡å®¡è®¡è®°å½•: {}", e);
        if let Some(state) = audit_health {
            state.record_failure(format!("åˆå§‹åŒ–å®¡è®¡è¡¨å¤±è´¥: {}", e));
        }
        return;
    }

    if let Err(e) = AuditRepository::save(&conn, &log) {
        tracing::warn!("[data_governance] å†™å…¥å®¡è®¡æ—¥å¿—å¤±è´¥: {}", e);
        if let Some(state) = audit_health {
            state.record_failure(format!("å†™å…¥å®¡è®¡æ—¥å¿—å¤±è´¥: {}", e));
        }
    } else if let Some(state) = audit_health {
        state.record_success();
    }
}

/// æŸ¥è¯¢å½“å‰æ˜¯å¦å¤„äºç»´æŠ¤æ¨¡å¼
///
/// å‰ç«¯åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨æ­¤å‘½ä»¤ï¼Œå°†åç«¯ç»´æŠ¤æ¨¡å¼çŠ¶æ€åŒæ­¥åˆ°å‰ç«¯ storeã€‚
/// ç”¨äºå¤„ç†åº”ç”¨åœ¨ç»´æŠ¤æ¨¡å¼ä¸­å´©æºƒåé‡å¯çš„åœºæ™¯ã€‚
#[tauri::command]
pub fn data_governance_get_maintenance_status(
    app: AppHandle,
) -> Result<MaintenanceStatusResponse, String> {
    let in_maintenance = if let Some(state) = app.try_state::<crate::commands::AppState>() {
        state.database.is_in_maintenance_mode()
    } else {
        false
    };

    Ok(MaintenanceStatusResponse {
        is_in_maintenance_mode: in_maintenance,
    })
}

/// è·å– Schema æ³¨å†Œè¡¨
///
/// è¿”å›æ‰€æœ‰æ•°æ®åº“çš„ç‰ˆæœ¬çŠ¶æ€å’Œè¿ç§»å†å²ã€‚
#[tauri::command]
pub fn data_governance_get_schema_registry(
    app: AppHandle,
    registry: State<'_, Arc<RwLock<SchemaRegistry>>>,
) -> Result<SchemaRegistryResponse, String> {
    let registry = refresh_schema_registry_from_live_state(&app, registry.inner())?;

    Ok(SchemaRegistryResponse {
        global_version: registry.global_version,
        aggregated_at: registry.aggregated_at.clone(),
        databases: registry
            .databases
            .iter()
            .map(|(id, status)| DatabaseStatusResponse {
                id: id.as_str().to_string(),
                schema_version: status.schema_version,
                min_compatible_version: status.min_compatible_version,
                max_compatible_version: status.max_compatible_version,
                data_contract_version: status.data_contract_version.clone(),
                migration_count: status.migration_history.len(),
                checksum: status.checksum.clone(),
                updated_at: status.updated_at.clone(),
            })
            .collect(),
    })
}

/// è·å–å®¡è®¡æ—¥å¿—
///
/// æ”¯æŒæŒ‰æ“ä½œç±»å‹ã€æ—¶é—´èŒƒå›´ã€çŠ¶æ€è¿‡æ»¤ï¼Œæ”¯æŒåˆ†é¡µã€‚
#[tauri::command]
pub fn data_governance_get_audit_logs(
    audit_db: State<'_, Arc<super::audit::AuditDatabase>>,
    operation_type: Option<String>,
    status: Option<String>,
    limit: Option<usize>,
    offset: Option<usize>,
) -> Result<AuditLogPagedResponse, String> {
    // ä»å®¡è®¡æ•°æ®åº“è·å–è¿æ¥
    let conn = audit_db
        .get_conn()
        .map_err(|e| format!("è·å–å®¡è®¡æ•°æ®åº“è¿æ¥å¤±è´¥: {}", e))?;

    let parsed_status = match status.as_deref() {
        Some("Started") => Some(AuditStatus::Started),
        Some("Completed") => Some(AuditStatus::Completed),
        Some("Failed") => Some(AuditStatus::Failed),
        Some("Partial") => Some(AuditStatus::Partial),
        Some(other) => {
            return Err(format!(
                "æ— æ•ˆçš„çŠ¶æ€è¿‡æ»¤å€¼: {}ã€‚å¯é€‰å€¼: Completed, Failed, Partial",
                other
            ))
        }
        None => None,
    };

    // æ„å»ºè¿‡æ»¤å™¨
    let filter = AuditFilter {
        operation_type,
        status: parsed_status,
        limit: Some(limit.unwrap_or(100)),
        offset,
        ..Default::default()
    };

    // åˆ†é¡µæŸ¥è¯¢å®¡è®¡æ—¥å¿—
    let result = AuditRepository::query_paged(&conn, filter)
        .map_err(|e| format!("æŸ¥è¯¢å®¡è®¡æ—¥å¿—å¤±è´¥: {}", e))?;

    Ok(AuditLogPagedResponse {
        logs: result
            .logs
            .into_iter()
            .map(AuditLogResponse::from)
            .collect(),
        total: result.total,
    })
}

/// æ¸…ç†å®¡è®¡æ—¥å¿—
///
/// æ”¯æŒä¸¤ç§æ¸…ç†ç­–ç•¥ï¼š
/// - `keep_recent`: ä¿ç•™æœ€è¿‘ N æ¡è®°å½•ï¼Œåˆ é™¤å…¶ä½™ï¼ˆæœ€å°‘ä¿ç•™ 100 æ¡ï¼‰
/// - `before_days`: åˆ é™¤ N å¤©ä¹‹å‰çš„è®°å½•ï¼ˆæœ€å°‘ä¿ç•™ 7 å¤©ï¼‰
///
/// ä¸¤ä¸ªå‚æ•°äº’æ–¥ï¼Œä¼˜å…ˆä½¿ç”¨ `keep_recent`ã€‚
/// å¦‚æœéƒ½æœªæŒ‡å®šï¼Œé»˜è®¤æ¸…ç† 90 å¤©ä¹‹å‰çš„è®°å½•ã€‚
///
/// ## å®‰å…¨æœºåˆ¶
///
/// - æœ€å°ä¿ç•™ä¸‹é™ï¼š`keep_recent` ä¸å¾—ä½äº 100 æ¡ï¼Œ`before_days` ä¸å¾—ä½äº 7 å¤©
/// - éœ€è¦ `confirmation_token` å‚æ•°ï¼Œæ ¼å¼ä¸º `AUDIT_CLEANUP_{unix_timestamp_secs}`ï¼Œ
///   ä¸”æ—¶é—´æˆ³å¿…é¡»åœ¨å½“å‰æ—¶é—´ 60 ç§’å†…ï¼Œé˜²æ­¢è¢«æ¶æ„è„šæœ¬é™é»˜è°ƒç”¨
/// - æ¯æ¬¡æ¸…ç†æ“ä½œæœ¬èº«ä¹Ÿä¼šè¢«è®°å½•åˆ°å®¡è®¡æ—¥å¿—ä¸­
///
/// ## è¿”å›
///
/// è¢«åˆ é™¤çš„è®°å½•æ•°é‡
#[tauri::command]
pub fn data_governance_cleanup_audit_logs(
    app: tauri::AppHandle,
    audit_db: State<'_, Arc<super::audit::AuditDatabase>>,
    keep_recent: Option<usize>,
    before_days: Option<u64>,
    confirmation_token: String,
) -> Result<u64, String> {
    // â”€â”€ å®‰å…¨éªŒè¯ï¼šç¡®è®¤ä»¤ç‰Œ â”€â”€
    const TOKEN_PREFIX: &str = "AUDIT_CLEANUP_";
    const TOKEN_VALIDITY_SECS: i64 = 60;

    if !confirmation_token.starts_with(TOKEN_PREFIX) {
        return Err("å®¡è®¡æ¸…ç†ä»¤ç‰Œæ ¼å¼æ— æ•ˆï¼Œéœ€è¦ AUDIT_CLEANUP_{unix_timestamp}".to_string());
    }
    let ts_str = &confirmation_token[TOKEN_PREFIX.len()..];
    let token_ts: i64 = ts_str
        .parse()
        .map_err(|_| "å®¡è®¡æ¸…ç†ä»¤ç‰Œä¸­çš„æ—¶é—´æˆ³æ— æ•ˆ".to_string())?;
    let now_ts = chrono::Utc::now().timestamp();
    let diff = (now_ts - token_ts).abs();
    if diff > TOKEN_VALIDITY_SECS {
        return Err(format!(
            "å®¡è®¡æ¸…ç†ä»¤ç‰Œå·²è¿‡æœŸï¼ˆå·®å€¼ {}sï¼Œå…è®¸ {}s å†…ï¼‰",
            diff, TOKEN_VALIDITY_SECS
        ));
    }

    // â”€â”€ å®‰å…¨éªŒè¯ï¼šæœ€å°ä¿ç•™ä¸‹é™ â”€â”€
    const MIN_KEEP_RECENT: usize = 100;
    const MIN_BEFORE_DAYS: u64 = 7;

    if let Some(keep) = keep_recent {
        if keep < MIN_KEEP_RECENT {
            return Err(format!(
                "keep_recent ä¸å¾—ä½äº {}ï¼Œå½“å‰å€¼: {}",
                MIN_KEEP_RECENT, keep
            ));
        }
    }
    if let Some(days) = before_days {
        if days < MIN_BEFORE_DAYS {
            return Err(format!(
                "before_days ä¸å¾—ä½äº {} å¤©ï¼Œå½“å‰å€¼: {}",
                MIN_BEFORE_DAYS, days
            ));
        }
    }

    let conn = audit_db
        .get_conn()
        .map_err(|e| format!("è·å–å®¡è®¡æ•°æ®åº“è¿æ¥å¤±è´¥: {}", e))?;

    // â”€â”€ æ¸…ç†å‰å…ˆè®°å½•å®¡è®¡æ—¥å¿— â”€â”€
    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Maintenance {
                    action: "cleanup_audit_logs".to_string(),
                },
                "cleanup_audit_logs_initiated".to_string(),
            )
            .with_details(serde_json::json!({
                "keep_recent": keep_recent,
                "before_days": before_days,
                "confirmation_token_ts": token_ts,
            }))
            .complete(0),
        );
    }

    // é»˜è®¤ä¿ç•™ 90 å¤©
    const DEFAULT_MAX_AGE_DAYS: u32 = 90;

    let deleted = if let Some(keep) = keep_recent {
        AuditRepository::cleanup_keep_recent(&conn, keep).map_err(|e| {
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Maintenance {
                            action: "cleanup_audit_logs".to_string(),
                        },
                        "cleanup_audit_logs".to_string(),
                    )
                    .fail(e.to_string()),
                );
            }
            format!("æ¸…ç†å®¡è®¡æ—¥å¿—å¤±è´¥: {}", e)
        })?
    } else {
        let days = before_days.unwrap_or(DEFAULT_MAX_AGE_DAYS as u64);
        AuditRepository::cleanup_old_entries(&conn, days as u32).map_err(|e| {
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Maintenance {
                            action: "cleanup_audit_logs".to_string(),
                        },
                        "cleanup_audit_logs".to_string(),
                    )
                    .fail(e.to_string()),
                );
            }
            format!("æ¸…ç†å®¡è®¡æ—¥å¿—å¤±è´¥: {}", e)
        })?
    };

    tracing::info!(deleted = deleted, "å®¡è®¡æ—¥å¿—æ¸…ç†å®Œæˆ");

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Maintenance {
                    action: "cleanup_audit_logs".to_string(),
                },
                "cleanup_audit_logs".to_string(),
            )
            .with_details(serde_json::json!({
                "deleted_count": deleted,
                "keep_recent": keep_recent,
                "before_days": before_days,
            }))
            .complete(0),
        );
    }

    Ok(deleted)
}

/// è·å–è¿ç§»çŠ¶æ€æ‘˜è¦
///
/// è¿”å›å„æ•°æ®åº“çš„å½“å‰ç‰ˆæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¾…æ‰§è¡Œè¿ç§»æ•°é‡ã€‚
#[tauri::command]
pub fn data_governance_get_migration_status(
    app_handle: AppHandle,
    registry: State<'_, Arc<RwLock<SchemaRegistry>>>,
) -> Result<MigrationStatusResponse, String> {
    use tracing::{debug, warn};

    let registry = refresh_schema_registry_from_live_state(&app_handle, registry.inner())?;

    let mut pending_total = 0;
    let mut last_error: Option<String> = None;

    let databases: Vec<_> = DatabaseId::all_ordered()
        .into_iter()
        .map(|id| {
            let status = registry.get_status(&id);
            let current_version = status.map(|s| s.schema_version).unwrap_or(0);
            let (target_version, pending_count) =
                resolve_target_and_pending(&id, current_version, status);
            pending_total += pending_count;

            // æ£€æµ‹è¿ç§»å¤±è´¥ï¼šæ•°æ®åº“å·²åˆå§‹åŒ–ä½†æœ‰å¾…æ‰§è¡Œè¿ç§»
            if pending_count > 0 && current_version > 0 {
                let msg = format!(
                    "{} æœ‰ {} ä¸ªè¿ç§»æœªæ‰§è¡Œ (å½“å‰: v{}, ç›®æ ‡: v{})",
                    id.as_str(),
                    pending_count,
                    current_version,
                    target_version
                );
                warn!("âš ï¸ [MigrationStatus] {}", msg);
                if last_error.is_none() {
                    last_error = Some(msg);
                }
            }

            MigrationDatabaseStatus {
                id: id.as_str().to_string(),
                current_version,
                target_version,
                is_initialized: current_version > 0,
                last_migration_at: status.and_then(|s| {
                    if s.updated_at.is_empty() {
                        None
                    } else {
                        Some(s.updated_at.clone())
                    }
                }),
                pending_count,
                has_pending: pending_count > 0,
            }
        })
        .collect();

    let all_healthy = databases.iter().all(|d| d.is_initialized && !d.has_pending);

    // ä¼˜å…ˆä½¿ç”¨æŒä¹…åŒ–çš„çœŸå®è¿ç§»é”™è¯¯ï¼ˆæ¥è‡ªå®é™… SQL æ‰§è¡Œå¤±è´¥ï¼‰ï¼Œ
    // è€Œéä»…é ç‰ˆæœ¬å·æ¯”è¾ƒç”Ÿæˆçš„"æœ‰Nä¸ªè¿ç§»æœªæ‰§è¡Œ"ä¼ªä¿¡æ¯
    if pending_total > 0 {
        if let Ok(app_data_dir) = get_live_app_data_dir(&app_handle) {
            if let Some((real_error, _ts)) = read_persisted_migration_error(&app_data_dir) {
                last_error = Some(real_error);
            }
        }
    }

    debug!(
        "ğŸ“Š [MigrationStatus] å…¨å±€ç‰ˆæœ¬={}, å¥åº·={}, å¾…æ‰§è¡Œè¿ç§»æ€»æ•°={}",
        registry.global_version, all_healthy, pending_total
    );

    Ok(MigrationStatusResponse {
        global_version: registry.global_version,
        all_healthy,
        databases,
        pending_migrations_total: pending_total,
        has_pending_migrations: pending_total > 0,
        last_error,
    })
}

/// è¿è¡Œå¥åº·æ£€æŸ¥
///
/// æ£€æŸ¥æ‰€æœ‰æ•°æ®åº“çš„å®Œæ•´æ€§å’Œä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬å¾…æ‰§è¡Œè¿ç§»æ£€æµ‹ã€‚
#[tauri::command]
pub fn data_governance_run_health_check(
    app: AppHandle,
    registry: State<'_, Arc<RwLock<SchemaRegistry>>>,
) -> Result<HealthCheckResponse, String> {
    use tracing::{info, warn};

    info!("ğŸ” [HealthCheck] å¼€å§‹è¿è¡Œå¥åº·æ£€æŸ¥...");
    let registry = refresh_schema_registry_from_live_state(&app, registry.inner())?;

    // æ£€æŸ¥ä¾èµ–å…³ç³»
    let dependency_check = registry.check_dependencies();
    let dependency_ok = dependency_check.is_ok();
    let dependency_error = dependency_check.err().map(|e| e.to_string());

    if let Some(ref err) = dependency_error {
        warn!("âš ï¸ [HealthCheck] ä¾èµ–å…³ç³»æ£€æŸ¥å¤±è´¥: {}", err);
    }

    // ç»Ÿè®¡å„çŠ¶æ€æ•°æ®åº“æ•°é‡
    let total_databases = DatabaseId::all_ordered().len();
    let initialized_count = registry
        .databases
        .values()
        .filter(|s| s.schema_version > 0)
        .count();
    let uninitialized_count = total_databases - initialized_count;

    info!(
        "ğŸ“Š [HealthCheck] æ•°æ®åº“ç»Ÿè®¡: æ€»æ•°={}, å·²åˆå§‹åŒ–={}, æœªåˆå§‹åŒ–={}",
        total_databases, initialized_count, uninitialized_count
    );

    let mut pending_migrations_total = 0;

    // æ„å»ºæ¯ä¸ªæ•°æ®åº“çš„å¥åº·çŠ¶æ€
    let database_health: Vec<_> = DatabaseId::all_ordered()
        .into_iter()
        .map(|id| {
            let status = registry.get_status(&id);
            let schema_version = status.map(|s| s.schema_version).unwrap_or(0);
            let (target_version, pending_count) =
                resolve_target_and_pending(&id, schema_version, status);
            let is_initialized = schema_version > 0;
            pending_migrations_total += pending_count;

            // æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
            let dependencies_met = id.dependencies().iter().all(|dep| {
                registry
                    .get_status(dep)
                    .map(|s| s.schema_version > 0)
                    .unwrap_or(false)
            });

            // æ”¶é›†æ‰€æœ‰é—®é¢˜
            let mut issues = Vec::new();
            if !is_initialized {
                issues.push("æ•°æ®åº“æœªåˆå§‹åŒ–".to_string());
            }
            if !dependencies_met {
                issues.push("ä¾èµ–æ•°æ®åº“æœªå°±ç»ª".to_string());
            }
            if pending_count > 0 {
                issues.push(format!(
                    "æœ‰ {} ä¸ªè¿ç§»å¾…æ‰§è¡Œ (å½“å‰: v{}, ç›®æ ‡: v{})",
                    pending_count, schema_version, target_version
                ));
            }

            // å¥åº·çŠ¶æ€ï¼šå·²åˆå§‹åŒ– + æ— å¾…æ‰§è¡Œè¿ç§» + ä¾èµ–æ»¡è¶³
            let is_healthy = is_initialized && pending_count == 0 && dependencies_met;

            // è¾“å‡ºæ¯ä¸ªæ•°æ®åº“çš„è¯¦ç»†çŠ¶æ€
            if is_healthy {
                info!(
                    "  âœ… [HealthCheck] {}: v{}, å¥åº·",
                    id.as_str(),
                    schema_version
                );
            } else {
                warn!(
                    "  âš ï¸ [HealthCheck] {}: v{} -> v{}, é—®é¢˜: {:?}",
                    id.as_str(),
                    schema_version,
                    target_version,
                    issues
                );
            }

            DatabaseHealthStatus {
                id: id.as_str().to_string(),
                is_healthy,
                dependencies_met,
                schema_version,
                target_version,
                pending_count,
                issues,
            }
        })
        .collect();

    // æ•´ä½“å¥åº·ï¼šä¾èµ–é€šè¿‡ + æ— æœªåˆå§‹åŒ–æ•°æ®åº“ + æ— å¾…æ‰§è¡Œè¿ç§»
    let overall_healthy =
        dependency_ok && uninitialized_count == 0 && pending_migrations_total == 0;

    if overall_healthy {
        info!("âœ… [HealthCheck] å¥åº·æ£€æŸ¥å®Œæˆ: æ‰€æœ‰æ•°æ®åº“çŠ¶æ€æ­£å¸¸");
    } else {
        warn!(
            "âš ï¸ [HealthCheck] å¥åº·æ£€æŸ¥å®Œæˆ: å‘ç°é—®é¢˜ (æœªåˆå§‹åŒ–: {}, ä¾èµ–æ£€æŸ¥: {}, å¾…æ‰§è¡Œè¿ç§»: {})",
            uninitialized_count,
            if dependency_ok { "é€šè¿‡" } else { "å¤±è´¥" },
            pending_migrations_total
        );
    }

    let audit_snapshot = app
        .try_state::<Arc<AuditHealthState>>()
        .map(|state| state.snapshot())
        .unwrap_or_else(AuditHealthSnapshot::healthy);

    Ok(HealthCheckResponse {
        overall_healthy,
        total_databases,
        initialized_count,
        uninitialized_count,
        dependency_check_passed: dependency_ok,
        dependency_error,
        databases: database_health,
        checked_at: chrono::Utc::now().to_rfc3339(),
        pending_migrations_count: pending_migrations_total,
        has_pending_migrations: pending_migrations_total > 0,
        audit_log_healthy: audit_snapshot.is_healthy,
        audit_log_error: audit_snapshot.last_error,
        audit_log_error_at: audit_snapshot.last_error_at,
    })
}

/// è·å–ç‰¹å®šæ•°æ®åº“çš„è¯¦ç»†çŠ¶æ€
#[tauri::command]
pub fn data_governance_get_database_status(
    app: AppHandle,
    registry: State<'_, Arc<RwLock<SchemaRegistry>>>,
    database_id: String,
) -> Result<Option<DatabaseDetailResponse>, String> {
    let registry = refresh_schema_registry_from_live_state(&app, registry.inner())?;

    let db_id = match database_id.as_str() {
        "vfs" => DatabaseId::Vfs,
        "chat_v2" => DatabaseId::ChatV2,
        "mistakes" => DatabaseId::Mistakes,
        "llm_usage" => DatabaseId::LlmUsage,
        _ => {
            return Err(format!(
                "æœªçŸ¥çš„æ•°æ®åº“ ID: {}ã€‚å¯é€‰å€¼: vfs, chat_v2, mistakes, llm_usage",
                database_id
            ))
        }
    };

    Ok(registry
        .get_status(&db_id)
        .map(|status| DatabaseDetailResponse {
            id: db_id.as_str().to_string(),
            schema_version: status.schema_version,
            min_compatible_version: status.min_compatible_version,
            max_compatible_version: status.max_compatible_version,
            data_contract_version: status.data_contract_version.clone(),
            checksum: status.checksum.clone(),
            updated_at: status.updated_at.clone(),
            migration_history: status
                .migration_history
                .iter()
                .map(|m| MigrationRecordResponse {
                    version: m.version,
                    name: m.name.clone(),
                    checksum: m.checksum.clone(),
                    applied_at: m.applied_at.clone(),
                    duration_ms: m.duration_ms,
                    success: m.success,
                })
                .collect(),
            dependencies: db_id
                .dependencies()
                .iter()
                .map(|d| d.as_str().to_string())
                .collect(),
        }))
}

/// ç”Ÿæˆè¿ç§»è¯Šæ–­æŠ¥å‘Š
///
/// æ”¶é›†æ‰€æœ‰æ•°æ®åº“çš„è¿ç§»çŠ¶æ€ã€é”™è¯¯ä¿¡æ¯ã€è¿ç§»å†å²ã€ç£ç›˜ç©ºé—´ç­‰ä¿¡æ¯ï¼Œ
/// è¿”å›æ ¼å¼åŒ–çš„çº¯æ–‡æœ¬æŠ¥å‘Šï¼Œç”¨äºç”¨æˆ·ä¸€é”®å¤åˆ¶ç»™å¼€å‘è€…ã€‚
#[tauri::command]
pub fn data_governance_get_migration_diagnostic_report(
    app_handle: AppHandle,
    registry: State<'_, Arc<RwLock<SchemaRegistry>>>,
) -> Result<String, String> {
    use std::fmt::Write;

    let app_data_dir = get_live_app_data_dir(&app_handle)?;
    let registry = refresh_schema_registry_from_live_state(&app_handle, registry.inner())?;

    let mut report = String::with_capacity(4096);

    // --- å¤´éƒ¨ ---
    let _ = writeln!(report, "=== Deep Student è¿ç§»è¯Šæ–­æŠ¥å‘Š ===");
    let _ = writeln!(report, "æ—¶é—´: {}", chrono::Utc::now().to_rfc3339());
    let _ = writeln!(
        report,
        "å¹³å°: {} {}",
        std::env::consts::OS,
        std::env::consts::ARCH
    );
    let _ = writeln!(report, "åº”ç”¨ç‰ˆæœ¬: {}", env!("CARGO_PKG_VERSION"));
    let _ = writeln!(report);

    // --- æ•°æ®åº“çŠ¶æ€ ---
    let _ = writeln!(report, "--- æ•°æ®åº“çŠ¶æ€ ---");
    let mut error_messages: Vec<String> = Vec::new();

    for id in DatabaseId::all_ordered() {
        let status = registry.get_status(&id);
        let current_version = status.map(|s| s.schema_version).unwrap_or(0);
        let (target_version, pending_count) =
            resolve_target_and_pending(&id, current_version, status);

        let flag = if pending_count > 0 && current_version > 0 {
            error_messages.push(format!(
                "{}: æœ‰ {} ä¸ªè¿ç§»æœªæ‰§è¡Œ (å½“å‰: v{}, ç›®æ ‡: v{})",
                id.as_str(),
                pending_count,
                current_version,
                target_version
            ));
            " âš ï¸"
        } else {
            ""
        };

        let _ = writeln!(
            report,
            "[{}] å½“å‰: v{}, ç›®æ ‡: v{}, å¾…æ‰§è¡Œ: {}{}",
            id.as_str(),
            current_version,
            target_version,
            pending_count,
            flag
        );
    }
    let _ = writeln!(report);

    // --- é”™è¯¯ä¿¡æ¯ï¼ˆå®æ—¶è¯Šæ–­ï¼‰ ---
    let _ = writeln!(report, "--- é”™è¯¯ä¿¡æ¯ ---");

    // ä¼˜å…ˆæ˜¾ç¤ºæŒä¹…åŒ–çš„çœŸå®è¿ç§»é”™è¯¯ï¼ˆæ¥è‡ªå®é™… SQL æ‰§è¡Œå¤±è´¥ï¼‰
    if let Some((real_error, error_ts)) = read_persisted_migration_error(&app_data_dir) {
        let _ = writeln!(report, "[{}] çœŸå®è¿ç§»é”™è¯¯: {}", error_ts, real_error);
    }

    if error_messages.is_empty() && read_persisted_migration_error(&app_data_dir).is_none() {
        let _ = writeln!(report, "(æ— )");
    } else {
        for msg in &error_messages {
            let _ = writeln!(report, "{}", msg);
        }
    }
    let _ = writeln!(report);

    // --- å®¡è®¡æ—¥å¿—ä¸­çš„è¿ç§»å¤±è´¥è®°å½• ---
    let _ = writeln!(report, "--- æœ€è¿‘è¿ç§»å¤±è´¥è®°å½•ï¼ˆå®¡è®¡æ—¥å¿—ï¼‰ ---");
    {
        let audit_db_path = app_data_dir.join("databases").join("audit.db");
        if audit_db_path.exists() {
            match rusqlite::Connection::open(&audit_db_path) {
                Ok(conn) => {
                    // ç›´æ¥æŸ¥æœ€è¿‘ 5 æ¡å¤±è´¥çš„è¿ç§»å®¡è®¡è®°å½•
                    let sql = "SELECT timestamp, target, error_message, details \
                               FROM __audit_log \
                               WHERE operation_type = 'migration' AND status = 'failed' \
                               ORDER BY timestamp DESC LIMIT 5";
                    match conn.prepare(sql) {
                        Ok(mut stmt) => {
                            let mut found = false;
                            if let Ok(rows) = stmt.query_map([], |row| {
                                Ok((
                                    row.get::<_, String>(0).unwrap_or_default(),
                                    row.get::<_, String>(1).unwrap_or_default(),
                                    row.get::<_, Option<String>>(2).unwrap_or(None),
                                    row.get::<_, Option<String>>(3).unwrap_or(None),
                                ))
                            }) {
                                for row in rows.flatten() {
                                    found = true;
                                    let (ts, target, err, details) = row;
                                    let _ = writeln!(report, "[{}] db={}", ts, target);
                                    if let Some(err) = err {
                                        let _ = writeln!(report, "  error: {}", err);
                                    }
                                    if let Some(details) = details {
                                        // æˆªå–å‰ 500 å­—ç¬¦ï¼Œé¿å…è¿‡é•¿
                                        let truncated = if details.chars().count() > 500 {
                                            format!(
                                                "{}...(truncated)",
                                                safe_truncate_chars(&details, 500)
                                            )
                                        } else {
                                            details
                                        };
                                        let _ = writeln!(report, "  details: {}", truncated);
                                    }
                                }
                            }
                            if !found {
                                let _ = writeln!(report, "(å®¡è®¡æ—¥å¿—ä¸­æ— è¿ç§»å¤±è´¥è®°å½•)");
                            }
                        }
                        Err(e) => {
                            let _ = writeln!(report, "(æŸ¥è¯¢å®¡è®¡æ—¥å¿—å¤±è´¥: {})", e);
                        }
                    }
                }
                Err(e) => {
                    let _ = writeln!(report, "(æ— æ³•æ‰“å¼€å®¡è®¡æ•°æ®åº“: {})", e);
                }
            }
        } else {
            let _ = writeln!(report, "(å®¡è®¡æ•°æ®åº“ä¸å­˜åœ¨)");
        }
    }
    let _ = writeln!(report);

    // --- æµ‹è¯•æ’æ§½è¿ç§»å¤ç°ï¼ˆå®‰å…¨æ²™ç®±ï¼‰ ---
    // ä½¿ç”¨æµ‹è¯•æ’æ§½ C/D åœ¨éš”ç¦»ç¯å¢ƒä¸­å¤ç°è¿ç§»é”™è¯¯ï¼Œä¸å½±å“ç”Ÿäº§æ•°æ®
    let _ = writeln!(report, "--- ç©ºåº“è¿ç§»æµ‹è¯• (Slot C) ---");
    {
        let result = run_slot_c_empty_db_test(&app_data_dir);
        let _ = write!(report, "{}", result.report);
    }
    let _ = writeln!(report);

    let _ = writeln!(report, "--- å½“å‰åº“é‡è¯•è¿ç§»æµ‹è¯• (Slot D) ---");
    let _ = writeln!(
        report,
        "(å¤åˆ¶å½“å‰æ´»è·ƒæ’æ§½çš„æ•°æ®åº“ï¼Œé‡æ–°æ‰§è¡Œè¿ç§»æµç¨‹ï¼›è‹¥æˆåŠŸè¯´æ˜é‡å¯å¯æ¢å¤)"
    );
    {
        let result = run_slot_d_clone_db_test(&app_data_dir);
        let _ = write!(report, "{}", result.report);
    }
    let _ = writeln!(report);

    // --- è¿ç§»å†å² ---
    let _ = writeln!(report, "--- è¿ç§»å†å² ---");
    for id in DatabaseId::all_ordered() {
        let status = registry.get_status(&id);
        if let Some(status) = status {
            let history_str: String = status
                .migration_history
                .iter()
                .map(|m| format!("v{}({})", m.version, m.name))
                .collect::<Vec<_>>()
                .join(" ");
            let _ = writeln!(
                report,
                "[{}] {}",
                id.as_str(),
                if history_str.is_empty() {
                    "(æ— è®°å½•)".to_string()
                } else {
                    history_str
                }
            );
        } else {
            let _ = writeln!(report, "[{}] (æ•°æ®åº“æœªåˆå§‹åŒ–)", id.as_str());
        }
    }
    let _ = writeln!(report);

    // --- ç£ç›˜ç©ºé—´ ---
    let _ = writeln!(report, "--- ç£ç›˜ç©ºé—´ ---");
    let available = crate::backup_common::get_available_disk_space(&app_data_dir).unwrap_or(0);
    let mut total_db_size: u64 = 0;
    for db_id in DatabaseId::all_ordered() {
        let db_path = match db_id {
            DatabaseId::Vfs => app_data_dir.join("databases").join("vfs.db"),
            DatabaseId::ChatV2 => app_data_dir.join("chat_v2.db"),
            DatabaseId::Mistakes => app_data_dir.join("mistakes.db"),
            DatabaseId::LlmUsage => app_data_dir.join("llm_usage.db"),
        };
        if db_path.exists() {
            if let Ok(meta) = std::fs::metadata(&db_path) {
                total_db_size += meta.len();
            }
        }
    }
    let _ = writeln!(
        report,
        "å¯ç”¨: {}MB, æ•°æ®åº“æ€»å¤§å°: {}MB",
        available / (1024 * 1024),
        total_db_size / (1024 * 1024)
    );
    let _ = writeln!(report);

    // --- æ•°æ®ç›®å½• ---
    let _ = writeln!(report, "--- æ•°æ®ç›®å½• ---");
    let _ = writeln!(report, "{}", app_data_dir.display());

    Ok(report)
}

/// è¿è¡Œ Slot C ç©ºåº“è¿ç§»æµ‹è¯•ï¼ˆæµ‹è¯•æ’æ§½ï¼Œä¸å½±å“å½“å‰æ•°æ®ï¼‰
#[tauri::command]
pub fn data_governance_run_slot_c_empty_db_test(
    app_handle: AppHandle,
) -> Result<SlotMigrationTestResponse, String> {
    let app_data_dir = get_live_app_data_dir(&app_handle)?;
    Ok(run_slot_c_empty_db_test(&app_data_dir))
}

/// è¿è¡Œ Slot D å…‹éš†åº“è¿ç§»æµ‹è¯•ï¼ˆæµ‹è¯•æ’æ§½ï¼Œä¸å½±å“å½“å‰æ•°æ®ï¼‰
#[tauri::command]
pub fn data_governance_run_slot_d_clone_db_test(
    app_handle: AppHandle,
) -> Result<SlotMigrationTestResponse, String> {
    let app_data_dir = get_live_app_data_dir(&app_handle)?;
    Ok(run_slot_d_clone_db_test(&app_data_dir))
}

// ==================== å“åº”ç±»å‹å®šä¹‰ ====================

/// ç»´æŠ¤æ¨¡å¼çŠ¶æ€å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct MaintenanceStatusResponse {
    pub is_in_maintenance_mode: bool,
}

/// Schema æ³¨å†Œè¡¨å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SchemaRegistryResponse {
    pub global_version: u64,
    pub aggregated_at: String,
    pub databases: Vec<DatabaseStatusResponse>,
}

/// æ•°æ®åº“çŠ¶æ€å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseStatusResponse {
    pub id: String,
    pub schema_version: u32,
    pub min_compatible_version: u32,
    pub max_compatible_version: u32,
    pub data_contract_version: String,
    pub migration_count: usize,
    pub checksum: String,
    pub updated_at: String,
}

/// å®¡è®¡æ—¥å¿—å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct AuditLogResponse {
    pub id: String,
    pub timestamp: String,
    pub operation_type: String,
    pub target: String,
    pub status: String,
    pub duration_ms: Option<u64>,
    pub error_message: Option<String>,
}

/// å®¡è®¡æ—¥å¿—åˆ†é¡µå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct AuditLogPagedResponse {
    /// å½“å‰é¡µçš„å®¡è®¡æ—¥å¿—åˆ—è¡¨
    pub logs: Vec<AuditLogResponse>,
    /// æ»¡è¶³è¿‡æ»¤æ¡ä»¶çš„æ€»è®°å½•æ•°ï¼ˆä¸å— limit/offset å½±å“ï¼‰
    pub total: u64,
}

impl From<AuditLog> for AuditLogResponse {
    fn from(log: AuditLog) -> Self {
        let operation_type = match &log.operation {
            super::audit::AuditOperation::Migration { .. } => "Migration",
            super::audit::AuditOperation::Backup { .. } => "Backup",
            super::audit::AuditOperation::Restore { .. } => "Restore",
            super::audit::AuditOperation::Sync { .. } => "Sync",
            super::audit::AuditOperation::Maintenance { .. } => "Maintenance",
        };

        let status = match &log.status {
            AuditStatus::Started => "Started",
            AuditStatus::Completed => "Completed",
            AuditStatus::Failed => "Failed",
            AuditStatus::Partial => "Partial",
        };

        Self {
            id: log.id,
            timestamp: log.timestamp.to_rfc3339(),
            operation_type: operation_type.to_string(),
            target: log.target,
            status: status.to_string(),
            duration_ms: log.duration_ms,
            error_message: log.error_message,
        }
    }
}

/// è¿ç§»çŠ¶æ€å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct MigrationStatusResponse {
    pub global_version: u64,
    pub all_healthy: bool,
    pub databases: Vec<MigrationDatabaseStatus>,
    /// å¾…æ‰§è¡Œè¿ç§»æ€»æ•°
    pub pending_migrations_total: usize,
    /// æ˜¯å¦æœ‰å¾…æ‰§è¡Œè¿ç§»
    pub has_pending_migrations: bool,
    /// æœ€åçš„è¿ç§»é”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub last_error: Option<String>,
}

/// è¿ç§»æ•°æ®åº“çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize)]
pub struct MigrationDatabaseStatus {
    pub id: String,
    pub current_version: u32,
    /// ç›®æ ‡ç‰ˆæœ¬ï¼ˆæœ€æ–°å¯ç”¨è¿ç§»ç‰ˆæœ¬ï¼‰
    pub target_version: u32,
    pub is_initialized: bool,
    pub last_migration_at: Option<String>,
    /// å¾…æ‰§è¡Œè¿ç§»æ•°é‡
    pub pending_count: usize,
    /// æ˜¯å¦æœ‰å¾…æ‰§è¡Œè¿ç§»
    pub has_pending: bool,
}

/// å¥åº·æ£€æŸ¥å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct HealthCheckResponse {
    pub overall_healthy: bool,
    pub total_databases: usize,
    pub initialized_count: usize,
    pub uninitialized_count: usize,
    pub dependency_check_passed: bool,
    pub dependency_error: Option<String>,
    pub databases: Vec<DatabaseHealthStatus>,
    pub checked_at: String,
    /// å¾…æ‰§è¡Œè¿ç§»æ€»æ•°
    pub pending_migrations_count: usize,
    /// æ˜¯å¦æœ‰å¾…æ‰§è¡Œè¿ç§»
    pub has_pending_migrations: bool,
    /// å®¡è®¡å†™å…¥æ˜¯å¦å¥åº·
    pub audit_log_healthy: bool,
    /// å®¡è®¡å†™å…¥é”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub audit_log_error: Option<String>,
    /// å®¡è®¡å†™å…¥é”™è¯¯æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
    pub audit_log_error_at: Option<String>,
}

/// æ•°æ®åº“å¥åº·çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseHealthStatus {
    pub id: String,
    pub is_healthy: bool,
    pub dependencies_met: bool,
    pub schema_version: u32,
    /// ç›®æ ‡ç‰ˆæœ¬ï¼ˆæœ€æ–°å¯ç”¨è¿ç§»ç‰ˆæœ¬ï¼‰
    pub target_version: u32,
    /// å¾…æ‰§è¡Œè¿ç§»æ•°é‡
    pub pending_count: usize,
    pub issues: Vec<String>,
}

/// æ•°æ®åº“è¯¦æƒ…å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseDetailResponse {
    pub id: String,
    pub schema_version: u32,
    pub min_compatible_version: u32,
    pub max_compatible_version: u32,
    pub data_contract_version: String,
    pub checksum: String,
    pub updated_at: String,
    pub migration_history: Vec<MigrationRecordResponse>,
    pub dependencies: Vec<String>,
}

/// è¿ç§»è®°å½•å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct MigrationRecordResponse {
    pub version: u32,
    pub name: String,
    pub checksum: String,
    pub applied_at: String,
    pub duration_ms: Option<u64>,
    pub success: bool,
}

// ==================== å¤‡ä»½ç›¸å…³å‘½ä»¤ ====================

use std::path::PathBuf;
use std::time::Instant;
use tracing::{debug, error, info, warn};

use super::backup::{
    export_backup_to_zip, AssetBackupConfig, AssetBackupResult, AssetType, AssetTypeStats,
    BackupManager, BackupSelection, BackupTier, BackupVerifyResult, TieredAssetConfig,
    TieredBackupResult, ZipExportOptions,
};

/// è·å–åº”ç”¨æ•°æ®åŸºç¡€ç›®å½•ï¼ˆTauri app_data_dirï¼‰
///
/// æ³¨æ„ï¼šæ­¤ç›®å½•æ˜¯åŸºç¡€ç›®å½•ï¼Œ**ä¸æ˜¯**è¿è¡Œæ—¶æ•°æ®åº“/èµ„äº§çš„å®é™…å­˜å‚¨ä½ç½®ã€‚
/// è¿è¡Œæ—¶å­˜å‚¨ä½ç½®è¯·ä½¿ç”¨ `get_active_data_dir`ã€‚
fn get_app_data_dir(app: &tauri::AppHandle) -> Result<PathBuf, String> {
    app.path()
        .app_data_dir()
        .map_err(|e| format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e))
}

/// è·å–æ´»åŠ¨æ•°æ®ç©ºé—´ç›®å½•ï¼ˆè¿è¡Œæ—¶æ‰€æœ‰æ•°æ®åº“å’Œèµ„äº§çš„å®é™…å­˜å‚¨ä½ç½®ï¼‰
///
/// é€šè¿‡ DataSpaceManager è·å–å½“å‰æ´»åŠ¨æ§½ä½ï¼ˆA/B åŒæ•°æ®ç©ºé—´ï¼‰çš„è·¯å¾„ã€‚
/// å›é€€åˆ° `base_dir/slots/slotA` ä½œä¸ºé»˜è®¤å€¼ã€‚
///
/// **é‡è¦**ï¼šæ‰€æœ‰æ•°æ®åº“è·¯å¾„è§£æã€åŒæ­¥æ“ä½œã€èµ„äº§æ‰«æéƒ½å¿…é¡»åŸºäºæ­¤ç›®å½•ï¼Œ
/// ç¦æ­¢ç›´æ¥ä½¿ç”¨ `get_app_data_dir` è®¿é—®æ•°æ®åº“æ–‡ä»¶ã€‚
fn get_active_data_dir(app: &tauri::AppHandle) -> Result<PathBuf, String> {
    let base_dir = get_app_data_dir(app)?;
    Ok(crate::data_space::get_data_space_manager()
        .map(|mgr| mgr.active_dir())
        .unwrap_or_else(|| base_dir.join("slots").join("slotA")))
}

/// è·å–å¤‡ä»½ç›®å½•
fn get_backup_dir(app_data_dir: &PathBuf) -> PathBuf {
    app_data_dir.join("backups")
}

/// ç»Ÿä¸€è§£ææ•°æ®åº“æ–‡ä»¶è·¯å¾„
///
/// æ ¹æ® `DatabaseId` å’Œæ´»åŠ¨æ•°æ®ç©ºé—´ç›®å½•è¿”å›å¯¹åº”æ•°æ®åº“æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚
/// è·¯å¾„è§„åˆ™ä¸ `MigrationCoordinator::get_database_path` å’Œ
/// `BackupManager::get_database_path` ä¿æŒä¸€è‡´ï¼š
/// - Vfs: `<active_dir>/databases/vfs.db`
/// - ChatV2: `<active_dir>/chat_v2.db`
/// - Mistakes: `<active_dir>/mistakes.db`
/// - LlmUsage: `<active_dir>/llm_usage.db`
fn resolve_database_path(db_id: &DatabaseId, active_dir: &Path) -> PathBuf {
    match db_id {
        DatabaseId::Vfs => active_dir.join("databases").join("vfs.db"),
        DatabaseId::ChatV2 => active_dir.join("chat_v2.db"),
        DatabaseId::Mistakes => active_dir.join("mistakes.db"),
        DatabaseId::LlmUsage => active_dir.join("llm_usage.db"),
    }
}

/// å¤šåº“åº”ç”¨ç»“æœ
struct ApplyToDbsResult {
    total_success: usize,
    total_skipped: usize,
    total_failed: usize,
}

/// æ ¹æ®è¡¨åæ¨æ–­å˜æ›´æ‰€å±çš„æ•°æ®åº“ï¼ˆç”¨äº legacy æ—  database_name çš„å˜æ›´ï¼‰
///
/// ä½¿ç”¨å·²çŸ¥çš„è¡¨åâ†’åº“æ˜ å°„ï¼Œé¿å…å°†é chat_v2 çš„å˜æ›´é”™è¯¯è·¯ç”±åˆ° chat_v2ã€‚
/// è¿”å› None è¡¨ç¤ºè¡¨åæœªçŸ¥ï¼Œè°ƒç”¨æ–¹åº”è·³è¿‡è¯¥å˜æ›´ã€‚
fn infer_database_from_table(table_name: &str) -> Option<&'static str> {
    // chat_v2 è¡¨ï¼ˆå‰ç¼€ chat_v2_ æˆ–å·²çŸ¥è¡¨åï¼‰
    match table_name {
        // chat_v2 æ•°æ®åº“
        t if t.starts_with("chat_v2_") => Some("chat_v2"),
        "workspace_index" | "sleep_block" | "subagent_task" => Some("chat_v2"),
        // "resources" åŒæ—¶å­˜åœ¨äº chat_v2 å’Œ vfsï¼Œæ— æ³•åˆ¤å®šï¼Œè·³è¿‡
        "resources" => {
            tracing::warn!(
                "[sync] 'resources' è¡¨åŒæ—¶å­˜åœ¨äº chat_v2 å’Œ vfsï¼Œlegacy å˜æ›´æ— æ³•åˆ¤å®šç›®æ ‡åº“ï¼Œè·³è¿‡"
            );
            None
        }
        // mistakes ä¸»æ•°æ®åº“
        "mistakes"
        | "chat_messages"
        | "temp_sessions"
        | "review_analyses"
        | "review_chat_messages"
        | "review_sessions"
        | "review_session_mistakes"
        | "settings"
        | "rag_configurations"
        | "document_tasks"
        | "anki_cards"
        | "custom_anki_templates"
        | "document_control_states"
        | "vectorized_data"
        | "rag_sub_libraries"
        | "search_logs"
        | "exam_sheet_sessions"
        | "migration_progress" => Some("mistakes"),
        // vfs æ•°æ®åº“
        "blobs"
        | "notes"
        | "notes_versions"
        | "files"
        | "exam_sheets"
        | "translations"
        | "essays"
        | "essay_sessions"
        | "folders"
        | "folder_items"
        | "path_cache"
        | "mindmaps"
        | "questions"
        | "question_history"
        | "question_bank_stats"
        | "review_plans"
        | "review_history"
        | "review_stats" => Some("vfs"),
        // llm_usage æ•°æ®åº“
        "llm_usage_logs" | "llm_usage_daily" => Some("llm_usage"),
        // __change_log æ˜¯ç³»ç»Ÿè¡¨ï¼Œä¸åº”è¢«åŒæ­¥å›æ”¾
        "__change_log" => None,
        // æœªçŸ¥è¡¨å
        _ => {
            tracing::debug!("[sync] æœªçŸ¥è¡¨å '{}', æ— æ³•æ¨æ–­æ•°æ®åº“", table_name);
            None
        }
    }
}

/// å°†ä¸‹è½½çš„å˜æ›´æŒ‰æ•°æ®åº“è·¯ç”±å¹¶åº”ç”¨
///
/// æ ¹æ®æ¯æ¡å˜æ›´çš„ `database_name` å­—æ®µå°†å˜æ›´è·¯ç”±åˆ°å¯¹åº”çš„æ•°æ®åº“ï¼Œ
/// ç¡®ä¿å¤šåº“åŒæ­¥æ—¶å˜æ›´ä¸ä¼šé”™è¯¯åœ°åº”ç”¨åˆ°å•ä¸€æ•°æ®åº“ã€‚
/// å¯¹äºæ²¡æœ‰ `database_name` çš„æ—§æ ¼å¼å˜æ›´ï¼Œé€šè¿‡è¡¨åæ¨æ–­ç›®æ ‡æ•°æ®åº“ã€‚
///
/// è¿”å›èšåˆçš„åº”ç”¨ç»“æœï¼Œè°ƒç”¨æ–¹å¯æ ¹æ® `total_skipped` å‘ç”¨æˆ·å‘å‡ºè­¦å‘Šã€‚
fn apply_downloaded_changes_to_databases(
    changes: &[SyncChangeWithData],
    active_dir: &std::path::Path,
) -> Result<ApplyToDbsResult, String> {
    use std::collections::HashMap;

    let mut agg = ApplyToDbsResult {
        total_success: 0,
        total_skipped: 0,
        total_failed: 0,
    };

    // æŒ‰æ•°æ®åº“åç§°åˆ†ç»„ï¼ˆlegacy å˜æ›´æŒ‰è¡¨åæ¨æ–­åº“ï¼‰
    let mut grouped: HashMap<String, Vec<&SyncChangeWithData>> = HashMap::new();
    for change in changes {
        let db_name = match change.database_name.as_deref() {
            Some(name) => name.to_string(),
            None => {
                // Legacy å˜æ›´æ—  database_nameï¼ŒæŒ‰è¡¨åæ¨æ–­ç›®æ ‡åº“
                match infer_database_from_table(&change.table_name) {
                    Some(name) => name.to_string(),
                    None => {
                        warn!(
                            "[data_governance] Legacy å˜æ›´è¡¨å '{}' æ— æ³•æ¨æ–­ç›®æ ‡æ•°æ®åº“ï¼Œè·³è¿‡ (record_id={})",
                            change.table_name, change.record_id
                        );
                        agg.total_skipped += 1;
                        continue;
                    }
                }
            }
        };
        grouped.entry(db_name).or_default().push(change);
    }

    for (db_name, db_changes) in &grouped {
        // è§£ææ•°æ®åº“ ID
        let db_id = DatabaseId::all_ordered()
            .into_iter()
            .find(|id| id.as_str() == db_name);

        let db_path = match db_id {
            Some(id) => resolve_database_path(&id, active_dir),
            None => {
                warn!(
                    "[data_governance] æœªçŸ¥æ•°æ®åº“åç§° '{}', è·³è¿‡ {} æ¡å˜æ›´",
                    db_name,
                    db_changes.len()
                );
                agg.total_skipped += db_changes.len();
                continue;
            }
        };

        if !db_path.exists() {
            warn!(
                "[data_governance] æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {}, è·³è¿‡ {} æ¡å˜æ›´",
                db_path.display(),
                db_changes.len()
            );
            agg.total_skipped += db_changes.len();
            continue;
        }

        let conn = rusqlite::Connection::open(&db_path)
            .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“ {} å¤±è´¥: {}", db_name, e))?;

        let owned_changes: Vec<SyncChangeWithData> = db_changes
            .iter()
            .map(|c| {
                let mut cloned = (*c).clone();
                cloned.suppress_change_log = Some(true);
                cloned
            })
            .collect();

        match SyncManager::apply_downloaded_changes(&conn, &owned_changes, None) {
            Ok(apply_result) => {
                agg.total_success += apply_result.success_count;
                agg.total_skipped += apply_result.skipped_count;
                agg.total_failed += apply_result.failure_count;
                info!(
                    "[data_governance] æ•°æ®åº“ {} åº”ç”¨å˜æ›´å®Œæˆ: success={}, failed={}, skipped={}",
                    db_name,
                    apply_result.success_count,
                    apply_result.failure_count,
                    apply_result.skipped_count
                );
            }
            Err(e) => {
                error!("[data_governance] æ•°æ®åº“ {} åº”ç”¨å˜æ›´å¤±è´¥: {}", db_name, e);
                return Err(format!(
                    "æ•°æ®åº“ {} åº”ç”¨ä¸‹è½½å˜æ›´å¤±è´¥: {}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•åŒæ­¥",
                    db_name, e
                ));
            }
        }
    }

    Ok(agg)
}

/// å°†è·¯å¾„ä¸­ç”¨æˆ·ä¸»ç›®å½•æ›¿æ¢ä¸º "~/"ï¼Œé¿å…åœ¨é¢å‘ç”¨æˆ·çš„é”™è¯¯ä¿¡æ¯ä¸­æ³„éœ²å®Œæ•´æ–‡ä»¶ç³»ç»Ÿè·¯å¾„
fn sanitize_path_for_user(path: &Path) -> String {
    let path_str = path.to_string_lossy();
    if let Some(home) = dirs::home_dir() {
        let home_str = home.to_string_lossy();
        if path_str.starts_with(home_str.as_ref()) {
            return format!("~/{}", &path_str[home_str.len()..].trim_start_matches('/'));
        }
    }
    // å¦‚æœæ— æ³•è·å– home ç›®å½•ï¼Œè‡³å°‘åªä¿ç•™æœ€åä¸¤çº§è·¯å¾„
    let components: Vec<&str> = path_str.split('/').filter(|s| !s.is_empty()).collect();
    if components.len() > 2 {
        format!(".../{}", components[components.len() - 2..].join("/"))
    } else {
        path_str.to_string()
    }
}

/// éªŒè¯ç”¨æˆ·æä¾›çš„è·¯å¾„ï¼ˆä¸å†é™åˆ¶ç›®å½•èŒƒå›´ï¼Œå…è®¸ä»»æ„è·¯å¾„ï¼‰
fn validate_user_path(_path: &Path, _app_data_dir: &Path) -> Result<(), String> {
    Ok(())
}

fn validate_backup_id(raw_backup_id: &str) -> Result<String, String> {
    let trimmed = raw_backup_id.trim();
    if trimmed.is_empty() {
        return Err("backup_id ä¸èƒ½ä¸ºç©º".to_string());
    }

    let decoded = urlencoding::decode(trimmed)
        .map_err(|e| format!("backup_id ç¼–ç éæ³•: {}", e))?
        .into_owned();

    if decoded != trimmed {
        return Err("backup_id ä¸å…è®¸åŒ…å« URL ç¼–ç ".to_string());
    }

    if decoded.len() > 128 {
        return Err("backup_id é•¿åº¦è¶…é™ï¼ˆæœ€å¤§ 128ï¼‰".to_string());
    }

    if decoded.contains('/')
        || decoded.contains('\\')
        || decoded.contains("..")
        || decoded.starts_with('.')
    {
        return Err("backup_id åŒ…å«éæ³•è·¯å¾„ç‰‡æ®µ".to_string());
    }

    if !decoded
        .chars()
        .all(|ch| ch.is_ascii_alphanumeric() || matches!(ch, '-' | '_' | '.'))
    {
        return Err("backup_id åŒ…å«éæ³•å­—ç¬¦".to_string());
    }

    Ok(decoded)
}

fn ensure_existing_path_within_backup_dir(
    path: &std::path::Path,
    backup_dir: &std::path::Path,
) -> Result<(), String> {
    let canonical_backup_dir =
        std::fs::canonicalize(backup_dir).map_err(|e| format!("è§£æå¤‡ä»½æ ¹ç›®å½•å¤±è´¥: {}", e))?;
    let canonical_path =
        std::fs::canonicalize(path).map_err(|e| format!("è§£æå¤‡ä»½è·¯å¾„å¤±è´¥: {}", e))?;

    if !canonical_path.starts_with(&canonical_backup_dir) {
        return Err(format!(
            "å¤‡ä»½è·¯å¾„è¶Šç•Œ: {}ã€‚è¯·ç¡®è®¤è·¯å¾„åœ¨å¤‡ä»½ç›®å½•å†…ï¼Œæˆ–å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç†ã€é‡æ–°é€‰æ‹©å¤‡ä»½ç›®å½•",
            sanitize_path_for_user(&canonical_path)
        ));
    }

    Ok(())
}

/// è·å–å…¨å±€å¤‡ä»½äº’æ–¥é”ï¼ˆå–æ¶ˆå‹å¥½ï¼‰
///
/// èƒŒæ™¯ï¼šå¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºéƒ½ä¼šè¯»å†™åŒä¸€å¥—å¤‡ä»½ç›®å½•å’Œæ•°æ®åº“æ–‡ä»¶ã€‚
/// è‹¥å¹¶å‘æ‰§è¡Œï¼Œå®¹æ˜“å¯¼è‡´ï¼š
/// - å¤‡ä»½ç›®å½•å†™å…¥è¦†ç›–ï¼ˆå°¤å…¶æ˜¯å†å²ä¸Šç§’çº§æ—¶é—´æˆ³ç›®å½•åï¼‰
/// - restore ä¸å¤‡ä»½/å¯¼å‡ºå¹¶å‘ï¼Œé€ æˆä¸€è‡´æ€§é£é™©æˆ– Windows æ–‡ä»¶é”é—®é¢˜
///
/// è¿™é‡Œç»Ÿä¸€ä½¿ç”¨ `backup_common::BACKUP_GLOBAL_LIMITER` ä¸²è¡ŒåŒ–æ‰€æœ‰ç›¸å…³ä»»åŠ¡ã€‚
async fn acquire_backup_global_permit(
    job_ctx: &BackupJobContext,
    waiting_message: &str,
) -> Option<tokio::sync::OwnedSemaphorePermit> {
    // å‘å‰ç«¯æš´éœ²â€œæ­£åœ¨ç­‰å¾…â€çŠ¶æ€ï¼ˆä¸é˜»å¡ UIï¼‰
    job_ctx.mark_running(
        BackupJobPhase::Queued,
        0.0,
        Some(waiting_message.to_string()),
        0,
        0,
    );

    let fut = BACKUP_GLOBAL_LIMITER.clone().acquire_owned();
    tokio::pin!(fut);

    loop {
        if job_ctx.is_cancelled() {
            job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆä»»åŠ¡".to_string()));
            return None;
        }

        tokio::select! {
            permit = &mut fut => {
                return match permit {
                    Ok(p) => Some(p),
                    Err(e) => {
                        job_ctx.fail(format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e));
                        None
                    }
                };
            }
            _ = tokio::time::sleep(std::time::Duration::from_millis(200)) => {}
        }
    }
}

/// è·å–å¤‡ä»½åˆ—è¡¨
///
/// è¿”å›æ‰€æœ‰å¯ç”¨çš„å¤‡ä»½æ–‡ä»¶åˆ—è¡¨ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
///
/// ## è¿”å›
/// - `Vec<BackupInfoResponse>`: å¤‡ä»½åˆ—è¡¨
#[tauri::command]
pub async fn data_governance_get_backup_list(
    app: tauri::AppHandle,
) -> Result<Vec<BackupInfoResponse>, String> {
    debug!("[data_governance] è·å–å¤‡ä»½åˆ—è¡¨");

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    // æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    if !backup_dir.exists() {
        debug!("[data_governance] å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨");
        return Ok(vec![]);
    }

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let manager = BackupManager::new(backup_dir.clone());

    // è·å–å¤‡ä»½åˆ—è¡¨
    let manifests = manager.list_backups().map_err(|e| {
        error!("[data_governance] è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e);
        format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e)
    })?;

    // è½¬æ¢ä¸ºå“åº”æ ¼å¼
    let backups: Vec<BackupInfoResponse> = manifests
        .iter()
        .map(|m| {
            let db_size: u64 = m.files.iter().map(|f| f.size).sum();
            let asset_size: u64 = m.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
            let size = db_size + asset_size;
            let databases: Vec<String> = m
                .files
                .iter()
                .filter_map(|f| f.database_id.clone())
                .collect();

            BackupInfoResponse {
                path: m.backup_id.clone(),
                created_at: m.created_at.clone(),
                size,
                backup_type: if m.is_incremental {
                    "incremental".to_string()
                } else {
                    "full".to_string()
                },
                databases,
            }
        })
        .collect();

    info!(
        "[data_governance] å¤‡ä»½åˆ—è¡¨è·å–æˆåŠŸ: {} ä¸ªå¤‡ä»½",
        backups.len()
    );

    Ok(backups)
}

/// åˆ é™¤å¤‡ä»½
///
/// åˆ é™¤æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦åˆ é™¤çš„å¤‡ä»½ ID
///
/// ## è¿”å›
/// - `bool`: åˆ é™¤æ˜¯å¦æˆåŠŸ
#[tauri::command]
pub async fn data_governance_delete_backup(
    app: tauri::AppHandle,
    backup_id: String,
) -> Result<bool, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;
    info!("[data_governance] åˆ é™¤å¤‡ä»½: {}", validated_backup_id);

    // å…¨å±€äº’æ–¥ï¼šé¿å…ä¸æ­£åœ¨è¿è¡Œçš„å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _permit = BACKUP_GLOBAL_LIMITER
        .clone()
        .acquire_owned()
        .await
        .map_err(|e| format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e))?;

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®".to_string());
    }

    let manager = BackupManager::new(backup_dir.clone());

    // é˜²æ­¢è·¯å¾„è¶Šç•Œï¼ˆå³ä½¿ validate_backup_id å·²è¿‡æ»¤ï¼Œä¹Ÿå†åšä¸€æ¬¡ canonicalize æ ¡éªŒï¼‰
    let target_dir = backup_dir.join(&validated_backup_id);
    if target_dir.exists() {
        ensure_existing_path_within_backup_dir(&target_dir, &backup_dir)?;
    }

    manager.delete_backup(&validated_backup_id).map_err(|e| {
        error!("[data_governance] åˆ é™¤å¤‡ä»½å¤±è´¥: {}", e);
        format!("åˆ é™¤å¤‡ä»½å¤±è´¥: {}", e)
    })?;

    info!("[data_governance] å¤‡ä»½åˆ é™¤æˆåŠŸ: {}", validated_backup_id);
    Ok(true)
}

/// æ¢å¤å‰ç£ç›˜ç©ºé—´æ£€æŸ¥
///
/// è¯»å–æŒ‡å®šå¤‡ä»½çš„å¤§å°ï¼Œæ£€æŸ¥åº”ç”¨æ•°æ®ç›®å½•æ‰€åœ¨ç£ç›˜æ˜¯å¦æœ‰è¶³å¤Ÿå¯ç”¨ç©ºé—´æ‰§è¡Œæ¢å¤ã€‚
/// æ‰€éœ€ç©ºé—´ = å¤‡ä»½å¤§å° Ã— 2ï¼ˆè§£å‹ + æ¢å¤é¢„ç•™ï¼‰ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦æ¢å¤çš„å¤‡ä»½ ID
///
/// ## è¿”å›
/// - `DiskSpaceCheckResponse`: ç£ç›˜ç©ºé—´æ£€æŸ¥ç»“æœ
#[tauri::command]
pub async fn data_governance_check_disk_space_for_restore(
    app: tauri::AppHandle,
    backup_id: String,
) -> Result<DiskSpaceCheckResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;
    debug!(
        "[data_governance] æ£€æŸ¥æ¢å¤ç£ç›˜ç©ºé—´: backup_id={}",
        validated_backup_id
    );

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®".to_string());
    }

    // è¯»å–å¤‡ä»½æ¸…å•ä»¥è·å–å¤‡ä»½å¤§å°
    let manager = BackupManager::new(backup_dir.clone());
    let manifests = manager.list_backups().map_err(|e| {
        error!("[data_governance] è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e);
        format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e)
    })?;

    let manifest = manifests
        .iter()
        .find(|m| m.backup_id == validated_backup_id)
        .ok_or_else(|| format!("æœªæ‰¾åˆ°å¤‡ä»½: {}", validated_backup_id))?;

    let db_size: u64 = manifest.files.iter().map(|f| f.size).sum();
    let asset_size: u64 = manifest.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
    let backup_size = db_size + asset_size;

    // æ‰€éœ€ç©ºé—´ = å¤‡ä»½å¤§å° Ã— 2ï¼ˆè§£å‹ + æ¢å¤é¢„ç•™ï¼‰
    let required_bytes = backup_size.saturating_mul(2);

    // è·å–åº”ç”¨æ•°æ®ç›®å½•æ‰€åœ¨ç£ç›˜çš„å¯ç”¨ç©ºé—´
    let available_bytes =
        crate::backup_common::get_available_disk_space(&app_data_dir).map_err(|e| {
            error!("[data_governance] è·å–å¯ç”¨ç£ç›˜ç©ºé—´å¤±è´¥: {}", e);
            format!("è·å–å¯ç”¨ç£ç›˜ç©ºé—´å¤±è´¥: {}", e)
        })?;

    let has_enough_space = available_bytes >= required_bytes;

    info!(
        "[data_governance] ç£ç›˜ç©ºé—´æ£€æŸ¥: backup_size={}, required={}, available={}, enough={}",
        backup_size, required_bytes, available_bytes, has_enough_space
    );

    Ok(DiskSpaceCheckResponse {
        has_enough_space,
        available_bytes,
        required_bytes,
        backup_size,
    })
}

/// éªŒè¯å¤‡ä»½
///
/// éªŒè¯å¤‡ä»½æ–‡ä»¶çš„å®Œæ•´æ€§ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦éªŒè¯çš„å¤‡ä»½ ID
///
/// ## è¿”å›
/// - `BackupVerifyResponse`: éªŒè¯ç»“æœ
#[tauri::command]
pub async fn data_governance_verify_backup(
    app: tauri::AppHandle,
    backup_id: String,
) -> Result<BackupVerifyResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;
    info!("[data_governance] éªŒè¯å¤‡ä»½: {}", validated_backup_id);

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®".to_string());
    }

    let manager = BackupManager::new(backup_dir.clone());

    // å…¨å±€äº’æ–¥ï¼šé¿å…ä¸æ­£åœ¨è¿è¡Œçš„å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _permit = BACKUP_GLOBAL_LIMITER
        .clone()
        .acquire_owned()
        .await
        .map_err(|e| format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e))?;

    // è·å–å¤‡ä»½åˆ—è¡¨å¹¶æŸ¥æ‰¾æŒ‡å®šçš„å¤‡ä»½
    let manifests = manager
        .list_backups()
        .map_err(|e| format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e))?;

    let manifest = manifests
        .iter()
        .find(|m| m.backup_id == validated_backup_id)
        .ok_or_else(|| format!("å¤‡ä»½ä¸å­˜åœ¨: {}", validated_backup_id))?;

    let manifest_dir = backup_dir.join(&manifest.backup_id);
    ensure_existing_path_within_backup_dir(&manifest_dir, &backup_dir)?;

    // éªŒè¯å¤‡ä»½ï¼ˆåŒ…å«èµ„äº§ï¼‰
    let verify_result = manager.verify_with_assets(manifest);

    let (is_valid, checksum_match, errors) = match verify_result {
        Ok(result) => {
            let mut db_errors = result.database_errors;
            let checksum_match = db_errors.is_empty();
            for ae in result.asset_errors {
                db_errors.push(format!("èµ„äº§æ ¡éªŒå¤±è´¥ [{}]: {}", ae.path, ae.message));
            }
            (result.is_valid, checksum_match, db_errors)
        }
        Err(e) => {
            let error_msg = e.to_string();
            (false, false, vec![error_msg])
        }
    };

    // æ„å»ºæ¯ä¸ªæ•°æ®åº“çš„éªŒè¯çŠ¶æ€
    let databases_verified: Vec<DatabaseVerifyStatus> = manifest
        .files
        .iter()
        .filter_map(|f| {
            f.database_id.as_ref().map(|db_id| DatabaseVerifyStatus {
                id: db_id.clone(),
                is_valid,
                error: if is_valid {
                    None
                } else {
                    Some("æ ¡éªŒå¤±è´¥".to_string())
                },
            })
        })
        .collect();

    info!(
        "[data_governance] å¤‡ä»½éªŒè¯å®Œæˆ: id={}, is_valid={}",
        backup_id, is_valid
    );

    Ok(BackupVerifyResponse {
        is_valid,
        checksum_match,
        databases_verified,
        errors,
    })
}

/// è‡ªåŠ¨éªŒè¯æœ€æ–°å¤‡ä»½çš„å®Œæ•´æ€§
///
/// æ‰¾åˆ°æœ€æ–°çš„å¤‡ä»½ï¼Œæ‰§è¡Œå®Œæ•´æ€§éªŒè¯ï¼ˆPRAGMA integrity_check + SHA256 æ ¡éªŒå’Œï¼‰ï¼Œ
/// å°†éªŒè¯ç»“æœå†™å…¥å®¡è®¡æ—¥å¿—ï¼Œå¹¶è¿”å›éªŒè¯ç»“æœã€‚
///
/// ## è¿”å›
/// - `AutoVerifyResponse`: éªŒè¯ç»“æœï¼ŒåŒ…å«å¤‡ä»½ IDã€éªŒè¯çŠ¶æ€å’Œæ—¶é—´
#[tauri::command]
pub async fn data_governance_auto_verify_latest_backup(
    app: tauri::AppHandle,
) -> Result<AutoVerifyResponse, String> {
    info!("[data_governance] è‡ªåŠ¨éªŒè¯æœ€æ–°å¤‡ä»½å®Œæ•´æ€§");

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err(
            "å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨éªŒè¯ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®"
                .to_string(),
        );
    }

    let manager = BackupManager::new(backup_dir.clone());

    // å…¨å±€äº’æ–¥ï¼šé¿å…ä¸æ­£åœ¨è¿è¡Œçš„å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _permit = BACKUP_GLOBAL_LIMITER
        .clone()
        .acquire_owned()
        .await
        .map_err(|e| format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e))?;

    // è·å–å¤‡ä»½åˆ—è¡¨å¹¶æ‰¾åˆ°æœ€æ–°çš„å¤‡ä»½
    let manifests = manager
        .list_backups()
        .map_err(|e| format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e))?;

    if manifests.is_empty() {
        return Err("æ²¡æœ‰å¯ç”¨çš„å¤‡ä»½ï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨éªŒè¯ã€‚è¯·å…ˆåˆ›å»ºä¸€ä¸ªå¤‡ä»½".to_string());
    }

    // æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
    let latest_manifest = manifests
        .iter()
        .max_by(|a, b| a.created_at.cmp(&b.created_at))
        .ok_or_else(|| "æ— æ³•ç¡®å®šæœ€æ–°å¤‡ä»½".to_string())?;

    let backup_id = latest_manifest.backup_id.clone();
    let verified_at = chrono::Utc::now().to_rfc3339();
    let start = std::time::Instant::now();

    info!("[data_governance] è‡ªåŠ¨éªŒè¯å¤‡ä»½: {}", backup_id);

    // æ‰§è¡ŒéªŒè¯
    let verify_result = manager.verify_with_assets(latest_manifest);

    let duration_ms = start.elapsed().as_millis() as u64;

    let (is_valid, errors) = match verify_result {
        Ok(result) => {
            let mut all_errors = result.database_errors;
            for ae in result.asset_errors {
                all_errors.push(format!("èµ„äº§æ ¡éªŒå¤±è´¥ [{}]: {}", ae.path, ae.message));
            }
            (result.is_valid, all_errors)
        }
        Err(e) => (false, vec![e.to_string()]),
    };

    // æ„å»ºæ¯ä¸ªæ•°æ®åº“çš„éªŒè¯çŠ¶æ€
    let databases_verified: Vec<DatabaseVerifyStatus> = latest_manifest
        .files
        .iter()
        .filter_map(|f| {
            f.database_id.as_ref().map(|db_id| DatabaseVerifyStatus {
                id: db_id.clone(),
                is_valid,
                error: if is_valid {
                    None
                } else {
                    Some("æ ¡éªŒå¤±è´¥".to_string())
                },
            })
        })
        .collect();

    // å†™å…¥å®¡è®¡æ—¥å¿—
    #[cfg(feature = "data_governance")]
    {
        let auto_verify_size: u64 = latest_manifest.files.iter().map(|f| f.size).sum::<u64>()
            + latest_manifest
                .assets
                .as_ref()
                .map(|a| a.total_size)
                .unwrap_or(0);
        let audit_log = AuditLog::new(
            AuditOperation::Backup {
                backup_type: super::audit::BackupType::Auto,
                file_count: latest_manifest.files.len(),
                total_size: auto_verify_size,
            },
            format!("auto_verify/{}", backup_id),
        )
        .with_details(serde_json::json!({
            "action": "auto_verify",
            "backup_id": backup_id,
            "is_valid": is_valid,
            "databases_verified": databases_verified.len(),
            "errors": errors,
            "duration_ms": duration_ms,
        }));

        let audit_log = if is_valid {
            audit_log.complete(duration_ms)
        } else {
            audit_log.fail(errors.join("; "))
        };

        try_save_audit_log(&app, audit_log);
    }

    info!(
        "[data_governance] è‡ªåŠ¨éªŒè¯å®Œæˆ: backup_id={}, is_valid={}, duration={}ms",
        backup_id, is_valid, duration_ms
    );

    Ok(AutoVerifyResponse {
        backup_id,
        is_valid,
        verified_at,
        duration_ms,
        databases_verified,
        errors,
    })
}

/// è‡ªåŠ¨éªŒè¯å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct AutoVerifyResponse {
    /// è¢«éªŒè¯çš„å¤‡ä»½ ID
    pub backup_id: String,
    /// æ˜¯å¦é€šè¿‡éªŒè¯
    pub is_valid: bool,
    /// éªŒè¯æ—¶é—´ (ISO 8601)
    pub verified_at: String,
    /// éªŒè¯è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// æ•°æ®åº“éªŒè¯çŠ¶æ€
    pub databases_verified: Vec<DatabaseVerifyStatus>,
    /// é”™è¯¯åˆ—è¡¨
    pub errors: Vec<String>,
}

/// å¤‡ä»½ç»“æœå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct BackupResultResponse {
    pub success: bool,
    pub backup_path: String,
    pub backup_size: u64,
    pub duration_ms: u64,
    pub databases_backed_up: Vec<String>,
    /// èµ„äº§å¤‡ä»½æ‘˜è¦ï¼ˆå¦‚æœåŒ…å«èµ„äº§å¤‡ä»½ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub assets_backed_up: Option<AssetBackupSummary>,
}

/// èµ„äº§å¤‡ä»½æ‘˜è¦
#[derive(Debug, Clone, serde::Serialize)]
pub struct AssetBackupSummary {
    /// å¤‡ä»½çš„æ–‡ä»¶æ€»æ•°
    pub total_files: usize,
    /// å¤‡ä»½çš„æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub total_size: u64,
    /// æŒ‰èµ„äº§ç±»å‹ç»Ÿè®¡
    pub by_type: std::collections::HashMap<String, AssetTypeStats>,
}

/// å¤‡ä»½ä¿¡æ¯å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct BackupInfoResponse {
    pub path: String,
    pub created_at: String,
    pub size: u64,
    pub backup_type: String,
    pub databases: Vec<String>,
}

/// å¤‡ä»½éªŒè¯å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct BackupVerifyResponse {
    pub is_valid: bool,
    pub checksum_match: bool,
    pub databases_verified: Vec<DatabaseVerifyStatus>,
    pub errors: Vec<String>,
}

/// æ•°æ®åº“éªŒè¯çŠ¶æ€
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseVerifyStatus {
    pub id: String,
    pub is_valid: bool,
    pub error: Option<String>,
}

/// åå°å¤‡ä»½ä»»åŠ¡å¯åŠ¨å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct BackupJobStartResponse {
    /// ä»»åŠ¡ IDï¼Œç”¨äºæŸ¥è¯¢çŠ¶æ€å’Œå–æ¶ˆ
    pub job_id: String,
    /// ä»»åŠ¡ç±»å‹
    pub kind: String,
    /// åˆå§‹çŠ¶æ€
    pub status: String,
    /// æç¤ºæ¶ˆæ¯
    pub message: String,
}

/// ç£ç›˜ç©ºé—´æ£€æŸ¥å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct DiskSpaceCheckResponse {
    /// æ˜¯å¦æœ‰è¶³å¤Ÿç©ºé—´
    pub has_enough_space: bool,
    /// å¯ç”¨ç©ºé—´ï¼ˆå­—èŠ‚ï¼‰
    pub available_bytes: u64,
    /// éœ€è¦ç©ºé—´ï¼ˆå­—èŠ‚ï¼Œå«å®‰å…¨ä½™é‡ï¼‰
    pub required_bytes: u64,
    /// å¤‡ä»½å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub backup_size: u64,
}

// ==================== åå°å¤‡ä»½ä»»åŠ¡å‘½ä»¤ ====================

/// å¼‚æ­¥åå°å¤‡ä»½ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
///
/// å¯åŠ¨åå°å¤‡ä»½ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡ IDã€‚å¤‡ä»½è¿›åº¦é€šè¿‡ `backup-job-progress` äº‹ä»¶å‘é€ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_type`: å¤‡ä»½ç±»å‹ï¼Œ"full"ï¼ˆå®Œæ•´ï¼‰æˆ– "incremental"ï¼ˆå¢é‡ï¼‰
/// - `base_version`: å¢é‡å¤‡ä»½çš„åŸºç¡€ç‰ˆæœ¬ï¼ˆä»…å¢é‡å¤‡ä»½éœ€è¦ï¼‰
/// - `include_assets`: æ˜¯å¦åŒ…å«èµ„äº§æ–‡ä»¶å¤‡ä»½
/// - `asset_types`: è¦å¤‡ä»½çš„èµ„äº§ç±»å‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…¨éƒ¨ï¼‰
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ ID
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
#[tauri::command]
pub async fn data_governance_run_backup(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    backup_type: Option<String>,
    base_version: Option<String>,
    include_assets: Option<bool>,
    asset_types: Option<Vec<String>>,
) -> Result<BackupJobStartResponse, String> {
    let backup_type = backup_type.unwrap_or_else(|| "full".to_string());
    let include_assets = include_assets.unwrap_or(false);
    info!(
        "[data_governance] å¯åŠ¨åå°å¤‡ä»½ä»»åŠ¡: type={}, include_assets={}",
        backup_type, include_assets
    );

    // ä½¿ç”¨å…¨å±€å•ä¾‹å¤‡ä»½ä»»åŠ¡ç®¡ç†å™¨
    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Export);
    let job_id = job_ctx.job_id.clone();

    #[cfg(feature = "data_governance")]
    {
        let audit_backup_type = if backup_type == "incremental" {
            super::audit::BackupType::Incremental
        } else {
            super::audit::BackupType::Full
        };
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: audit_backup_type,
                    file_count: 0,
                    total_size: 0,
                },
                format!("governance_backup/{}", backup_type),
            )
            .with_details(serde_json::json!({
                "job_id": job_id.clone(),
                "backup_type": backup_type.clone(),
                "base_version": base_version.clone(),
                "include_assets": include_assets,
                "asset_types": asset_types.clone(),
            })),
        );
    }

    // åœ¨åå°æ‰§è¡Œå¤‡ä»½
    let app_clone = app.clone();
    let base_version_clone = base_version.clone();
    let asset_types_clone = asset_types.clone();

    tauri::async_runtime::spawn(async move {
        execute_backup_with_progress(
            app_clone,
            job_ctx,
            backup_type,
            base_version_clone,
            include_assets,
            asset_types_clone,
        )
        .await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "export".to_string(),
        status: "queued".to_string(),
        message: "å¤‡ä»½ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

/// æ‰§è¡Œå¤‡ä»½ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œå¸¦è¿›åº¦å›è°ƒï¼‰
async fn execute_backup_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    backup_type: String,
    base_version: Option<String>,
    include_assets: bool,
    asset_types: Option<Vec<String>>,
) {
    use super::backup::{AssetBackupConfig, AssetType, BackupManager};
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è®¾ç½®ä»»åŠ¡å‚æ•°ï¼ˆç”¨äºæŒä¹…åŒ–å’Œæ¢å¤ï¼‰
    job_ctx.set_params(BackupJobParams {
        backup_type: Some(backup_type.clone()),
        base_version: base_version.clone(),
        include_assets,
        asset_types: asset_types.clone(),
        ..Default::default()
    });

    // åˆå§‹åŒ–æ£€æŸ¥ç‚¹
    job_ctx.init_checkpoint(4); // 4 ä¸ªæ•°æ®åº“

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            let msg = format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("governance_backup/{}", job_ctx.job_id),
                    )
                    .fail(msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_id": job_ctx.job_id.clone(),
                        "subtype": "backup",
                    })),
                );
            }
            job_ctx.fail(msg);
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    // é˜¶æ®µ 1: å‡†å¤‡ä¸­
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        5.0,
        Some("æ­£åœ¨å‡†å¤‡å¤‡ä»½...".to_string()),
        0,
        4, // æ€»å…± 4 ä¸ªæ•°æ®åº“
    );

    // æ£€æŸ¥å–æ¶ˆ
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let mut manager = BackupManager::new(backup_dir);
    manager.set_app_data_dir(app_data_dir.clone());
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    // è®¾ç½®é€æ•°æ®åº“è¿›åº¦å›è°ƒï¼ˆé¡µé¢çº§ç»†ç²’åº¦ï¼‰
    {
        let job_ctx_clone = job_ctx.clone();
        manager.set_progress_callback(
            move |db_idx, total_dbs, db_name, pages_copied, pages_total| {
                // æ•´ä½“è¿›åº¦ï¼š15% ~ 75%ï¼ŒæŒ‰æ•°æ®åº“+é¡µé¢æ¯”ä¾‹ç»†åˆ†
                let db_fraction = if total_dbs > 0 {
                    db_idx as f32 / total_dbs as f32
                } else {
                    1.0
                };
                let page_fraction = if pages_total > 0 {
                    pages_copied as f32 / pages_total as f32
                } else {
                    0.0
                };
                let per_db = if total_dbs > 0 {
                    1.0 / total_dbs as f32
                } else {
                    1.0
                };
                let progress = 15.0 + (db_fraction + page_fraction * per_db) * 60.0;

                let msg = if pages_total > 0 {
                    format!(
                        "æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{}) - {:.0}%",
                        db_name,
                        db_idx + 1,
                        total_dbs,
                        page_fraction * 100.0
                    )
                } else {
                    format!("æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{})", db_name, db_idx + 1, total_dbs)
                };

                job_ctx_clone.mark_running(
                    BackupJobPhase::Compress,
                    progress,
                    Some(msg),
                    db_idx as u64,
                    total_dbs as u64,
                );
            },
        );
    }

    // é˜¶æ®µ 2: æ‰§è¡Œ checkpoint
    job_ctx.mark_running(
        BackupJobPhase::Checkpoint,
        10.0,
        Some("æ­£åœ¨æ‰§è¡Œæ•°æ®åº“ checkpoint...".to_string()),
        0,
        4,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // æ ¹æ®å¤‡ä»½ç±»å‹æ‰§è¡Œå¤‡ä»½
    let result = match backup_type.as_str() {
        "incremental" => {
            let base = match base_version {
                Some(v) => v,
                None => {
                    job_ctx.fail("å¢é‡å¤‡ä»½éœ€è¦æŒ‡å®š base_version å‚æ•°".to_string());
                    return;
                }
            };

            // é˜¶æ®µ 3: å¤åˆ¶æ•°æ®åº“
            job_ctx.mark_running(
                BackupJobPhase::Compress,
                30.0,
                Some("æ­£åœ¨æ‰§è¡Œå¢é‡å¤‡ä»½...".to_string()),
                0,
                4,
            );

            manager.backup_incremental(&base)
        }
        _ => {
            if include_assets {
                // æ„å»ºèµ„äº§å¤‡ä»½é…ç½®
                let asset_config = if let Some(types) = asset_types {
                    let parsed_types: Vec<AssetType> = types
                        .iter()
                        .filter_map(|s| AssetType::from_str(s))
                        .collect();
                    if parsed_types.is_empty() {
                        AssetBackupConfig::default()
                    } else {
                        AssetBackupConfig {
                            asset_types: parsed_types,
                            ..Default::default()
                        }
                    }
                } else {
                    AssetBackupConfig::default()
                };

                // é˜¶æ®µ 3: å¤åˆ¶æ•°æ®åº“å’Œèµ„äº§
                job_ctx.mark_running(
                    BackupJobPhase::Compress,
                    30.0,
                    Some("æ­£åœ¨å¤‡ä»½æ•°æ®åº“å’Œèµ„äº§æ–‡ä»¶...".to_string()),
                    0,
                    4,
                );

                manager.backup_with_assets(Some(asset_config))
            } else {
                // é˜¶æ®µ 3: å¤åˆ¶æ•°æ®åº“
                job_ctx.mark_running(
                    BackupJobPhase::Compress,
                    30.0,
                    Some("æ­£åœ¨å¤‡ä»½æ•°æ®åº“...".to_string()),
                    0,
                    4,
                );

                manager.backup_full()
            }
        }
    };

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // é˜¶æ®µ 4: éªŒè¯
    job_ctx.mark_running(
        BackupJobPhase::Verify,
        80.0,
        Some("æ­£åœ¨éªŒè¯å¤‡ä»½...".to_string()),
        3,
        4,
    );

    let duration_ms = start.elapsed().as_millis() as u64;

    match result {
        Ok(manifest) => {
            // è®¡ç®—å¤‡ä»½å¤§å°
            let db_size: u64 = manifest.files.iter().map(|f| f.size).sum();
            let asset_size: u64 = manifest.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
            let backup_size = db_size + asset_size;

            let databases_backed_up: Vec<String> = manifest
                .files
                .iter()
                .filter_map(|f| f.database_id.clone())
                .collect();

            info!(
                "[data_governance] åå°å¤‡ä»½æˆåŠŸ: id={}, files={}, size={}, duration={}ms",
                manifest.backup_id,
                manifest.files.len(),
                backup_size,
                duration_ms
            );

            #[cfg(feature = "data_governance")]
            {
                let audit_backup_type = if backup_type == "incremental" {
                    super::audit::BackupType::Incremental
                } else {
                    super::audit::BackupType::Full
                };
                let asset_files = manifest.assets.as_ref().map(|a| a.total_files).unwrap_or(0);
                let file_count = manifest.files.len() + asset_files;

                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: audit_backup_type,
                            file_count,
                            total_size: backup_size,
                        },
                        manifest.backup_id.clone(),
                    )
                    .complete(duration_ms)
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_type": backup_type.clone(),
                        "include_assets": include_assets,
                        "db_files": manifest.files.len(),
                        "asset_files": asset_files,
                        "db_size": db_size,
                        "asset_size": asset_size,
                    })),
                );
            }

            // å¤‡ä»½æˆåŠŸåè‡ªåŠ¨éªŒè¯å®Œæ•´æ€§
            let auto_verify_result = manager.verify_with_assets(&manifest);
            let (verify_is_valid, verify_errors): (bool, Vec<String>) = match auto_verify_result {
                Ok(result) => {
                    let mut all_errors = result.database_errors;
                    for ae in result.asset_errors {
                        all_errors.push(format!("èµ„äº§æ ¡éªŒå¤±è´¥ [{}]: {}", ae.path, ae.message));
                    }
                    (result.is_valid, all_errors)
                }
                Err(e) => (false, vec![e.to_string()]),
            };

            if verify_is_valid {
                info!(
                    "[data_governance] å¤‡ä»½åè‡ªåŠ¨éªŒè¯é€šè¿‡: {}",
                    manifest.backup_id
                );
            } else {
                warn!(
                    "[data_governance] å¤‡ä»½åè‡ªåŠ¨éªŒè¯å¤±è´¥: {}, errors={:?}",
                    manifest.backup_id, verify_errors
                );
            }

            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Auto,
                            file_count: manifest.files.len(),
                            total_size: backup_size,
                        },
                        format!("post_backup_verify/{}", manifest.backup_id),
                    )
                    .with_details(serde_json::json!({
                        "action": "post_backup_auto_verify",
                        "backup_id": manifest.backup_id.clone(),
                        "is_valid": verify_is_valid,
                        "errors": verify_errors,
                    }))
                    .complete(start.elapsed().as_millis() as u64),
                );
            }

            // æ„å»ºç»“æœ payload
            let verify_error = if verify_is_valid {
                None
            } else {
                Some("å¤‡ä»½å®Œæˆä½†æ ¡éªŒå¤±è´¥ï¼Œè¯·åœ¨å®¡è®¡é¡µæŸ¥çœ‹è¯¦æƒ…å¹¶é‡æ–°æ‰§è¡Œå¤‡ä»½ã€‚".to_string())
            };

            let result_payload = BackupJobResultPayload {
                success: verify_is_valid,
                output_path: Some(manifest.backup_id.clone()),
                resolved_path: None,
                message: Some(format!(
                    "å¤‡ä»½å®Œæˆ: {} ä¸ªæ•°æ®åº“, {} å­—èŠ‚",
                    databases_backed_up.len(),
                    backup_size
                )),
                error: verify_error,
                duration_ms: Some(duration_ms),
                stats: Some(serde_json::json!({
                    "databases_backed_up": databases_backed_up,
                    "backup_size": backup_size,
                    "db_files": manifest.files.len(),
                    "asset_files": manifest.assets.as_ref().map(|a| a.total_files).unwrap_or(0),
                    "auto_verify": {
                        "is_valid": verify_is_valid,
                        "errors": verify_errors,
                    },
                })),
                requires_restart: false,
                checkpoint_path: None,
                resumable_job_id: None,
            };

            job_ctx.complete(
                Some(format!("å¤‡ä»½å®Œæˆ: {}", manifest.backup_id)),
                databases_backed_up.len() as u64,
                databases_backed_up.len() as u64,
                result_payload,
            );
        }
        Err(e) => {
            error!("[data_governance] åå°å¤‡ä»½å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                let audit_backup_type = if backup_type == "incremental" {
                    super::audit::BackupType::Incremental
                } else {
                    super::audit::BackupType::Full
                };
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: audit_backup_type,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("governance_backup/{}", backup_type),
                    )
                    .fail(e.to_string())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_type": backup_type.clone(),
                        "include_assets": include_assets,
                    })),
                );
            }
            job_ctx.fail(format!("å¤‡ä»½å¤±è´¥: {}", e));
        }
    }
}

/// å–æ¶ˆå¤‡ä»½ä»»åŠ¡
///
/// è¯·æ±‚å–æ¶ˆæŒ‡å®šçš„å¤‡ä»½ä»»åŠ¡ã€‚ä»»åŠ¡ä¼šåœ¨ä¸‹ä¸€ä¸ªå®‰å…¨ç‚¹åœæ­¢ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `job_id`: ä»»åŠ¡ ID
///
/// ## è¿”å›
/// - `bool`: æ˜¯å¦æˆåŠŸè¯·æ±‚å–æ¶ˆ
#[tauri::command]
pub async fn data_governance_cancel_backup(
    backup_job_state: State<'_, BackupJobManagerState>,
    job_id: String,
) -> Result<bool, String> {
    info!("[data_governance] è¯·æ±‚å–æ¶ˆå¤‡ä»½ä»»åŠ¡: {}", job_id);

    let job_manager = backup_job_state.get();
    let cancelled = job_manager.request_cancel(&job_id);

    if cancelled {
        info!("[data_governance] å¤‡ä»½ä»»åŠ¡å–æ¶ˆè¯·æ±‚å·²å‘é€: {}", job_id);
    } else {
        warn!(
            "[data_governance] å¤‡ä»½ä»»åŠ¡å–æ¶ˆè¯·æ±‚å¤±è´¥ï¼ˆä»»åŠ¡å¯èƒ½å·²å®Œæˆæˆ–ä¸å­˜åœ¨ï¼‰: {}",
            job_id
        );
    }

    Ok(cancelled)
}

/// è·å–å¤‡ä»½ä»»åŠ¡çŠ¶æ€
///
/// æŸ¥è¯¢æŒ‡å®šå¤‡ä»½ä»»åŠ¡çš„å½“å‰çŠ¶æ€ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `job_id`: ä»»åŠ¡ ID
///
/// ## è¿”å›
/// - `BackupJobSummary`: ä»»åŠ¡æ‘˜è¦
#[tauri::command]
pub async fn data_governance_get_backup_job(
    backup_job_state: State<'_, BackupJobManagerState>,
    job_id: String,
) -> Result<Option<BackupJobSummary>, String> {
    let job_manager = backup_job_state.get();
    Ok(job_manager.get_job(&job_id))
}

/// è·å–æ‰€æœ‰å¤‡ä»½ä»»åŠ¡åˆ—è¡¨
///
/// è¿”å›æ‰€æœ‰å¤‡ä»½ä»»åŠ¡çš„æ‘˜è¦åˆ—è¡¨ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
///
/// ## è¿”å›
/// - `Vec<BackupJobSummary>`: ä»»åŠ¡åˆ—è¡¨
#[tauri::command]
pub async fn data_governance_list_backup_jobs(
    backup_job_state: State<'_, BackupJobManagerState>,
) -> Result<Vec<BackupJobSummary>, String> {
    let job_manager = backup_job_state.get();
    Ok(job_manager.list_jobs())
}

/// è·å–å¯æ¢å¤çš„å¤‡ä»½ä»»åŠ¡åˆ—è¡¨
///
/// è¿”å›æ‰€æœ‰å¯ä»¥æ¢å¤çš„å¤±è´¥å¤‡ä»½ä»»åŠ¡åˆ—è¡¨ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
///
/// ## è¿”å›
/// - `Vec<PersistedJob>`: å¯æ¢å¤çš„ä»»åŠ¡åˆ—è¡¨
#[tauri::command]
pub async fn data_governance_list_resumable_jobs(
    backup_job_state: State<'_, BackupJobManagerState>,
) -> Result<Vec<PersistedJob>, String> {
    let job_manager = backup_job_state.get();
    job_manager.list_resumable_jobs()
}

/// æ¢å¤ä¸­æ–­çš„å¤‡ä»½ä»»åŠ¡
///
/// æ ¹æ®ä»»åŠ¡ç±»å‹é‡‡å–ä¸åŒçš„æ¢å¤ç­–ç•¥ï¼š
/// - **å¯¼å‡ºï¼ˆExportï¼‰**ï¼šç”±äºå¤‡ä»½æ“ä½œæ˜¯åŸå­çš„ï¼Œæ¢å¤ = ä½¿ç”¨ç›¸åŒå‚æ•°é‡æ–°æ‰§è¡Œå®Œæ•´å¤‡ä»½
/// - **å¯¼å…¥ï¼ˆImport/ZIPï¼‰**ï¼šçœŸæ­£çš„æ–­ç‚¹ç»­ä¼ ï¼Œè·³è¿‡å·²è§£å‹ä¸”å¤§å°åŒ¹é…çš„æ–‡ä»¶
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `job_id`: è¦æ¢å¤çš„ä»»åŠ¡ ID
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ IDï¼ˆæ¢å¤ä»»åŠ¡ä½¿ç”¨åŸ IDï¼‰
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
///
/// ## æ³¨æ„
/// - åªèƒ½æ¢å¤å¤±è´¥çŠ¶æ€ä¸”æœ‰æ£€æŸ¥ç‚¹çš„ä»»åŠ¡
/// - æˆåŠŸæ¢å¤åï¼ŒåŸæŒä¹…åŒ–æ–‡ä»¶ä¼šåœ¨ä»»åŠ¡å®Œæˆæ—¶åˆ é™¤
#[tauri::command]
pub async fn data_governance_resume_backup_job(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    job_id: String,
) -> Result<BackupJobStartResponse, String> {
    info!("[data_governance] å°è¯•æ¢å¤å¤‡ä»½ä»»åŠ¡: job_id={}", job_id);

    let job_manager = backup_job_state.get();

    // åŠ è½½æŒä¹…åŒ–çš„ä»»åŠ¡
    let persisted_jobs = job_manager.load_persisted_jobs()?;
    let persisted = persisted_jobs
        .into_iter()
        .find(|j| j.job_id == job_id)
        .ok_or_else(|| format!("æœªæ‰¾åˆ°å¯æ¢å¤çš„ä»»åŠ¡: {}", job_id))?;

    // æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯æ¢å¤
    if persisted.status != BackupJobStatus::Failed {
        return Err(format!(
            "ä»»åŠ¡çŠ¶æ€ä¸º {:?}ï¼Œä»…å¤±è´¥çŠ¶æ€çš„ä»»åŠ¡å¯æ¢å¤ã€‚è¯·ç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–åˆ›å»ºæ–°ä»»åŠ¡",
            persisted.status
        ));
    }

    if persisted.checkpoint.is_none() {
        return Err("ä»»åŠ¡æ²¡æœ‰æ£€æŸ¥ç‚¹ä¿¡æ¯ï¼Œæ— æ³•æ¢å¤ã€‚è¯·åˆ›å»ºæ–°çš„å¤‡ä»½ä»»åŠ¡é‡è¯•".to_string());
    }

    // æ¢å¤ä»»åŠ¡ä¸Šä¸‹æ–‡
    let job_ctx = job_manager.restore_job_from_persisted(&persisted);
    let restored_job_id = job_ctx.job_id.clone();

    // æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œæ¢å¤
    match persisted.kind {
        BackupJobKind::Export => {
            // è§£æå‚æ•°
            let params: BackupJobParams =
                serde_json::from_value(persisted.params.clone()).unwrap_or_default();

            let app_clone = app.clone();
            tauri::async_runtime::spawn(async move {
                execute_backup_with_progress_resumable(
                    app_clone,
                    job_ctx,
                    params.backup_type.unwrap_or_else(|| "full".to_string()),
                    params.base_version,
                    params.include_assets,
                    params.asset_types,
                )
                .await;
            });

            Ok(BackupJobStartResponse {
                job_id: restored_job_id,
                kind: "export".to_string(),
                status: "queued".to_string(),
                message: "å¤‡ä»½ä»»åŠ¡å·²æ¢å¤ï¼Œå°†ä½¿ç”¨ç›¸åŒå‚æ•°é‡æ–°æ‰§è¡Œ".to_string(),
            })
        }
        BackupJobKind::Import => {
            // è§£æå‚æ•°
            let params: BackupJobParams =
                serde_json::from_value(persisted.params.clone()).unwrap_or_default();

            let zip_path = params
                .zip_path
                .ok_or_else(|| "å¯¼å…¥ä»»åŠ¡ç¼ºå°‘ ZIP è·¯å¾„å‚æ•°".to_string())?;
            let zip_file_path = PathBuf::from(&zip_path);

            if !zip_file_path.exists() {
                return Err(format!(
                    "ZIP æ–‡ä»¶ä¸å­˜åœ¨: {}ã€‚è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–é‡æ–°é€‰æ‹©æ–‡ä»¶",
                    sanitize_path_for_user(&zip_file_path)
                ));
            }

            let app_clone = app.clone();
            tauri::async_runtime::spawn(async move {
                execute_zip_import_with_progress_resumable(
                    app_clone,
                    job_ctx,
                    zip_file_path,
                    params.backup_id,
                )
                .await;
            });

            Ok(BackupJobStartResponse {
                job_id: restored_job_id,
                kind: "import".to_string(),
                status: "queued".to_string(),
                message: "å¯¼å…¥ä»»åŠ¡å·²æ¢å¤ï¼Œå°†ä»æ–­ç‚¹ç»§ç»­è§£å‹".to_string(),
            })
        }
    }
}

/// æ¸…ç†æ‰€æœ‰å·²å®Œæˆçš„æŒä¹…åŒ–ä»»åŠ¡
///
/// åˆ é™¤æ‰€æœ‰å·²å®Œæˆæˆ–å·²å–æ¶ˆçš„ä»»åŠ¡çš„æŒä¹…åŒ–æ–‡ä»¶ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
///
/// ## è¿”å›
/// - `usize`: æ¸…ç†çš„ä»»åŠ¡æ•°é‡
#[tauri::command]
pub async fn data_governance_cleanup_persisted_jobs(
    backup_job_state: State<'_, BackupJobManagerState>,
) -> Result<usize, String> {
    let job_manager = backup_job_state.get();
    job_manager.cleanup_finished_persisted_jobs()
}

// ==================== åˆ†å±‚å¤‡ä»½å‘½ä»¤ ====================

/// å¼‚æ­¥åˆ†å±‚å¤‡ä»½ï¼ˆåå°ä»»åŠ¡æ¨¡å¼ï¼‰
///
/// å¯åŠ¨åå°åˆ†å±‚å¤‡ä»½ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡ IDã€‚å¤‡ä»½è¿›åº¦é€šè¿‡ `backup-job-progress` äº‹ä»¶å‘é€ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `tiers`: è¦å¤‡ä»½çš„å±‚çº§åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»… Coreï¼‰
/// - `include_databases`: æ˜¾å¼åŒ…å«çš„æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
/// - `exclude_databases`: æ˜¾å¼æ’é™¤çš„æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
/// - `include_assets`: æ˜¯å¦åŒ…å«èµ„äº§æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤ falseï¼‰
/// - `max_asset_size`: æœ€å¤§èµ„äº§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼ˆå¯é€‰ï¼‰
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ ID
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
///
/// ## è¿›åº¦é˜¶æ®µ
/// - Scan (5%): æ‰«ææ•°æ®åº“å’Œèµ„äº§
/// - Checkpoint (15%): WAL checkpoint
/// - Compress (15-80%): æŒ‰å±‚çº§å¤‡ä»½æ•°æ®åº“ï¼ˆæ¯ä¸ªæ•°æ®åº“æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼‰
/// - Assets (80-95%): å¤‡ä»½èµ„äº§æ–‡ä»¶ï¼ˆå¦‚æœåŒ…å«ï¼‰
/// - Verify (95-100%): éªŒè¯å¤‡ä»½
#[tauri::command]
pub async fn data_governance_backup_tiered(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    tiers: Option<Vec<String>>,
    include_databases: Option<Vec<String>>,
    exclude_databases: Option<Vec<String>>,
    include_assets: Option<bool>,
    max_asset_size: Option<u64>,
    asset_types: Option<Vec<String>>,
) -> Result<BackupJobStartResponse, String> {
    info!(
        "[data_governance] å¯åŠ¨åå°åˆ†å±‚å¤‡ä»½ä»»åŠ¡: tiers={:?}, include_assets={:?}, asset_types={:?}",
        tiers, include_assets, asset_types
    );

    // ä½¿ç”¨å…¨å±€å•ä¾‹å¤‡ä»½ä»»åŠ¡ç®¡ç†å™¨
    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Export);
    let job_id = job_ctx.job_id.clone();

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Full,
                    file_count: 0,
                    total_size: 0,
                },
                "governance_backup/tiered".to_string(),
            )
            .with_details(serde_json::json!({
                "job_id": job_id.clone(),
                "tiers": tiers.clone(),
                "include_databases": include_databases.clone(),
                "exclude_databases": exclude_databases.clone(),
                "include_assets": include_assets.unwrap_or(false),
                "max_asset_size": max_asset_size,
            })),
        );
    }

    // åœ¨åå°æ‰§è¡Œåˆ†å±‚å¤‡ä»½
    let app_clone = app.clone();
    let tiers_clone = tiers.clone();
    let include_databases_clone = include_databases.clone();
    let exclude_databases_clone = exclude_databases.clone();
    let asset_types_clone = asset_types.clone();

    tauri::async_runtime::spawn(async move {
        execute_tiered_backup_with_progress(
            app_clone,
            job_ctx,
            tiers_clone,
            include_databases_clone,
            exclude_databases_clone,
            include_assets.unwrap_or(false),
            max_asset_size,
            asset_types_clone,
        )
        .await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "export".to_string(),
        status: "queued".to_string(),
        message: "åˆ†å±‚å¤‡ä»½ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

/// æ‰§è¡Œåˆ†å±‚å¤‡ä»½ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œå¸¦è¿›åº¦å›è°ƒï¼‰
async fn execute_tiered_backup_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    tiers: Option<Vec<String>>,
    include_databases: Option<Vec<String>>,
    exclude_databases: Option<Vec<String>>,
    include_assets: bool,
    max_asset_size: Option<u64>,
    asset_types: Option<Vec<String>>,
) {
    use super::backup::{BackupManager, BackupSelection, BackupTier, TieredAssetConfig};
    use super::schema_registry::DatabaseId;
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    // é˜¶æ®µ 1: æ‰«æ (5%)
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        5.0,
        Some("æ­£åœ¨æ‰«ææ•°æ®åº“å’Œèµ„äº§...".to_string()),
        0,
        0,
    );

    // æ£€æŸ¥å–æ¶ˆ
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // è§£æå±‚çº§å‚æ•°
    let parsed_tiers: Vec<BackupTier> = tiers
        .unwrap_or_else(|| vec!["core".to_string()])
        .iter()
        .filter_map(|t| match t.to_lowercase().as_str() {
            "core" => Some(BackupTier::Core),
            "important" => Some(BackupTier::Important),
            "rebuildable" => Some(BackupTier::Rebuildable),
            "large_assets" | "largeassets" => Some(BackupTier::LargeAssets),
            _ => {
                warn!("[data_governance] æœªçŸ¥çš„å¤‡ä»½å±‚çº§: {}", t);
                None
            }
        })
        .collect();

    // æ„å»ºèµ„äº§é…ç½®ï¼ˆæ”¯æŒ assetTypes ç­›é€‰ï¼‰
    let asset_config = if include_assets {
        let mut config = TieredAssetConfig {
            max_file_size: max_asset_size.unwrap_or(100 * 1024 * 1024),
            ..Default::default()
        };
        // å¦‚æœå‰ç«¯ä¼ å…¥äº† asset_typesï¼ŒæŒ‰ç±»å‹è¿‡æ»¤
        if let Some(types) = asset_types {
            let parsed_types: Vec<AssetType> = types
                .iter()
                .filter_map(|s| AssetType::from_str(s))
                .collect();
            if !parsed_types.is_empty() {
                config.asset_types = parsed_types;
            }
        }
        Some(config)
    } else {
        None
    };

    // æ„å»ºå¤‡ä»½é€‰æ‹©é…ç½®
    let selection = BackupSelection {
        tiers: parsed_tiers.clone(),
        include_databases: include_databases.unwrap_or_default(),
        exclude_databases: exclude_databases.unwrap_or_default(),
        include_assets,
        asset_config,
    };

    // è®¡ç®—éœ€è¦å¤‡ä»½çš„æ•°æ®åº“æ•°é‡
    let db_ids: Vec<DatabaseId> = DatabaseId::all_ordered()
        .into_iter()
        .filter(|db_id| selection.should_backup_database(db_id))
        .collect();
    let total_databases = db_ids.len();

    // é˜¶æ®µ 2: Checkpoint (15%)
    job_ctx.mark_running(
        BackupJobPhase::Checkpoint,
        15.0,
        Some("æ­£åœ¨æ‰§è¡Œæ•°æ®åº“ checkpoint...".to_string()),
        0,
        total_databases as u64,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let mut manager = BackupManager::new(backup_dir.clone());
    manager.set_app_data_dir(app_data_dir.clone());
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    // é˜¶æ®µ 3: å‹ç¼©/å¤‡ä»½æ•°æ®åº“ (15-80%)
    // é€šè¿‡è¿›åº¦å›è°ƒå®æ—¶æŠ¥å‘Šæ¯ä¸ªæ•°æ®åº“çš„å¤‡ä»½è¿›åº¦
    let db_progress_start = 15.0;
    let db_progress_end = if include_assets { 80.0 } else { 95.0 };
    let db_progress_range = db_progress_end - db_progress_start;

    {
        let job_ctx_clone = job_ctx.clone();
        manager.set_progress_callback(
            move |db_idx, total_dbs, db_name, pages_copied, pages_total| {
                // æ£€æŸ¥å–æ¶ˆ
                if job_ctx_clone.is_cancelled() {
                    return;
                }
                let db_fraction = if total_dbs > 0 {
                    db_idx as f32 / total_dbs as f32
                } else {
                    1.0
                };
                let page_fraction = if pages_total > 0 {
                    pages_copied as f32 / pages_total as f32
                } else {
                    0.0
                };
                let per_db = if total_dbs > 0 {
                    1.0 / total_dbs as f32
                } else {
                    1.0
                };
                let progress =
                    db_progress_start + (db_fraction + page_fraction * per_db) * db_progress_range;

                let msg = if pages_total > 0 {
                    format!(
                        "æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{}) - {:.0}%",
                        db_name,
                        db_idx + 1,
                        total_dbs,
                        page_fraction * 100.0
                    )
                } else {
                    format!("æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{})", db_name, db_idx + 1, total_dbs)
                };

                job_ctx_clone.mark_running(
                    BackupJobPhase::Compress,
                    progress,
                    Some(msg),
                    db_idx as u64,
                    total_dbs as u64,
                );
            },
        );
    }

    // æ‰§è¡Œå®é™…çš„åˆ†å±‚å¤‡ä»½
    let result = match manager.backup_tiered(&selection) {
        Ok(r) => r,
        Err(e) => {
            error!("[data_governance] åˆ†å±‚å¤‡ä»½å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        "governance_backup/tiered".to_string(),
                    )
                    .fail(e.to_string())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "include_assets": include_assets,
                        "tiers": parsed_tiers.iter().map(|t| format!("{:?}", t)).collect::<Vec<_>>(),
                    })),
                );
            }
            job_ctx.fail(format!("åˆ†å±‚å¤‡ä»½å¤±è´¥: {}", e));
            return;
        }
    };

    // é˜¶æ®µ 4: èµ„äº§å¤‡ä»½ (80-95%) - ä»…åœ¨åŒ…å«èµ„äº§æ—¶
    if include_assets {
        job_ctx.mark_running(
            BackupJobPhase::Compress,
            90.0,
            Some("æ­£åœ¨å¤‡ä»½èµ„äº§æ–‡ä»¶...".to_string()),
            total_databases as u64,
            total_databases as u64,
        );

        if job_ctx.is_cancelled() {
            job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
            return;
        }
    }

    // é˜¶æ®µ 5: éªŒè¯ (95-100%)
    job_ctx.mark_running(
        BackupJobPhase::Verify,
        95.0,
        Some("æ­£åœ¨éªŒè¯å¤‡ä»½...".to_string()),
        total_databases as u64,
        total_databases as u64,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // æ„å»ºç»“æœç»Ÿè®¡
    let duration_ms = start.elapsed().as_millis() as u64;
    let total_size: u64 = result.manifest.files.iter().map(|f| f.size).sum();

    // åˆ†å±‚å¤‡ä»½æˆåŠŸåè‡ªåŠ¨éªŒè¯å®Œæ•´æ€§
    let auto_verify_result = manager.verify(&result.manifest);
    let verify_is_valid = auto_verify_result.is_ok();
    let verify_errors: Vec<String> = match &auto_verify_result {
        Ok(()) => vec![],
        Err(e) => vec![e.to_string()],
    };

    if verify_is_valid {
        info!(
            "[data_governance] åˆ†å±‚å¤‡ä»½åè‡ªåŠ¨éªŒè¯é€šè¿‡: {}",
            result.manifest.backup_id
        );
    } else {
        warn!(
            "[data_governance] åˆ†å±‚å¤‡ä»½åè‡ªåŠ¨éªŒè¯å¤±è´¥: {}, errors={:?}",
            result.manifest.backup_id, verify_errors
        );
    }

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Auto,
                    file_count: result.manifest.files.len(),
                    total_size,
                },
                format!("post_backup_verify/{}", result.manifest.backup_id),
            )
            .with_details(serde_json::json!({
                "action": "post_backup_auto_verify",
                "backup_id": result.manifest.backup_id.clone(),
                "is_valid": verify_is_valid,
                "errors": verify_errors,
            }))
            .complete(start.elapsed().as_millis() as u64),
        );
    }

    // æ„å»ºç»“æœ payload
    let stats = serde_json::json!({
        "backup_id": result.manifest.backup_id,
        "backed_up_tiers": result.backed_up_tiers.iter().map(|t| format!("{:?}", t)).collect::<Vec<_>>(),
        "tier_file_counts": result.tier_file_counts,
        "tier_sizes": result.tier_sizes,
        "total_files": result.manifest.files.len(),
        "total_size": total_size,
        "skipped_files_count": result.skipped_files.len(),
        "auto_verify": {
            "is_valid": verify_is_valid,
            "errors": verify_errors,
        },
    });

    let verify_error = if verify_is_valid {
        None
    } else {
        Some("åˆ†å±‚å¤‡ä»½å®Œæˆä½†æ ¡éªŒå¤±è´¥ï¼Œè¯·åœ¨å®¡è®¡é¡µæŸ¥çœ‹è¯¦æƒ…å¹¶é‡æ–°æ‰§è¡Œå¤‡ä»½ã€‚".to_string())
    };

    let result_payload = BackupJobResultPayload {
        success: verify_is_valid,
        output_path: Some(
            backup_dir
                .join(&result.manifest.backup_id)
                .to_string_lossy()
                .to_string(),
        ),
        resolved_path: None,
        message: Some(format!(
            "åˆ†å±‚å¤‡ä»½å®Œæˆï¼Œå…± {} ä¸ªæ–‡ä»¶ï¼Œå¤§å° {} å­—èŠ‚",
            result.manifest.files.len(),
            total_size
        )),
        error: verify_error,
        duration_ms: Some(duration_ms),
        stats: Some(stats),
        requires_restart: false,
        checkpoint_path: None,
        resumable_job_id: None,
    };

    info!(
        "[data_governance] åˆ†å±‚å¤‡ä»½æˆåŠŸ: id={}, files={}, duration={}ms",
        result.manifest.backup_id,
        result.manifest.files.len(),
        duration_ms
    );

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Full,
                    file_count: result.manifest.files.len(),
                    total_size,
                },
                result.manifest.backup_id.clone(),
            )
            .complete(duration_ms)
            .with_details(serde_json::json!({
                "job_id": job_ctx.job_id.clone(),
                "include_assets": include_assets,
                "tiers": parsed_tiers.iter().map(|t| format!("{:?}", t)).collect::<Vec<_>>(),
                "tier_file_counts": result.tier_file_counts,
                "tier_sizes": result.tier_sizes,
                "skipped_files_count": result.skipped_files.len(),
            })),
        );
    }

    job_ctx.complete(
        Some(format!(
            "åˆ†å±‚å¤‡ä»½å®Œæˆ: {}ï¼Œå…± {} ä¸ªæ–‡ä»¶",
            result.manifest.backup_id,
            result.manifest.files.len()
        )),
        result.manifest.files.len() as u64,
        result.manifest.files.len() as u64,
        result_payload,
    );
}

// ==================== ZIP å¯¼å‡ºå‘½ä»¤ ====================

/// ä¸€æ­¥å®Œæˆã€Œå¤‡ä»½ + å¯¼å‡º ZIPã€ï¼ˆåå°ä»»åŠ¡æ¨¡å¼ï¼‰
///
/// é»˜è®¤è¡Œä¸ºï¼šå®Œæ•´å¤‡ä»½ï¼ˆæ•°æ®åº“ + èµ„äº§ï¼‰åç›´æ¥å¯¼å‡ºåˆ°æŒ‡å®š ZIP è·¯å¾„ã€‚
/// è‹¥ `use_tiered=true`ï¼Œåˆ™æŒ‰åˆ†å±‚å‚æ•°æ‰§è¡Œå¤‡ä»½åå¯¼å‡º ZIPã€‚
#[tauri::command]
pub async fn data_governance_backup_and_export_zip(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    output_path: String,
    compression_level: Option<u32>,
    add_to_backup_list: Option<bool>,
    use_tiered: Option<bool>,
    tiers: Option<Vec<String>>,
    include_assets: Option<bool>,
    asset_types: Option<Vec<String>>,
) -> Result<BackupJobStartResponse, String> {
    let app_data_dir = get_app_data_dir(&app)?;
    let user_output = PathBuf::from(&output_path);
    validate_user_path(&user_output, &app_data_dir)?;

    let compression_level = compression_level.unwrap_or(6).min(9);
    let add_to_backup_list = add_to_backup_list.unwrap_or(true);
    let use_tiered = use_tiered.unwrap_or(false);

    info!(
        "[data_governance] å¯åŠ¨åå°å¤‡ä»½å¹¶å¯¼å‡º ZIP ä»»åŠ¡: output_path={}, compression={}, add_to_backup_list={}, use_tiered={}",
        sanitize_path_for_user(&user_output),
        compression_level,
        add_to_backup_list,
        use_tiered
    );

    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Export);
    let job_id = job_ctx.job_id.clone();

    let app_clone = app.clone();
    tauri::async_runtime::spawn(async move {
        execute_backup_and_export_zip_with_progress(
            app_clone,
            job_ctx,
            output_path,
            compression_level,
            add_to_backup_list,
            use_tiered,
            tiers,
            include_assets,
            asset_types,
        )
        .await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "export".to_string(),
        status: "queued".to_string(),
        message: "å¤‡ä»½å¯¼å‡ºä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

async fn execute_backup_and_export_zip_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    output_path: String,
    compression_level: u32,
    add_to_backup_list: bool,
    use_tiered: bool,
    tiers: Option<Vec<String>>,
    include_assets: Option<bool>,
    asset_types: Option<Vec<String>>,
) {
    use super::backup::BackupTier;

    let start = Instant::now();

    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    job_ctx.set_params(BackupJobParams {
        backup_type: Some(if use_tiered {
            "tiered".to_string()
        } else {
            "full".to_string()
        }),
        include_assets: include_assets.unwrap_or(!use_tiered),
        asset_types: asset_types.clone(),
        output_path: Some(output_path.clone()),
        compression_level: Some(compression_level),
        include_checksums: true,
        ..Default::default()
    });

    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    let mut manager = BackupManager::new(backup_dir.clone());
    manager.set_app_data_dir(app_data_dir);
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    job_ctx.mark_running(
        BackupJobPhase::Scan,
        2.0,
        Some("æ­£åœ¨å‡†å¤‡å¤‡ä»½...".to_string()),
        0,
        1,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½å¯¼å‡º".to_string()));
        return;
    }

    let backup_progress_start = 5.0;
    let backup_progress_end = 60.0;
    let backup_progress_range = backup_progress_end - backup_progress_start;
    {
        let job_ctx_clone = job_ctx.clone();
        manager.set_progress_callback(
            move |db_idx, total_dbs, db_name, pages_copied, pages_total| {
                let db_fraction = if total_dbs > 0 {
                    db_idx as f32 / total_dbs as f32
                } else {
                    1.0
                };
                let page_fraction = if pages_total > 0 {
                    pages_copied as f32 / pages_total as f32
                } else {
                    0.0
                };
                let per_db = if total_dbs > 0 {
                    1.0 / total_dbs as f32
                } else {
                    1.0
                };
                let progress = backup_progress_start
                    + (db_fraction + page_fraction * per_db) * backup_progress_range;
                let msg = if pages_total > 0 {
                    format!(
                        "æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{}) - {:.0}%",
                        db_name,
                        db_idx + 1,
                        total_dbs,
                        page_fraction * 100.0
                    )
                } else {
                    format!("æ­£åœ¨å¤‡ä»½æ•°æ®åº“: {} ({}/{})", db_name, db_idx + 1, total_dbs)
                };

                job_ctx_clone.mark_running(
                    BackupJobPhase::Checkpoint,
                    progress,
                    Some(msg),
                    db_idx as u64,
                    total_dbs as u64,
                );
            },
        );
    }

    let include_assets = include_assets.unwrap_or(!use_tiered);

    let backup_result: Result<String, String> = if use_tiered {
        let parsed_tiers: Vec<BackupTier> = tiers
            .unwrap_or_else(|| vec!["core".to_string()])
            .into_iter()
            .filter_map(|tier| match tier.to_lowercase().as_str() {
                "core" => Some(BackupTier::Core),
                "important" => Some(BackupTier::Important),
                "rebuildable" => Some(BackupTier::Rebuildable),
                "large_assets" | "largeassets" => Some(BackupTier::LargeAssets),
                other => {
                    warn!("[data_governance] æœªçŸ¥åˆ†å±‚å¤‡ä»½å±‚çº§: {}", other);
                    None
                }
            })
            .collect();

        if parsed_tiers.is_empty() {
            job_ctx.fail("åˆ†å±‚å¤‡ä»½è‡³å°‘éœ€è¦ä¸€ä¸ªæœ‰æ•ˆå±‚çº§".to_string());
            return;
        }

        let tiered_asset_config = if include_assets {
            let mut config = TieredAssetConfig::default();
            if let Some(types) = asset_types.clone() {
                let parsed_types: Vec<AssetType> = types
                    .iter()
                    .filter_map(|s| AssetType::from_str(s))
                    .collect();
                if !parsed_types.is_empty() {
                    config.asset_types = parsed_types;
                }
            }
            Some(config)
        } else {
            None
        };

        let selection = BackupSelection {
            tiers: parsed_tiers,
            include_databases: vec![],
            exclude_databases: vec![],
            include_assets,
            asset_config: tiered_asset_config,
        };

        manager
            .backup_tiered(&selection)
            .map(|result| result.manifest.backup_id)
            .map_err(|e| format!("åˆ†å±‚å¤‡ä»½å¤±è´¥: {}", e))
    } else if include_assets {
        let asset_config = if let Some(types) = asset_types.clone() {
            let parsed_types: Vec<AssetType> = types
                .iter()
                .filter_map(|s| AssetType::from_str(s))
                .collect();
            if parsed_types.is_empty() {
                AssetBackupConfig::default()
            } else {
                AssetBackupConfig {
                    asset_types: parsed_types,
                    ..Default::default()
                }
            }
        } else {
            AssetBackupConfig::default()
        };

        manager
            .backup_with_assets(Some(asset_config))
            .map(|manifest| manifest.backup_id)
            .map_err(|e| format!("å®Œæ•´å¤‡ä»½å¤±è´¥: {}", e))
    } else {
        manager
            .backup_full()
            .map(|manifest| manifest.backup_id)
            .map_err(|e| format!("å¤‡ä»½å¤±è´¥: {}", e))
    };

    let backup_id = match backup_result {
        Ok(id) => id,
        Err(err) => {
            job_ctx.fail(err);
            return;
        }
    };

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½å¯¼å‡º".to_string()));
        return;
    }

    let source_backup_dir = backup_dir.join(&backup_id);
    if let Err(e) = ensure_existing_path_within_backup_dir(&source_backup_dir, &backup_dir) {
        job_ctx.fail(format!("å¤‡ä»½è·¯å¾„æ ¡éªŒå¤±è´¥: {}", e));
        return;
    }

    job_ctx.mark_running(
        BackupJobPhase::Compress,
        62.0,
        Some("æ­£åœ¨å‹ç¼© ZIP æ–‡ä»¶...".to_string()),
        0,
        1,
    );

    let export_result = export_backup_to_zip(
        &source_backup_dir,
        &ZipExportOptions {
            output_path: Some(PathBuf::from(&output_path)),
            compression_level,
            include_checksums: true,
            ..Default::default()
        },
    );

    let export_result = match export_result {
        Ok(result) => result,
        Err(e) => {
            job_ctx.fail(format!("ZIP å¯¼å‡ºå¤±è´¥: {}", e));
            return;
        }
    };

    job_ctx.mark_running(
        BackupJobPhase::Verify,
        96.0,
        Some("æ­£åœ¨å®Œæˆå¯¼å‡º...".to_string()),
        1,
        1,
    );

    if !add_to_backup_list {
        if let Err(e) = manager.delete_backup(&backup_id) {
            warn!(
                "[data_governance] å¤‡ä»½å·²å¯¼å‡ºä½†æ¸…ç†ä¸­é—´ç›®å½•å¤±è´¥: {} - {}",
                backup_id, e
            );
        }
    }

    let duration_ms = start.elapsed().as_millis() as u64;
    let result_payload = BackupJobResultPayload {
        success: true,
        output_path: Some(export_result.zip_path.to_string_lossy().to_string()),
        resolved_path: None,
        message: Some(format!(
            "å¤‡ä»½å¹¶å¯¼å‡ºå®Œæˆ: {} ä¸ªæ–‡ä»¶ï¼Œ{} å­—èŠ‚",
            export_result.file_count, export_result.compressed_size
        )),
        error: None,
        duration_ms: Some(duration_ms),
        stats: Some(serde_json::json!({
            "backup_id": backup_id,
            "zip_path": export_result.zip_path,
            "compression_level": compression_level,
            "compression_ratio": export_result.compression_ratio(),
            "add_to_backup_list": add_to_backup_list,
            "use_tiered": use_tiered,
            "include_assets": include_assets,
        })),
        requires_restart: false,
        checkpoint_path: None,
        resumable_job_id: None,
    };

    job_ctx.complete(
        Some("å¤‡ä»½å¹¶å¯¼å‡º ZIP å®Œæˆ".to_string()),
        1,
        1,
        result_payload,
    );
}

/// å¼‚æ­¥å¯¼å‡ºå¤‡ä»½ä¸º ZIP æ–‡ä»¶ï¼ˆåå°ä»»åŠ¡æ¨¡å¼ï¼‰
///
/// å°†å¤‡ä»½ç›®å½•å¼‚æ­¥å‹ç¼©ä¸º ZIP æ–‡ä»¶ï¼Œæ”¯æŒè¿›åº¦äº‹ä»¶å’Œå–æ¶ˆæ“ä½œã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: å¤‡ä»½ IDï¼ˆå¤‡ä»½ç›®å½•åï¼‰
/// - `output_path`: è¾“å‡º ZIP æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
/// - `compression_level`: å‹ç¼©çº§åˆ« 0-9ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 6ï¼‰
/// - `include_checksums`: æ˜¯å¦åŒ…å«æ ¡éªŒå’Œæ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤ trueï¼‰
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ ID çš„å“åº”
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
#[tauri::command]
pub async fn data_governance_export_zip(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    backup_id: String,
    output_path: Option<String>,
    compression_level: Option<u32>,
    include_checksums: Option<bool>,
) -> Result<BackupJobStartResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;

    // P0-4: å¯¹ç”¨æˆ·æŒ‡å®šçš„ output_path è¿›è¡Œå®‰å…¨æ ¡éªŒ
    if let Some(ref p) = output_path {
        let app_data_dir = get_app_data_dir(&app)?;
        let user_output = std::path::PathBuf::from(p);
        validate_user_path(&user_output, &app_data_dir)?;
    }

    info!(
        "[data_governance] å¯åŠ¨åå° ZIP å¯¼å‡ºä»»åŠ¡: backup_id={}, output_path={:?}",
        validated_backup_id, output_path
    );

    // ä½¿ç”¨å…¨å±€å•ä¾‹å¤‡ä»½ä»»åŠ¡ç®¡ç†å™¨
    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Export);
    let job_id = job_ctx.job_id.clone();

    // å‡†å¤‡å‚æ•°
    let compression_level = compression_level.unwrap_or(6).min(9);
    let include_checksums = include_checksums.unwrap_or(true);

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Full,
                    file_count: 0,
                    total_size: 0,
                },
                format!("zip_export/{}", validated_backup_id),
            )
            .with_details(serde_json::json!({
                "job_id": job_id.clone(),
                "backup_id": validated_backup_id.clone(),
                "compression_level": compression_level,
                "include_checksums": include_checksums,
                "output_path": output_path.clone(),
                "subtype": "zip_export",
            })),
        );
    }

    // åœ¨åå°æ‰§è¡Œ ZIP å¯¼å‡º
    tauri::async_runtime::spawn(async move {
        execute_zip_export_with_progress(
            app,
            job_ctx,
            validated_backup_id,
            output_path,
            compression_level,
            include_checksums,
        )
        .await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "export".to_string(),
        status: "queued".to_string(),
        message: "ZIP å¯¼å‡ºä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

/// æ‰§è¡Œ ZIP å¯¼å‡ºï¼ˆå†…éƒ¨å‡½æ•°ï¼Œå¸¦è¿›åº¦å›è°ƒï¼‰
async fn execute_zip_export_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    backup_id: String,
    output_path: Option<String>,
    compression_level: u32,
    include_checksums: bool,
) {
    use sha2::{Digest, Sha256};
    use std::fs::File;
    use std::io::{BufReader, Read, Write};
    use std::time::Instant;
    use walkdir::WalkDir;
    use zip::write::FileOptions;
    use zip::CompressionMethod;
    use zip::ZipWriter;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    let source_backup_dir = backup_dir.join(&backup_id);
    if !source_backup_dir.exists() {
        let msg = format!("å¤‡ä»½ä¸å­˜åœ¨: {}", backup_id);
        #[cfg(feature = "data_governance")]
        {
            try_save_audit_log(
                &app,
                AuditLog::new(
                    AuditOperation::Backup {
                        backup_type: super::audit::BackupType::Full,
                        file_count: 0,
                        total_size: 0,
                    },
                    format!("zip_export/{}", backup_id),
                )
                .fail(msg.clone())
                .with_details(serde_json::json!({
                    "job_id": job_ctx.job_id.clone(),
                    "backup_id": backup_id.clone(),
                    "subtype": "zip_export",
                })),
            );
        }
        job_ctx.fail(msg);
        return;
    }

    if let Err(e) = ensure_existing_path_within_backup_dir(&source_backup_dir, &backup_dir) {
        let msg = format!("å¤‡ä»½è·¯å¾„æ ¡éªŒå¤±è´¥: {}", e);
        #[cfg(feature = "data_governance")]
        {
            try_save_audit_log(
                &app,
                AuditLog::new(
                    AuditOperation::Backup {
                        backup_type: super::audit::BackupType::Full,
                        file_count: 0,
                        total_size: 0,
                    },
                    format!("zip_export/{}", backup_id),
                )
                .fail(msg.clone())
                .with_details(serde_json::json!({
                    "job_id": job_ctx.job_id.clone(),
                    "backup_id": backup_id.clone(),
                    "subtype": "zip_export",
                })),
            );
        }
        job_ctx.fail(msg);
        return;
    }

    // ========== é˜¶æ®µ 1: æ‰«æ (0-5%) ==========
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        0.0,
        Some("æ­£åœ¨æ‰«æå¤‡ä»½ç›®å½•...".to_string()),
        0,
        0,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆ ZIP å¯¼å‡º".to_string()));
        return;
    }

    // æ‰«æç›®å½•ï¼Œç»Ÿè®¡æ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°
    let mut files_to_compress: Vec<(PathBuf, String)> = Vec::new();
    let mut total_size: u64 = 0;

    for entry in WalkDir::new(&source_backup_dir)
        .into_iter()
        .filter_map(log_and_skip_entry_err)
    {
        let path = entry.path();
        let relative_path = match path.strip_prefix(&source_backup_dir) {
            Ok(p) => p,
            Err(_) => continue,
        };

        // è·³è¿‡ç©ºè·¯å¾„ï¼ˆæ ¹ç›®å½•ï¼‰
        if relative_path.as_os_str().is_empty() {
            continue;
        }

        let relative_path_str = relative_path.to_string_lossy().replace('\\', "/");

        if entry.file_type().is_file() {
            if let Ok(metadata) = entry.metadata() {
                total_size += metadata.len();
            }
            files_to_compress.push((path.to_path_buf(), relative_path_str));
        } else if entry.file_type().is_dir() {
            // ç›®å½•ä¹Ÿéœ€è¦è®°å½•ï¼Œä½†ä¸è®¡å…¥æ–‡ä»¶æ•°
            files_to_compress.push((path.to_path_buf(), relative_path_str));
        }
    }

    let total_files = files_to_compress
        .iter()
        .filter(|(p, _)| p.is_file())
        .count();

    job_ctx.mark_running(
        BackupJobPhase::Scan,
        5.0,
        Some(format!(
            "æ‰«æå®Œæˆ: {} ä¸ªæ–‡ä»¶, {} å­—èŠ‚",
            total_files, total_size
        )),
        0,
        total_files as u64,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆ ZIP å¯¼å‡º".to_string()));
        return;
    }

    // ========== é˜¶æ®µ 2: å‹ç¼© (5-90%) ==========
    // ç¡®å®šè¾“å‡ºè·¯å¾„
    let zip_path = match output_path {
        Some(path) => PathBuf::from(path),
        None => backup_dir.join(format!("{}.zip", backup_id)),
    };

    // ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if let Some(parent) = zip_path.parent() {
        if let Err(e) = std::fs::create_dir_all(parent) {
            let msg = format!("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_export/{}", backup_id),
                    )
                    .fail(msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_id": backup_id.clone(),
                        "subtype": "zip_export",
                        "zip_path": zip_path.to_string_lossy(),
                    })),
                );
            }
            job_ctx.fail(msg);
            return;
        }
    }

    // åˆ›å»º ZIP æ–‡ä»¶
    let zip_file = match File::create(&zip_path) {
        Ok(f) => f,
        Err(e) => {
            let msg = format!("åˆ›å»º ZIP æ–‡ä»¶å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_export/{}", backup_id),
                    )
                    .fail(msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_id": backup_id.clone(),
                        "subtype": "zip_export",
                        "zip_path": zip_path.to_string_lossy(),
                    })),
                );
            }
            job_ctx.fail(msg);
            return;
        }
    };
    let mut zip_writer = ZipWriter::new(zip_file);

    // é…ç½®å‹ç¼©é€‰é¡¹
    let compression_method = if compression_level == 0 {
        CompressionMethod::Stored
    } else {
        CompressionMethod::Deflated
    };
    let file_options = FileOptions::default().compression_method(compression_method);

    let mut compressed_files: usize = 0;
    let mut checksums: Vec<(String, String)> = Vec::new();
    let mut skipped_files: Vec<String> = Vec::new();

    for (path, relative_path_str) in &files_to_compress {
        // æ£€æŸ¥å–æ¶ˆ
        if job_ctx.is_cancelled() {
            // æ¸…ç†æœªå®Œæˆçš„ ZIP æ–‡ä»¶
            drop(zip_writer);
            let _ = std::fs::remove_file(&zip_path);
            job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆ ZIP å¯¼å‡º".to_string()));
            return;
        }

        if path.is_dir() {
            // æ·»åŠ ç›®å½•
            if let Err(e) = zip_writer.add_directory(relative_path_str, file_options) {
                warn!("[zip_export] æ·»åŠ ç›®å½•å¤±è´¥: {} - {}", relative_path_str, e);
            }
        } else if path.is_file() {
            // æ·»åŠ æ–‡ä»¶
            let mut file = match File::open(path) {
                Ok(f) => f,
                Err(e) => {
                    warn!("[zip_export] æ‰“å¼€æ–‡ä»¶å¤±è´¥: {:?} - {}", path, e);
                    skipped_files.push(format!("{}: {}", relative_path_str, e));
                    continue;
                }
            };

            // è®¡ç®—æ ¡éªŒå’Œï¼ˆå¦‚æœéœ€è¦ï¼‰
            if include_checksums {
                if let Ok(checksum) = crate::backup_common::calculate_file_hash(path) {
                    checksums.push((relative_path_str.clone(), checksum));
                }
            }

            // å†™å…¥ ZIP
            if let Err(e) = zip_writer.start_file(relative_path_str, file_options) {
                warn!(
                    "[zip_export] å¼€å§‹å†™å…¥æ–‡ä»¶å¤±è´¥: {} - {}",
                    relative_path_str, e
                );
                skipped_files.push(format!("{}: {}", relative_path_str, e));
                continue;
            }

            if let Err(e) = std::io::copy(&mut file, &mut zip_writer) {
                warn!("[zip_export] å†™å…¥ ZIP å¤±è´¥: {} - {}", relative_path_str, e);
                skipped_files.push(format!("{}: {}", relative_path_str, e));
                continue;
            }

            compressed_files += 1;

            // æ›´æ–°è¿›åº¦ (5% - 90%)
            let progress = 5.0 + (compressed_files as f32 / total_files.max(1) as f32) * 85.0;
            job_ctx.mark_running(
                BackupJobPhase::Compress,
                progress,
                Some(format!(
                    "æ­£åœ¨å‹ç¼©: {}/{} ({:.1}%)",
                    compressed_files, total_files, progress
                )),
                compressed_files as u64,
                total_files as u64,
            );
        }
    }

    // å¦‚æœéœ€è¦ï¼Œæ·»åŠ æ ¡éªŒå’Œæ–‡ä»¶
    if include_checksums && !checksums.is_empty() {
        let checksums_content = checksums
            .iter()
            .map(|(path, hash)| format!("{}  {}", hash, path))
            .collect::<Vec<_>>()
            .join("\n");

        if let Err(e) = zip_writer.start_file("checksums.sha256", file_options) {
            warn!("[zip_export] æ·»åŠ æ ¡éªŒå’Œæ–‡ä»¶å¤±è´¥: {}", e);
        } else if let Err(e) = zip_writer.write_all(checksums_content.as_bytes()) {
            warn!("[zip_export] å†™å…¥æ ¡éªŒå’Œæ–‡ä»¶å¤±è´¥: {}", e);
        }
    }

    // å®Œæˆ ZIP æ–‡ä»¶
    if let Err(e) = zip_writer.finish() {
        let msg = format!("å®Œæˆ ZIP æ–‡ä»¶å¤±è´¥: {}", e);
        #[cfg(feature = "data_governance")]
        {
            try_save_audit_log(
                &app,
                AuditLog::new(
                    AuditOperation::Backup {
                        backup_type: super::audit::BackupType::Full,
                        file_count: 0,
                        total_size: 0,
                    },
                    format!("zip_export/{}", backup_id),
                )
                .fail(msg.clone())
                .with_details(serde_json::json!({
                    "job_id": job_ctx.job_id.clone(),
                    "backup_id": backup_id.clone(),
                    "subtype": "zip_export",
                    "zip_path": zip_path.to_string_lossy(),
                })),
            );
        }
        job_ctx.fail(msg);
        return;
    }

    if job_ctx.is_cancelled() {
        let _ = std::fs::remove_file(&zip_path);
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆ ZIP å¯¼å‡º".to_string()));
        return;
    }

    // ========== é˜¶æ®µ 3: éªŒè¯ (90-95%) ==========
    job_ctx.mark_running(
        BackupJobPhase::Verify,
        90.0,
        Some("æ­£åœ¨éªŒè¯ ZIP æ–‡ä»¶...".to_string()),
        compressed_files as u64,
        total_files as u64,
    );

    // è·å–å‹ç¼©åçš„å¤§å°
    let compressed_size = match std::fs::metadata(&zip_path) {
        Ok(m) => m.len(),
        Err(e) => {
            let msg = format!("è·å– ZIP æ–‡ä»¶å¤§å°å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_export/{}", backup_id),
                    )
                    .fail(msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_id": backup_id.clone(),
                        "subtype": "zip_export",
                        "zip_path": zip_path.to_string_lossy(),
                    })),
                );
            }
            job_ctx.fail(msg);
            return;
        }
    };

    // è®¡ç®— ZIP æ–‡ä»¶çš„æ ¡éªŒå’Œ
    let zip_checksum = match crate::backup_common::calculate_file_hash(&zip_path) {
        Ok(c) => c,
        Err(e) => {
            let msg = format!("è®¡ç®— ZIP æ ¡éªŒå’Œå¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_export/{}", backup_id),
                    )
                    .fail(msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "backup_id": backup_id.clone(),
                        "subtype": "zip_export",
                        "zip_path": zip_path.to_string_lossy(),
                    })),
                );
            }
            job_ctx.fail(msg);
            return;
        }
    };

    job_ctx.mark_running(
        BackupJobPhase::Verify,
        95.0,
        Some("éªŒè¯å®Œæˆ".to_string()),
        compressed_files as u64,
        total_files as u64,
    );

    // ========== é˜¶æ®µ 4: æ¸…ç† (95-100%) ==========
    job_ctx.mark_running(
        BackupJobPhase::Cleanup,
        98.0,
        Some("æ­£åœ¨å®Œæˆå¯¼å‡º...".to_string()),
        compressed_files as u64,
        total_files as u64,
    );

    let duration_ms = start.elapsed().as_millis() as u64;
    let compression_ratio = if total_size > 0 {
        1.0 - (compressed_size as f64 / total_size as f64)
    } else {
        0.0
    };

    info!(
        "[data_governance] ZIP å¯¼å‡ºæˆåŠŸ: path={:?}, files={}, size={}->{}, ratio={:.1}%, duration={}ms",
        zip_path, compressed_files, total_size, compressed_size, compression_ratio * 100.0, duration_ms
    );

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Full,
                    file_count: compressed_files,
                    total_size: compressed_size,
                },
                format!("zip_export/{}", backup_id),
            )
            .complete(duration_ms)
            .with_details(serde_json::json!({
                "job_id": job_ctx.job_id.clone(),
                "backup_id": backup_id.clone(),
                "zip_path": zip_path.to_string_lossy(),
                "file_count": compressed_files,
                "total_size": total_size,
                "compressed_size": compressed_size,
                "compression_ratio": compression_ratio,
                "zip_checksum": zip_checksum,
                "subtype": "zip_export",
            })),
        );
    }

    // æ„å»ºç»“æœ payloadï¼ˆå¦‚æœ‰è·³è¿‡æ–‡ä»¶ï¼Œæ ‡è®° success=false å¹¶é™„ä¸Šé”™è¯¯è¯¦æƒ…ï¼‰
    let has_skipped = !skipped_files.is_empty();
    if has_skipped {
        warn!(
            "[zip_export] å¯¼å‡ºå®Œæˆä½†æœ‰ {} ä¸ªæ–‡ä»¶è¢«è·³è¿‡: {:?}",
            skipped_files.len(),
            skipped_files
        );
    }
    let export_error = if has_skipped {
        Some(format!(
            "å¯¼å‡ºå®Œæˆä½† {} ä¸ªæ–‡ä»¶è¢«è·³è¿‡: {}",
            skipped_files.len(),
            skipped_files.join("; ")
        ))
    } else {
        None
    };

    let result_payload = BackupJobResultPayload {
        success: !has_skipped,
        output_path: Some(zip_path.to_string_lossy().to_string()),
        resolved_path: Some(zip_path.to_string_lossy().to_string()),
        message: Some(format!(
            "ZIP å¯¼å‡ºå®Œæˆ: {} ä¸ªæ–‡ä»¶, å‹ç¼©ç‡ {:.1}%{}",
            compressed_files,
            compression_ratio * 100.0,
            if has_skipped {
                format!("ï¼ˆ{} ä¸ªæ–‡ä»¶è¢«è·³è¿‡ï¼‰", skipped_files.len())
            } else {
                "".to_string()
            }
        )),
        error: export_error,
        duration_ms: Some(duration_ms),
        stats: Some(serde_json::json!({
            "file_count": compressed_files,
            "total_size": total_size,
            "compressed_size": compressed_size,
            "compression_ratio": compression_ratio,
            "zip_checksum": zip_checksum,
            "skipped_files": skipped_files,
        })),
        requires_restart: false,
        checkpoint_path: None,
        resumable_job_id: None,
    };

    job_ctx.complete(
        Some(format!("ZIP å¯¼å‡ºå®Œæˆ: {}", zip_path.to_string_lossy())),
        compressed_files as u64,
        total_files as u64,
        result_payload,
    );
}

/// ZIP å¯¼å‡ºç»“æœå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct ZipExportResultResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// ZIP æ–‡ä»¶è·¯å¾„
    pub zip_path: String,
    /// åŸå§‹æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub total_size: u64,
    /// å‹ç¼©åå¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub compressed_size: u64,
    /// å‹ç¼©ç‡ï¼ˆ0.0-1.0ï¼‰
    pub compression_ratio: f64,
    /// æ–‡ä»¶æ•°é‡
    pub file_count: usize,
    /// æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// ZIP æ–‡ä»¶çš„ SHA256 æ ¡éªŒå’Œ
    pub zip_checksum: String,
}

/// å¼‚æ­¥åå° ZIP å¯¼å…¥ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
///
/// å¯åŠ¨åå° ZIP å¯¼å…¥ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡ IDã€‚å¯¼å…¥è¿›åº¦é€šè¿‡ `backup-job-progress` äº‹ä»¶å‘é€ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `zip_path`: ZIP æ–‡ä»¶è·¯å¾„
/// - `backup_id`: è§£å‹åçš„å¤‡ä»½ IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»æ–‡ä»¶åç”Ÿæˆï¼‰
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ ID
///
/// ## è¿›åº¦é˜¶æ®µ
/// - Scan (0-5%): éªŒè¯ ZIP æ–‡ä»¶
/// - Extract (5-80%): è§£å‹æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶æ•°é‡æ›´æ–°è¿›åº¦ï¼‰
/// - Verify (80-90%): éªŒè¯è§£å‹çš„æ–‡ä»¶
/// - Cleanup (90-100%): æ¸…ç†ä¸´æ—¶æ–‡ä»¶
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
#[tauri::command]
pub async fn data_governance_import_zip(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    zip_path: String,
    backup_id: Option<String>,
) -> Result<BackupJobStartResponse, String> {
    let validated_backup_id = match backup_id {
        Some(id) => Some(validate_backup_id(&id)?),
        None => None,
    };

    // å®‰å…¨éªŒè¯ï¼šç¡®ä¿ zip_path åœ¨å®‰å…¨èŒƒå›´å†…ï¼ˆéç³»ç»Ÿç›®å½•ã€éåº”ç”¨æ•°æ®ç›®å½•å†…éƒ¨ï¼‰
    let app_data_dir = get_app_data_dir(&app)?;
    let zip_file_path = PathBuf::from(&zip_path);
    validate_user_path(&zip_file_path, &app_data_dir)?;

    info!(
        "[data_governance] å¯åŠ¨åå° ZIP å¯¼å…¥ä»»åŠ¡: zip_path={}, backup_id={:?}",
        zip_path, validated_backup_id
    );

    if !zip_file_path.exists() {
        return Err(format!(
            "ZIP æ–‡ä»¶ä¸å­˜åœ¨: {}ã€‚è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–é‡æ–°é€‰æ‹©æ–‡ä»¶",
            sanitize_path_for_user(&zip_file_path)
        ));
    }

    // ä½¿ç”¨å…¨å±€å•ä¾‹å¤‡ä»½ä»»åŠ¡ç®¡ç†å™¨
    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Import);
    let job_id = job_ctx.job_id.clone();

    #[cfg(feature = "data_governance")]
    {
        let target_id = validated_backup_id
            .clone()
            .unwrap_or_else(|| "auto".to_string());
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Backup {
                    backup_type: super::audit::BackupType::Full,
                    file_count: 0,
                    total_size: 0,
                },
                format!("zip_import/{}", target_id),
            )
            .with_details(serde_json::json!({
                "job_id": job_id.clone(),
                "zip_path": zip_path,
                "backup_id": validated_backup_id,
                "subtype": "zip_import",
            })),
        );
    }

    // åœ¨åå°æ‰§è¡Œå¯¼å…¥
    tauri::async_runtime::spawn(async move {
        execute_zip_import_with_progress(app, job_ctx, zip_file_path, validated_backup_id).await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "import".to_string(),
        status: "queued".to_string(),
        message: "ZIP å¯¼å…¥ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

/// æ‰§è¡Œ ZIP å¯¼å…¥ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œå¸¦è¿›åº¦å›è°ƒï¼‰
async fn execute_zip_import_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    zip_file_path: PathBuf,
    backup_id: Option<String>,
) {
    use super::backup::zip_export::{import_backup_from_zip_with_progress, ZipImportPhase};
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è®¾ç½®ä»»åŠ¡å‚æ•°ï¼ˆç”¨äºæŒä¹…åŒ–å’Œæ¢å¤ï¼‰
    job_ctx.set_params(BackupJobParams {
        zip_path: Some(zip_file_path.to_string_lossy().to_string()),
        backup_id: backup_id.clone(),
        ..Default::default()
    });

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    // ç¡®å®šå¤‡ä»½ ID
    let generated_backup_id = backup_id.unwrap_or_else(|| {
        use uuid::Uuid;
        let now = chrono::Utc::now();
        let timestamp = now.format("%Y%m%d_%H%M%S").to_string();
        let millis = now.timestamp_subsec_millis();
        let rand8 = &Uuid::new_v4().simple().to_string()[..8];
        format!("{}_{}_{:03}_imported", timestamp, rand8, millis)
    });

    let target_backup_id = match validate_backup_id(&generated_backup_id) {
        Ok(id) => id,
        Err(e) => {
            job_ctx.fail(format!("backup_id éæ³•: {}", e));
            return;
        }
    };

    let target_dir = backup_dir.join(&target_backup_id);

    // ç¡®ä¿ç›®æ ‡ç›®å½•ä¸å­˜åœ¨
    if target_dir.exists() {
        if let Err(e) = ensure_existing_path_within_backup_dir(&target_dir, &backup_dir) {
            job_ctx.fail(format!("å¤‡ä»½è·¯å¾„æ ¡éªŒå¤±è´¥: {}", e));
            return;
        }
        job_ctx.fail(format!("å¤‡ä»½å·²å­˜åœ¨: {}", target_backup_id));
        return;
    }

    // åˆå§‹åŒ–æ£€æŸ¥ç‚¹
    job_ctx.init_checkpoint(0); // æ–‡ä»¶æ•°åœ¨æ‰«æåç¡®å®š

    // é˜¶æ®µ 1: æ‰«æ
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        0.0,
        Some("æ­£åœ¨éªŒè¯ ZIP æ–‡ä»¶...".to_string()),
        0,
        0,
    );

    // æ£€æŸ¥å–æ¶ˆ
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¯¼å…¥".to_string()));
        return;
    }

    // ä½¿ç”¨å¸¦è¿›åº¦çš„å¯¼å…¥å‡½æ•°
    let job_ctx_for_progress = job_ctx.clone();
    let job_ctx_for_cancel = job_ctx.clone();

    let result = import_backup_from_zip_with_progress(
        &zip_file_path,
        &target_dir,
        |progress| {
            // å°† ZipImportPhase è½¬æ¢ä¸º BackupJobPhase
            let phase = match progress.phase {
                ZipImportPhase::Scan => BackupJobPhase::Scan,
                ZipImportPhase::Extract => BackupJobPhase::Extract,
                ZipImportPhase::Verify => BackupJobPhase::Verify,
                ZipImportPhase::Completed => BackupJobPhase::Completed,
            };

            job_ctx_for_progress.mark_running(
                phase,
                progress.progress,
                Some(progress.message),
                progress.processed_files as u64,
                progress.total_files as u64,
            );
        },
        || job_ctx_for_cancel.is_cancelled(),
    );

    match result {
        Ok(file_count) => {
            let duration_ms = start.elapsed().as_millis() as u64;

            // é˜¶æ®µ 4: æ¸…ç†ï¼ˆ90% - 100%ï¼‰
            job_ctx.mark_running(
                BackupJobPhase::Cleanup,
                95.0,
                Some("æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...".to_string()),
                file_count as u64,
                file_count as u64,
            );

            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count,
                            total_size: 0,
                        },
                        format!("zip_import/{}", target_backup_id),
                    )
                    .complete(duration_ms)
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "zip_path": zip_file_path.to_string_lossy(),
                        "backup_id": target_backup_id,
                        "backup_path": target_dir.to_string_lossy(),
                        "file_count": file_count,
                        "subtype": "zip_import",
                    })),
                );
            }

            // å®Œæˆ
            let result_payload = BackupJobResultPayload {
                success: true,
                output_path: Some(target_dir.to_string_lossy().to_string()),
                resolved_path: None,
                message: Some(format!(
                    "ZIP å¯¼å…¥æˆåŠŸ: {} ä¸ªæ–‡ä»¶, å¤‡ä»½ ID: {}",
                    file_count, target_backup_id
                )),
                error: None,
                duration_ms: Some(duration_ms),
                stats: Some(serde_json::json!({
                    "file_count": file_count,
                    "backup_id": target_backup_id,
                    "backup_path": target_dir.to_string_lossy().to_string(),
                })),
                requires_restart: false,
                checkpoint_path: None,
                resumable_job_id: None,
            };

            job_ctx.complete(
                Some(format!("ZIP å¯¼å…¥æˆåŠŸ: {} ä¸ªæ–‡ä»¶", file_count)),
                file_count as u64,
                file_count as u64,
                result_payload,
            );

            info!(
                "[data_governance] ZIP å¯¼å…¥ä»»åŠ¡å®Œæˆ: backup_id={}, files={}, duration={}ms",
                target_backup_id, file_count, duration_ms
            );
        }
        Err(e) => {
            // æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·å–æ¶ˆ
            let error_msg = e.to_string();
            if error_msg.contains("ç”¨æˆ·å–æ¶ˆ") {
                job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¯¼å…¥".to_string()));
            } else {
                error!("[data_governance] ZIP å¯¼å…¥ä»»åŠ¡å¤±è´¥: {}", e);
                job_ctx.fail(format!("ZIP å¯¼å…¥å¤±è´¥: {}", e));
            }

            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_import/{}", target_backup_id),
                    )
                    .fail(error_msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "zip_path": zip_file_path.to_string_lossy(),
                        "backup_id": target_backup_id,
                        "backup_path": target_dir.to_string_lossy(),
                        "subtype": "zip_import",
                    })),
                );
            }

            // æ¸…ç†å·²åˆ›å»ºçš„ç›®å½•
            if target_dir.exists() {
                if let Err(cleanup_err) = std::fs::remove_dir_all(&target_dir) {
                    warn!(
                        "[data_governance] æ¸…ç†å¤±è´¥çš„å¯¼å…¥ç›®å½•æ—¶å‡ºé”™: {}",
                        cleanup_err
                    );
                }
            }
        }
    }
}

// ==================== æ¢å¤ç›¸å…³å‘½ä»¤ ====================

/// å¼‚æ­¥åå°æ¢å¤ï¼ˆå¸¦è¿›åº¦äº‹ä»¶ï¼‰
///
/// å¯åŠ¨åå°æ¢å¤ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡ IDã€‚æ¢å¤è¿›åº¦é€šè¿‡ `backup-job-progress` äº‹ä»¶å‘é€ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦æ¢å¤çš„å¤‡ä»½ ID
///
/// ## è¿”å›
/// - `BackupJobStartResponse`: åŒ…å«ä»»åŠ¡ ID
///
/// ## äº‹ä»¶
/// - `backup-job-progress`: è¿›åº¦æ›´æ–°äº‹ä»¶
///
/// ## è¿›åº¦é˜¶æ®µ
/// - Scan (5%): éªŒè¯å¤‡ä»½æ¸…å•
/// - Verify (5-15%): éªŒè¯å¤‡ä»½æ–‡ä»¶æ ¡éªŒå’Œ
/// - Replace (15-90%): æ¢å¤æ•°æ®åº“ï¼ˆæ¯ä¸ªæ•°æ®åº“æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼‰
/// - Cleanup (90-100%): æ¸…ç†å’ŒéªŒè¯
#[tauri::command]
pub async fn data_governance_restore_backup(
    app: tauri::AppHandle,
    backup_job_state: State<'_, BackupJobManagerState>,
    backup_id: String,
    restore_assets: Option<bool>,
) -> Result<BackupJobStartResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;

    info!(
        "[data_governance] å¯åŠ¨åå°æ¢å¤ä»»åŠ¡: backup_id={}",
        validated_backup_id
    );

    // ä½¿ç”¨å…¨å±€å•ä¾‹å¤‡ä»½ä»»åŠ¡ç®¡ç†å™¨
    let job_manager = backup_job_state.get();
    let job_ctx = job_manager.create_job(BackupJobKind::Import);
    let job_id = job_ctx.job_id.clone();

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Restore {
                    backup_path: validated_backup_id.clone(),
                },
                validated_backup_id.clone(),
            )
            .with_details(serde_json::json!({
                "job_id": job_id.clone(),
                "restore_assets": restore_assets,
            })),
        );
    }

    // åœ¨åå°æ‰§è¡Œæ¢å¤
    let app_clone = app.clone();

    tauri::async_runtime::spawn(async move {
        execute_restore_with_progress(app_clone, job_ctx, validated_backup_id, restore_assets)
            .await;
    });

    Ok(BackupJobStartResponse {
        job_id,
        kind: "import".to_string(),
        status: "queued".to_string(),
        message: "æ¢å¤ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·é€šè¿‡ backup-job-progress äº‹ä»¶ç›‘å¬è¿›åº¦".to_string(),
    })
}

/// æ‰§è¡Œæ¢å¤ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œå¸¦ç»†ç²’åº¦è¿›åº¦å›è°ƒï¼‰
///
/// è¿›åº¦é˜¶æ®µè®¾è®¡ï¼ˆç»†ç²’åº¦ï¼Œæ¯ä¸ªæ•°æ®åº“/èµ„äº§æ–‡ä»¶ç‹¬ç«‹ä¸ŠæŠ¥ï¼‰ï¼š
/// - Scan (0-5%): éªŒè¯å¤‡ä»½æ¸…å•ã€ç‰ˆæœ¬å…¼å®¹æ€§
/// - Verify (5-15%): é€æ–‡ä»¶éªŒè¯æ ¡éªŒå’Œ + å®Œæ•´æ€§æ£€æŸ¥
/// - Replace (15-80%): é€æ•°æ®åº“æ¢å¤ï¼ˆæ¯å®Œæˆä¸€ä¸ªæ•°æ®åº“æ›´æ–°ä¸€æ¬¡è¿›åº¦ï¼‰
/// - Replace (80-92%): é€æ–‡ä»¶æ¢å¤èµ„äº§ï¼ˆå¸¦ per-file è¿›åº¦ï¼‰
/// - Cleanup (92-100%): æ’æ§½åˆ‡æ¢æ ‡è®°ã€å®¡è®¡æ—¥å¿—
async fn execute_restore_with_progress(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    backup_id: String,
    restore_assets: Option<bool>,
) {
    use super::backup::BackupManager;
    use super::backup::assets;
    use super::schema_registry::DatabaseId;
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    if !backup_dir.exists() {
        job_ctx.fail("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨".to_string());
        return;
    }

    // ============ é˜¶æ®µ 1: Scan (0-5%) - éªŒè¯å¤‡ä»½æ¸…å• ============
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        2.0,
        Some("æ­£åœ¨éªŒè¯å¤‡ä»½æ¸…å•...".to_string()),
        0,
        0,
    );

    // æ£€æŸ¥å–æ¶ˆï¼ˆå®‰å…¨ç‚¹ï¼‰
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆæ¢å¤".to_string()));
        return;
    }

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let mut manager = BackupManager::new(backup_dir.clone());
    manager.set_app_data_dir(app_data_dir.clone());
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    // è·å–å¤‡ä»½åˆ—è¡¨
    let manifests = match manager.list_backups() {
        Ok(m) => m,
        Err(e) => {
            error!("[data_governance] è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e);
            job_ctx.fail(format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e));
            return;
        }
    };

    // æŸ¥æ‰¾ç›®æ ‡å¤‡ä»½
    let manifest = match manifests.iter().find(|m| m.backup_id == backup_id) {
        Some(m) => m.clone(),
        None => {
            job_ctx.fail(format!("å¤‡ä»½ä¸å­˜åœ¨: {}", backup_id));
            return;
        }
    };

    let manifest_dir = app_data_dir.join("backups").join(&manifest.backup_id);
    if let Err(e) =
        ensure_existing_path_within_backup_dir(&manifest_dir, &app_data_dir.join("backups"))
    {
        job_ctx.fail(format!("å¤‡ä»½è·¯å¾„æ ¡éªŒå¤±è´¥: {}", e));
        return;
    }

    // ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
    if let Err(e) = manager.check_manifest_compatibility(&manifest) {
        job_ctx.fail(format!("å¤‡ä»½ç‰ˆæœ¬ä¸å…¼å®¹: {}", e));
        return;
    }

    // è®¡ç®—æ•°æ®åº“æ–‡ä»¶åˆ—è¡¨å’Œèµ„äº§æ€»æ•°ï¼Œç”¨äºç²¾ç¡®çš„ total_items
    let database_files: Vec<_> = manifest
        .files
        .iter()
        .filter(|f| f.path.ends_with(".db") && f.database_id.is_some())
        .collect();
    let total_databases = database_files.len() as u64;
    let asset_file_count: u64 = manifest
        .assets
        .as_ref()
        .map(|a| a.total_files as u64)
        .unwrap_or(0);
    // total_items = databases + asset filesï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤º "X / Y é¡¹"ï¼‰
    let total_items = total_databases + asset_file_count;

    job_ctx.mark_running(
        BackupJobPhase::Scan,
        5.0,
        Some(format!(
            "å¤‡ä»½æ¸…å•éªŒè¯é€šè¿‡: {} ä¸ªæ•°æ®åº“, {} ä¸ªèµ„äº§æ–‡ä»¶",
            total_databases, asset_file_count
        )),
        0,
        total_items,
    );

    info!(
        "[data_governance] å¤‡ä»½æ¸…å•éªŒè¯é€šè¿‡: backup_id={}, databases={}, assets={}",
        backup_id, total_databases, asset_file_count
    );

    // ============ é˜¶æ®µ 2: Verify (5-15%) - é€æ–‡ä»¶éªŒè¯å¤‡ä»½å®Œæ•´æ€§ ============
    let backup_subdir = backup_dir.join(&manifest.backup_id);
    if !backup_subdir.exists() {
        job_ctx.fail(format!("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {:?}", backup_subdir));
        return;
    }

    // æ£€æŸ¥å–æ¶ˆï¼ˆå®‰å…¨ç‚¹ - æ¢å¤å‰æœ€åä¸€æ¬¡å®‰å…¨æ£€æŸ¥ï¼‰
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆæ¢å¤".to_string()));
        return;
    }

    // é€æ–‡ä»¶éªŒè¯æ ¡éªŒå’Œï¼ˆç»†ç²’åº¦è¿›åº¦ï¼š5% â†’ 15%ï¼‰
    let verify_total = manifest.files.len();
    for (idx, backup_file) in manifest.files.iter().enumerate() {
        // éªŒè¯é˜¶æ®µå…è®¸å–æ¶ˆï¼ˆå°šæœªä¿®æ”¹ä»»ä½•æ•°æ®ï¼‰
        if job_ctx.is_cancelled() {
            job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆæ¢å¤ï¼ˆéªŒè¯é˜¶æ®µï¼‰".to_string()));
            return;
        }

        let verify_progress = 5.0 + (idx as f32 / verify_total.max(1) as f32) * 10.0;
        job_ctx.mark_running(
            BackupJobPhase::Verify,
            verify_progress,
            Some(format!("æ­£åœ¨éªŒè¯: {} ({}/{})", backup_file.path, idx + 1, verify_total)),
            0,
            total_items,
        );

        let file_path = backup_subdir.join(&backup_file.path);
        if !file_path.exists() {
            job_ctx.fail(format!("å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {}", backup_file.path));
            return;
        }

        // éªŒè¯ SHA256 æ ¡éªŒå’Œ
        match super::backup::calculate_file_sha256(&file_path) {
            Ok(actual_sha256) => {
                if actual_sha256 != backup_file.sha256 {
                    job_ctx.fail(format!(
                        "å¤‡ä»½æ–‡ä»¶æ ¡éªŒå’Œä¸åŒ¹é…: {} (expected={}, actual={})",
                        backup_file.path, backup_file.sha256, actual_sha256
                    ));
                    return;
                }
            }
            Err(e) => {
                job_ctx.fail(format!("è®¡ç®—æ ¡éªŒå’Œå¤±è´¥ {}: {}", backup_file.path, e));
                return;
            }
        }

        // å¯¹ .db æ–‡ä»¶æ‰§è¡Œ PRAGMA integrity_checkï¼ˆä¸åŸ verify_internal ä¸€è‡´ï¼‰
        if backup_file.path.ends_with(".db") {
            match rusqlite::Connection::open(&file_path) {
                Ok(conn) => {
                    match conn.query_row("PRAGMA integrity_check", [], |row| row.get::<_, String>(0)) {
                        Ok(result) if result == "ok" => {
                            debug!("[data_governance] å¤‡ä»½æ•°æ®åº“å®Œæ•´æ€§éªŒè¯é€šè¿‡: {}", backup_file.path);
                        }
                        Ok(result) => {
                            job_ctx.fail(format!(
                                "å¤‡ä»½æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {} ({})",
                                backup_file.path, result
                            ));
                            return;
                        }
                        Err(e) => {
                            job_ctx.fail(format!(
                                "å¤‡ä»½æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {} ({})",
                                backup_file.path, e
                            ));
                            return;
                        }
                    }
                }
                Err(e) => {
                    job_ctx.fail(format!(
                        "æ— æ³•æ‰“å¼€å¤‡ä»½æ•°æ®åº“æ–‡ä»¶: {} ({})",
                        backup_file.path, e
                    ));
                    return;
                }
            }
        }
    }

    info!("[data_governance] å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡: {} ä¸ªæ–‡ä»¶", verify_total);

    // ============ é˜¶æ®µ 3: Replace (15-80%) - é€æ•°æ®åº“æ¢å¤ ============
    // è·å–éæ´»è·ƒæ’æ§½ç›®å½•ï¼šæ¢å¤å†™å…¥éæ´»è·ƒæ’æ§½ï¼Œé¿å… Windows OS error 32
    // ï¼ˆæ´»è·ƒæ’æ§½çš„æ•°æ®åº“æ–‡ä»¶è¢«è¿æ¥æ± æŒæœ‰ï¼ŒWindows ä¸Šæ— æ³•å†™å…¥/åˆ é™¤ï¼‰
    let (inactive_dir, inactive_slot) = match crate::data_space::get_data_space_manager() {
        Some(mgr) => {
            let slot = mgr.inactive_slot();
            let dir = mgr.slot_dir(slot);
            info!(
                "[data_governance] æ¢å¤ç›®æ ‡: éæ´»è·ƒæ’æ§½ {} ({})",
                slot.name(),
                dir.display()
            );
            (dir, Some(slot))
        }
        None => {
            // æœªå¯ç”¨åŒç©ºé—´æ¨¡å¼ï¼Œå›é€€åˆ° slots/slotB
            let dir = app_data_dir.join("slots").join("slotB");
            warn!("[data_governance] DataSpaceManager æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ° slotB");
            (dir, None)
        }
    };

    // ç£ç›˜ç©ºé—´é¢„æ£€æŸ¥ï¼šå¤‡ä»½å¤§å° Ã— 2 ä½œä¸ºå®‰å…¨ä½™é‡ï¼ˆAndroid è®¾å¤‡å­˜å‚¨è¾ƒç´§å¼ ï¼‰
    {
        let db_size: u64 = manifest.files.iter().map(|f| f.size).sum();
        let asset_size: u64 = manifest.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
        let required = (db_size + asset_size).saturating_mul(2);
        match crate::backup_common::get_available_disk_space(&app_data_dir) {
            Ok(available) if available < required => {
                let msg = format!(
                    "ç£ç›˜ç©ºé—´ä¸è¶³ï¼šéœ€è¦ {:.1} MBï¼Œä»…å‰© {:.1} MBã€‚è¯·æ¸…ç†å­˜å‚¨ç©ºé—´åé‡è¯•",
                    required as f64 / 1024.0 / 1024.0,
                    available as f64 / 1024.0 / 1024.0
                );
                error!("[data_governance] {}", msg);
                job_ctx.fail(msg);
                return;
            }
            Err(e) => {
                warn!("[data_governance] ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥ï¼ˆç»§ç»­æ¢å¤ï¼‰: {}", e);
            }
            _ => {}
        }
    }

    // ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    if let Err(e) = std::fs::create_dir_all(&inactive_dir) {
        job_ctx.fail(format!("åˆ›å»ºæ¢å¤ç›®æ ‡ç›®å½•å¤±è´¥: {}", e));
        return;
    }

    // é€æ•°æ®åº“æ¢å¤ï¼ˆç»†ç²’åº¦è¿›åº¦ï¼š15% â†’ 80%ï¼‰
    let mut databases_restored: Vec<String> = Vec::new();
    let mut restore_errors: Vec<String> = Vec::new();
    let db_progress_range = 65.0; // 15% â†’ 80%

    for (idx, backup_file) in database_files.iter().enumerate() {
        let db_id_str = match backup_file.database_id.as_ref() {
            Some(id) => id,
            None => continue,
        };

        let db_id = match db_id_str.as_str() {
            "vfs" => DatabaseId::Vfs,
            "chat_v2" => DatabaseId::ChatV2,
            "mistakes" => DatabaseId::Mistakes,
            "llm_usage" => DatabaseId::LlmUsage,
            _ => {
                let msg = format!("å¤‡ä»½ä¸­åŒ…å«æœªçŸ¥çš„æ•°æ®åº“ ID: {}", db_id_str);
                error!("{}", msg);
                restore_errors.push(msg);
                continue;
            }
        };

        let db_progress = 15.0 + (idx as f32 / total_databases.max(1) as f32) * db_progress_range;
        job_ctx.mark_running(
            BackupJobPhase::Replace,
            db_progress,
            Some(format!(
                "æ­£åœ¨æ¢å¤æ•°æ®åº“: {} ({}/{})",
                db_id_str,
                idx + 1,
                total_databases
            )),
            idx as u64,
            total_items,
        );

        match manager.restore_single_database_to_dir(&db_id, &backup_subdir, &inactive_dir) {
            Ok(()) => {
                info!("[data_governance] æ¢å¤æ•°æ®åº“æˆåŠŸ: {:?}", db_id);
                databases_restored.push(db_id_str.clone());
            }
            Err(e) => {
                error!("[data_governance] æ¢å¤æ•°æ®åº“å¤±è´¥: {:?}, é”™è¯¯: {}", db_id, e);
                restore_errors.push(format!("{}: {}", db_id_str, e));
            }
        }
    }

    // æ•°æ®åº“æ¢å¤å®Œæˆåçš„è¿›åº¦
    job_ctx.mark_running(
        BackupJobPhase::Replace,
        80.0,
        Some(format!(
            "æ•°æ®åº“æ¢å¤å®Œæˆ: {}/{}",
            databases_restored.len(),
            total_databases
        )),
        total_databases,
        total_items,
    );

    // æ£€æŸ¥æ•°æ®åº“æ¢å¤é”™è¯¯
    if !restore_errors.is_empty() {
        let err_msg = format!("éƒ¨åˆ†æ•°æ®åº“æ¢å¤å¤±è´¥: {}", restore_errors.join("; "));
        error!("[data_governance] {}", err_msg);
        #[cfg(feature = "data_governance")]
        {
            try_save_audit_log(
                &app,
                AuditLog::new(
                    AuditOperation::Restore {
                        backup_path: backup_id.clone(),
                    },
                    backup_id.clone(),
                )
                .fail(err_msg.clone())
                .with_details(serde_json::json!({
                    "job_id": job_ctx.job_id.clone(),
                    "restore_assets": restore_assets,
                    "errors": restore_errors,
                })),
            );
        }
        job_ctx.fail(err_msg);
        return;
    }

    // ============ é˜¶æ®µ 3b: Replace/Assets (80-92%) - æ¢å¤èµ„äº§æ–‡ä»¶ ============
    let should_restore_assets = restore_assets.unwrap_or_else(|| {
        manifest
            .assets
            .as_ref()
            .map(|a| a.total_files > 0)
            .unwrap_or(false)
    });

    let mut restored_assets: usize = 0;

    if should_restore_assets {
        let asset_progress_base = 80.0_f32;
        let asset_progress_range = 12.0_f32; // 80% â†’ 92%

        if let Some(asset_result) = &manifest.assets {
            info!(
                "[data_governance] å¼€å§‹æ¢å¤èµ„äº§æ–‡ä»¶: {} ä¸ª",
                asset_result.total_files
            );

            job_ctx.mark_running(
                BackupJobPhase::Replace,
                asset_progress_base,
                Some(format!(
                    "æ­£åœ¨æ¢å¤èµ„äº§æ–‡ä»¶: 0/{}",
                    asset_result.total_files
                )),
                total_databases,
                total_items,
            );

            match assets::restore_assets_with_progress(
                &backup_subdir,
                &inactive_dir,
                &asset_result.files,
                |restored, total_asset| {
                    if job_ctx.is_cancelled() {
                        return false;
                    }

                    let asset_pct = if total_asset > 0 {
                        restored as f32 / total_asset as f32
                    } else {
                        1.0
                    };
                    let progress = asset_progress_base + asset_pct * asset_progress_range;
                    job_ctx.mark_running(
                        BackupJobPhase::Replace,
                        progress,
                        Some(format!(
                            "æ­£åœ¨æ¢å¤èµ„äº§æ–‡ä»¶: {}/{}",
                            restored, total_asset
                        )),
                        total_databases + restored as u64,
                        total_items,
                    );

                    true
                },
            ) {
                Ok(count) => {
                    restored_assets = count;
                    info!("[data_governance] èµ„äº§æ¢å¤å®Œæˆ: {} ä¸ªæ–‡ä»¶", count);
                }
                Err(e) => {
                    if e.is_cancelled() {
                        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆæ¢å¤ï¼ˆèµ„äº§é˜¶æ®µï¼‰".to_string()));
                        return;
                    }

                    // èµ„äº§æ¢å¤å¤±è´¥ä¸é˜»å¡æ•°æ®åº“æ¢å¤ç»“æœï¼Œè®°å½•è­¦å‘Š
                    error!("[data_governance] èµ„äº§æ¢å¤å¤±è´¥: {}", e);
                    restore_errors.push(format!("èµ„äº§æ¢å¤: {}", e));
                }
            }
        } else {
            // manifest.assets ä¸º None æ—¶ï¼Œå°è¯•ç›´æ¥æ‰«æå¤‡ä»½ç›®å½•ä¸­çš„ assets/ å­ç›®å½•
            let assets_subdir = backup_subdir.join("assets");
            if assets_subdir.exists() && assets_subdir.is_dir() {
                info!(
                    "[data_governance] manifest.assets ä¸ºç©ºï¼Œå°è¯•ä» assets/ ç›®å½•ç›´æ¥æ¢å¤: {:?}",
                    assets_subdir
                );

                job_ctx.mark_running(
                    BackupJobPhase::Replace,
                    asset_progress_base,
                    Some("æ­£åœ¨ä»ç›®å½•æ¢å¤èµ„äº§æ–‡ä»¶...".to_string()),
                    total_databases,
                    total_items,
                );

                match assets::restore_assets_from_dir_with_progress(
                    &assets_subdir,
                    &inactive_dir,
                    |restored, total_asset| {
                        if job_ctx.is_cancelled() {
                            return false;
                        }

                        let asset_pct = if total_asset > 0 {
                            restored as f32 / total_asset as f32
                        } else {
                            1.0
                        };
                        let progress = asset_progress_base + asset_pct * asset_progress_range;
                        job_ctx.mark_running(
                            BackupJobPhase::Replace,
                            progress,
                            Some(format!(
                                "æ­£åœ¨æ¢å¤èµ„äº§æ–‡ä»¶: {}/{}",
                                restored, total_asset
                            )),
                            total_databases + restored as u64,
                            total_items,
                        );

                        true
                    },
                ) {
                    Ok(count) => {
                        restored_assets = count;
                        info!("[data_governance] èµ„äº§ç›®å½•ç›´æ¥æ¢å¤å®Œæˆ: {} ä¸ªæ–‡ä»¶", count);
                    }
                    Err(e) => {
                        if e.is_cancelled() {
                            job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆæ¢å¤ï¼ˆèµ„äº§é˜¶æ®µï¼‰".to_string()));
                            return;
                        }

                        error!("[data_governance] èµ„äº§ç›®å½•ç›´æ¥æ¢å¤å¤±è´¥: {}", e);
                        restore_errors.push(format!("èµ„äº§ç›®å½•æ¢å¤: {}", e));
                    }
                }
            } else {
                warn!("[data_governance] å¤‡ä»½ä¸­æ— èµ„äº§æ–‡ä»¶å¯æ¢å¤");
            }
        }
    }

    // æ”¶é›†æ‰€æœ‰éè‡´å‘½è­¦å‘Šï¼ˆèµ„äº§é”™è¯¯ + æ’æ§½åˆ‡æ¢è­¦å‘Šï¼‰
    let has_asset_errors = !restore_errors.is_empty();
    if has_asset_errors {
        warn!(
            "[data_governance] èµ„äº§æ¢å¤æœ‰éƒ¨åˆ†é”™è¯¯ï¼ˆæ•°æ®åº“å·²æˆåŠŸæ¢å¤ï¼‰: {:?}",
            restore_errors
        );
    }

    // ============ é˜¶æ®µ 4: Cleanup (92-100%) - æ’æ§½åˆ‡æ¢ä¸å®¡è®¡ ============
    job_ctx.mark_running(
        BackupJobPhase::Cleanup,
        93.0,
        Some("æ­£åœ¨æ ‡è®°æ’æ§½åˆ‡æ¢...".to_string()),
        total_items,
        total_items,
    );

    let duration_ms = start.elapsed().as_millis() as u64;
    let restore_target_path = inactive_dir.to_string_lossy().to_string();

    info!(
        "[data_governance] æ¢å¤æˆåŠŸ: id={}, databases={:?}, restored_assets={}, duration={}ms, target={}",
        backup_id, databases_restored, restored_assets, duration_ms, inactive_dir.display()
    );

    // æ ‡è®°ä¸‹æ¬¡é‡å¯æ—¶åˆ‡æ¢åˆ°æ¢å¤ç›®æ ‡æ’æ§½
    let switch_warning: Option<String> = if let Some(slot) = inactive_slot {
        if let Some(mgr) = crate::data_space::get_data_space_manager() {
            match mgr.mark_pending_switch(slot) {
                Ok(()) => {
                    info!("[data_governance] å·²æ ‡è®°ä¸‹æ¬¡é‡å¯åˆ‡æ¢åˆ° {}", slot.name());
                    None
                }
                Err(e) => {
                    let warn_msg = format!(
                        "æ¢å¤æˆåŠŸä½†æ ‡è®°æ’æ§½åˆ‡æ¢å¤±è´¥: {}ã€‚æ¢å¤çš„æ•°æ®åœ¨ {} ä¸­ï¼Œè¯·æ‰‹åŠ¨é‡å¯åé‡è¯•",
                        e, inactive_dir.display()
                    );
                    error!("[data_governance] {}", warn_msg);
                    Some(warn_msg)
                }
            }
        } else {
            None
        }
    } else {
        None
    };

    // åˆå¹¶æ‰€æœ‰è­¦å‘Šä¿¡æ¯ï¼ˆèµ„äº§é”™è¯¯ + æ’æ§½åˆ‡æ¢è­¦å‘Šï¼‰ï¼Œç¡®ä¿å‰ç«¯èƒ½çœ‹åˆ°
    let combined_warnings: Vec<String> = {
        let mut warnings = restore_errors.clone();
        if let Some(ref sw) = switch_warning {
            warnings.push(sw.clone());
        }
        warnings
    };
    let error_for_result = if combined_warnings.is_empty() {
        None
    } else {
        Some(combined_warnings.join("; "))
    };

    job_ctx.mark_running(
        BackupJobPhase::Cleanup,
        97.0,
        Some("æ­£åœ¨è®°å½•å®¡è®¡æ—¥å¿—...".to_string()),
        total_items,
        total_items,
    );

    #[cfg(feature = "data_governance")]
    {
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Restore {
                    backup_path: backup_id.clone(),
                },
                backup_id.clone(),
            )
            .complete(duration_ms)
            .with_details(serde_json::json!({
                "job_id": job_ctx.job_id.clone(),
                "restore_assets": should_restore_assets,
                "restored_assets": restored_assets,
                "databases_restored": databases_restored.clone(),
                "asset_errors": restore_errors,
            })),
        );
    }

    // å®Œæˆä»»åŠ¡ï¼ˆæ•°æ®åº“æ¢å¤æˆåŠŸï¼Œä½†å¦‚æœæœ‰èµ„äº§é”™è¯¯åˆ™ success=false ä»¥è§¦å‘å‰ç«¯ warningï¼‰
    let result_success = !has_asset_errors;
    job_ctx.complete(
        Some(format!(
            "æ¢å¤å®Œæˆï¼Œå·²æ¢å¤ {} ä¸ªæ•°æ®åº“{}{}",
            databases_restored.len(),
            if should_restore_assets {
                format!("ï¼Œèµ„äº§æ–‡ä»¶ {} ä¸ª", restored_assets)
            } else {
                "".to_string()
            },
            if has_asset_errors {
                format!("ï¼ˆ{} ä¸ªèµ„äº§æ¢å¤å¤±è´¥ï¼‰", restore_errors.len())
            } else {
                "".to_string()
            }
        )),
        total_items,
        total_items,
        BackupJobResultPayload {
            success: result_success,
            output_path: Some(restore_target_path.clone()),
            resolved_path: Some(restore_target_path.clone()),
            message: Some(if should_restore_assets {
                format!(
                    "å·²æ¢å¤æ•°æ®åº“: {}ï¼›èµ„äº§æ–‡ä»¶: {}",
                    databases_restored.join(", "),
                    restored_assets
                )
            } else {
                format!("å·²æ¢å¤æ•°æ®åº“: {}", databases_restored.join(", "))
            }),
            error: error_for_result,
            duration_ms: Some(duration_ms),
            stats: Some(serde_json::json!({
                "backup_id": backup_id,
                "databases_restored": databases_restored,
                "database_count": databases_restored.len(),
                "restore_assets": should_restore_assets,
                "restored_assets": restored_assets,
                "restore_target": restore_target_path,
                "asset_errors": restore_errors,
            })),
            // æ¢å¤å®Œæˆåéœ€è¦é‡å¯ä»¥åˆ‡æ¢åˆ°æ¢å¤çš„æ•°æ®æ’æ§½
            requires_restart: true,
            checkpoint_path: None,
            resumable_job_id: None,
        },
    );
}

// ==================== å¯æ¢å¤çš„æ‰§è¡Œå‡½æ•° ====================

/// æ‰§è¡Œå¯æ¢å¤çš„å¤‡ä»½ï¼ˆæ”¯æŒä»å¤±è´¥ä¸­é‡æ–°å¼€å§‹ï¼‰
///
/// ä¸ execute_backup_with_progress ç±»ä¼¼ï¼Œä½†ä¼šï¼š
/// 1. è®¾ç½®ä»»åŠ¡å‚æ•°ä¾›æŒä¹…åŒ–ï¼ˆç”¨äºå¤±è´¥åé‡æ–°å¯åŠ¨ï¼‰
/// 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹è¿½è¸ª
/// 3. åœ¨å¤„ç†æ¯ä¸ªæ•°æ®åº“åæ›´æ–°æ£€æŸ¥ç‚¹ï¼ˆç”¨äºè¿›åº¦è®°å½•ï¼‰
///
/// æ³¨æ„ï¼šç”±äº BackupManager çš„å¤‡ä»½æ–¹æ³•æ˜¯åŸå­æ“ä½œï¼ˆä¸€æ¬¡æ€§å¤‡ä»½æ‰€æœ‰æ•°æ®åº“ï¼‰ï¼Œ
/// æ¢å¤å®é™…ä¸Šæ˜¯ä½¿ç”¨ç›¸åŒå‚æ•°é‡æ–°æ‰§è¡Œå®Œæ•´å¤‡ä»½ï¼Œè€Œéä»ä¸­æ–­ç‚¹ç»§ç»­ã€‚
/// æ£€æŸ¥ç‚¹ä¿¡æ¯ä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºå’Œæ—¥å¿—è¿½è¸ªã€‚
async fn execute_backup_with_progress_resumable(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    backup_type: String,
    base_version: Option<String>,
    include_assets: bool,
    asset_types: Option<Vec<String>>,
) {
    use super::backup::{AssetBackupConfig, AssetType, BackupManager};
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è®¾ç½®ä»»åŠ¡å‚æ•°ï¼ˆç”¨äºæŒä¹…åŒ–å’Œæ¢å¤ï¼‰
    job_ctx.set_params(BackupJobParams {
        backup_type: Some(backup_type.clone()),
        base_version: base_version.clone(),
        include_assets,
        asset_types: asset_types.clone(),
        ..Default::default()
    });

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    // æ£€æŸ¥æ˜¯å¦ä»å¤±è´¥ä»»åŠ¡æ¢å¤ï¼ˆå¤‡ä»½æ“ä½œæ˜¯åŸå­çš„ï¼Œæ¢å¤ = é‡æ–°æ‰§è¡Œï¼‰
    let previous_items = job_ctx.get_processed_items();
    let is_retrying = !previous_items.is_empty();

    if is_retrying {
        info!("[data_governance] ä»å¤±è´¥ä»»åŠ¡é‡æ–°æ‰§è¡Œå¤‡ä»½ï¼ˆåŸå­æ“ä½œï¼Œé‡æ–°å¼€å§‹ï¼‰");
    }

    // é˜¶æ®µ 1: å‡†å¤‡ä¸­
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        5.0,
        Some(if is_retrying {
            "é‡æ–°æ‰§è¡Œå¤‡ä»½ï¼Œæ­£åœ¨å‡†å¤‡...".to_string()
        } else {
            "æ­£åœ¨å‡†å¤‡å¤‡ä»½...".to_string()
        }),
        0,
        4, // æ€»å…± 4 ä¸ªæ•°æ®åº“
    );

    // åˆå§‹åŒ–æ£€æŸ¥ç‚¹ï¼ˆå§‹ç»ˆé‡æ–°åˆå§‹åŒ–ï¼Œå› ä¸ºå¤‡ä»½æ˜¯åŸå­æ“ä½œï¼‰
    job_ctx.init_checkpoint(4); // 4 ä¸ªæ•°æ®åº“

    // æ£€æŸ¥å–æ¶ˆ
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let mut manager = BackupManager::new(backup_dir);
    manager.set_app_data_dir(app_data_dir.clone());
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    // é˜¶æ®µ 2: æ‰§è¡Œ checkpoint
    job_ctx.mark_running(
        BackupJobPhase::Checkpoint,
        10.0,
        Some("æ­£åœ¨æ‰§è¡Œæ•°æ®åº“ checkpoint...".to_string()),
        0,
        4,
    );

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // æ‰§è¡Œå¤‡ä»½ï¼ˆåŸå­æ“ä½œï¼šä¸€æ¬¡æ€§å¤‡ä»½æ‰€æœ‰æ•°æ®åº“ï¼‰
    let result = match backup_type.as_str() {
        "incremental" => {
            let base = match base_version {
                Some(v) => v,
                None => {
                    job_ctx.fail("å¢é‡å¤‡ä»½éœ€è¦æŒ‡å®š base_version å‚æ•°".to_string());
                    return;
                }
            };

            job_ctx.mark_running(
                BackupJobPhase::Compress,
                30.0,
                Some("æ­£åœ¨æ‰§è¡Œå¢é‡å¤‡ä»½...".to_string()),
                0,
                4,
            );

            manager.backup_incremental(&base)
        }
        _ => {
            if include_assets {
                let asset_config = if let Some(types) = asset_types {
                    let parsed_types: Vec<AssetType> = types
                        .iter()
                        .filter_map(|s| AssetType::from_str(s))
                        .collect();
                    if parsed_types.is_empty() {
                        AssetBackupConfig::default()
                    } else {
                        AssetBackupConfig {
                            asset_types: parsed_types,
                            ..Default::default()
                        }
                    }
                } else {
                    AssetBackupConfig::default()
                };

                job_ctx.mark_running(
                    BackupJobPhase::Compress,
                    30.0,
                    Some("æ­£åœ¨å¤‡ä»½æ•°æ®åº“å’Œèµ„äº§æ–‡ä»¶...".to_string()),
                    0,
                    4,
                );

                manager.backup_with_assets(Some(asset_config))
            } else {
                job_ctx.mark_running(
                    BackupJobPhase::Compress,
                    30.0,
                    Some("æ­£åœ¨å¤‡ä»½æ•°æ®åº“...".to_string()),
                    0,
                    4,
                );

                manager.backup_full()
            }
        }
    };

    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¤‡ä»½".to_string()));
        return;
    }

    // é˜¶æ®µ 4: éªŒè¯
    job_ctx.mark_running(
        BackupJobPhase::Verify,
        80.0,
        Some("æ­£åœ¨éªŒè¯å¤‡ä»½...".to_string()),
        3,
        4,
    );

    let duration_ms = start.elapsed().as_millis() as u64;

    match result {
        Ok(manifest) => {
            // æ ‡è®°æ‰€æœ‰æ•°æ®åº“ä¸ºå·²å¤„ç†
            for file in &manifest.files {
                if let Some(db_id) = &file.database_id {
                    job_ctx.update_checkpoint(db_id);
                }
            }

            let db_size: u64 = manifest.files.iter().map(|f| f.size).sum();
            let asset_size: u64 = manifest.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
            let backup_size = db_size + asset_size;

            let databases_backed_up: Vec<String> = manifest
                .files
                .iter()
                .filter_map(|f| f.database_id.clone())
                .collect();

            info!(
                "[data_governance] åå°å¤‡ä»½æˆåŠŸ: id={}, files={}, size={}, duration={}ms, retried={}",
                manifest.backup_id,
                manifest.files.len(),
                backup_size,
                duration_ms,
                is_retrying
            );

            let result_payload = BackupJobResultPayload {
                success: true,
                output_path: Some(manifest.backup_id.clone()),
                resolved_path: None,
                message: Some(format!(
                    "å¤‡ä»½å®Œæˆ: {} ä¸ªæ•°æ®åº“, {} å­—èŠ‚{}",
                    databases_backed_up.len(),
                    backup_size,
                    if is_retrying { " (é‡æ–°æ‰§è¡Œ)" } else { "" }
                )),
                error: None,
                duration_ms: Some(duration_ms),
                stats: Some(serde_json::json!({
                    "databases_backed_up": databases_backed_up,
                    "backup_size": backup_size,
                    "db_files": manifest.files.len(),
                    "asset_files": manifest.assets.as_ref().map(|a| a.total_files).unwrap_or(0),
                    "retried_from_failure": is_retrying,
                })),
                requires_restart: false,
                checkpoint_path: None,
                resumable_job_id: None,
            };

            job_ctx.complete(
                Some(format!("å¤‡ä»½å®Œæˆ: {}", manifest.backup_id)),
                databases_backed_up.len() as u64,
                databases_backed_up.len() as u64,
                result_payload,
            );
        }
        Err(e) => {
            error!("[data_governance] åå°å¤‡ä»½å¤±è´¥: {}", e);
            job_ctx.fail(format!("å¤‡ä»½å¤±è´¥: {}", e));
        }
    }
}

/// æ‰§è¡Œå¯æ¢å¤çš„ ZIP å¯¼å…¥ï¼ˆå¸¦æ–­ç‚¹ç»­ä¼ æ”¯æŒï¼‰
///
/// ä¸ execute_zip_import_with_progress ç±»ä¼¼ï¼Œä½†ä¼šï¼š
/// 1. è®¾ç½®ä»»åŠ¡å‚æ•°ä¾›æŒä¹…åŒ–
/// 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹
/// 3. æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ç›®æ ‡ç›®å½•ä¸­å·²å­˜åœ¨ä¸”å¤§å°åŒ¹é…çš„æ–‡ä»¶
async fn execute_zip_import_with_progress_resumable(
    app: tauri::AppHandle,
    job_ctx: BackupJobContext,
    zip_file_path: PathBuf,
    backup_id: Option<String>,
) {
    use super::backup::zip_export::{import_backup_from_zip_resumable, ZipImportPhase};
    use std::time::Instant;

    let start = Instant::now();

    // å…¨å±€äº’æ–¥ï¼šé¿å…å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _global_permit =
        match acquire_backup_global_permit(&job_ctx, "æ­£åœ¨ç­‰å¾…å…¶ä»–å¤‡ä»½/æ¢å¤ä»»åŠ¡å®Œæˆ...").await
        {
            Some(p) => p,
            None => return,
        };

    // è®¾ç½®ä»»åŠ¡å‚æ•°ï¼ˆç”¨äºæŒä¹…åŒ–å’Œæ¢å¤ï¼‰
    job_ctx.set_params(BackupJobParams {
        zip_path: Some(zip_file_path.to_string_lossy().to_string()),
        backup_id: backup_id.clone(),
        ..Default::default()
    });

    // è·å–åº”ç”¨æ•°æ®ç›®å½•
    let app_data_dir = match get_app_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            job_ctx.fail(format!("è·å–åº”ç”¨æ•°æ®ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    };
    let backup_dir = get_backup_dir(&app_data_dir);

    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if !backup_dir.exists() {
        if let Err(e) = std::fs::create_dir_all(&backup_dir) {
            job_ctx.fail(format!("åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {}", e));
            return;
        }
    }

    // è·å–å·²å¤„ç†çš„é¡¹ç›®åˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
    let processed_items = job_ctx.get_processed_items();
    let is_resuming = !processed_items.is_empty();

    if is_resuming {
        info!(
            "[data_governance] ä»æ£€æŸ¥ç‚¹æ¢å¤ ZIP å¯¼å…¥ä»»åŠ¡ï¼Œå·²å¤„ç† {} ä¸ªæ–‡ä»¶",
            processed_items.len()
        );
    }

    // ç¡®å®šå¤‡ä»½ ID
    let generated_backup_id = backup_id.unwrap_or_else(|| {
        use uuid::Uuid;
        let now = chrono::Utc::now();
        let timestamp = now.format("%Y%m%d_%H%M%S").to_string();
        let millis = now.timestamp_subsec_millis();
        let rand8 = &Uuid::new_v4().simple().to_string()[..8];
        format!("{}_{}_{:03}_imported", timestamp, rand8, millis)
    });

    let target_backup_id = match validate_backup_id(&generated_backup_id) {
        Ok(id) => id,
        Err(e) => {
            job_ctx.fail(format!("backup_id éæ³•: {}", e));
            return;
        }
    };

    let target_dir = backup_dir.join(&target_backup_id);

    // å¦‚æœæ˜¯æ¢å¤ï¼Œç›®æ ‡ç›®å½•å¯èƒ½å·²ç»å­˜åœ¨ï¼ˆéƒ¨åˆ†è§£å‹ï¼‰
    if target_dir.exists() && !is_resuming {
        if let Err(e) = ensure_existing_path_within_backup_dir(&target_dir, &backup_dir) {
            job_ctx.fail(format!("å¤‡ä»½è·¯å¾„æ ¡éªŒå¤±è´¥: {}", e));
            return;
        }
        job_ctx.fail(format!("å¤‡ä»½å·²å­˜åœ¨: {}", target_backup_id));
        return;
    }

    // é˜¶æ®µ 1: æ‰«æ
    job_ctx.mark_running(
        BackupJobPhase::Scan,
        0.0,
        Some(if is_resuming {
            "ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œæ­£åœ¨éªŒè¯ ZIP æ–‡ä»¶...".to_string()
        } else {
            "æ­£åœ¨éªŒè¯ ZIP æ–‡ä»¶...".to_string()
        }),
        processed_items.len() as u64,
        0,
    );

    // æ£€æŸ¥å–æ¶ˆ
    if job_ctx.is_cancelled() {
        job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¯¼å…¥".to_string()));
        return;
    }

    // ä½¿ç”¨å¸¦è¿›åº¦çš„å¯¼å…¥å‡½æ•°
    let job_ctx_for_progress = job_ctx.clone();
    let job_ctx_for_cancel = job_ctx.clone();

    // æ–­ç‚¹ç»­ä¼ ï¼šä½¿ç”¨ import_backup_from_zip_resumableï¼Œ
    // è‡ªåŠ¨è·³è¿‡ç›®æ ‡ç›®å½•ä¸­å·²å­˜åœ¨ä¸”å¤§å°åŒ¹é…çš„æ–‡ä»¶
    let result = import_backup_from_zip_resumable(
        &zip_file_path,
        &target_dir,
        |progress| {
            let phase = match progress.phase {
                ZipImportPhase::Scan => BackupJobPhase::Scan,
                ZipImportPhase::Extract => BackupJobPhase::Extract,
                ZipImportPhase::Verify => BackupJobPhase::Verify,
                ZipImportPhase::Completed => BackupJobPhase::Completed,
            };

            job_ctx_for_progress.mark_running(
                phase,
                progress.progress,
                Some(
                    if is_resuming && progress.phase == ZipImportPhase::Extract {
                        format!("(æ–­ç‚¹ç»­ä¼ ) {}", progress.message)
                    } else {
                        progress.message
                    },
                ),
                progress.processed_files as u64,
                progress.total_files as u64,
            );

            // æ›´æ–°æ£€æŸ¥ç‚¹
            if let Some(ref file_name) = progress.current_file {
                job_ctx_for_progress.update_checkpoint(file_name);
            }
        },
        || job_ctx_for_cancel.is_cancelled(),
    );

    match result {
        Ok(file_count) => {
            let duration_ms = start.elapsed().as_millis() as u64;

            // é˜¶æ®µ 4: æ¸…ç†ï¼ˆ90% - 100%ï¼‰
            job_ctx.mark_running(
                BackupJobPhase::Cleanup,
                95.0,
                Some("æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...".to_string()),
                file_count as u64,
                file_count as u64,
            );

            // å®Œæˆ
            let result_payload = BackupJobResultPayload {
                success: true,
                output_path: Some(target_backup_id.clone()),
                resolved_path: Some(target_dir.to_string_lossy().to_string()),
                message: Some(format!(
                    "ZIP å¯¼å…¥å®Œæˆ: {} ä¸ªæ–‡ä»¶, è€—æ—¶ {}ms{}",
                    file_count,
                    duration_ms,
                    if is_resuming {
                        " (ä»æ£€æŸ¥ç‚¹æ¢å¤)"
                    } else {
                        ""
                    }
                )),
                error: None,
                duration_ms: Some(duration_ms),
                stats: Some(serde_json::json!({
                    "backup_id": target_backup_id,
                    "file_count": file_count,
                    "zip_path": zip_file_path.to_string_lossy(),
                    "resumed_from_checkpoint": is_resuming,
                })),
                requires_restart: false,
                checkpoint_path: None,
                resumable_job_id: None,
            };

            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count,
                            total_size: 0,
                        },
                        format!("zip_import/{}", target_backup_id),
                    )
                    .complete(duration_ms)
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "zip_path": zip_file_path.to_string_lossy(),
                        "backup_id": target_backup_id,
                        "backup_path": target_dir.to_string_lossy(),
                        "file_count": file_count,
                        "resumed_from_checkpoint": is_resuming,
                        "subtype": "zip_import_resumable",
                    })),
                );
            }

            job_ctx.complete(
                Some(format!("ZIP å¯¼å…¥å®Œæˆ: {}", target_backup_id)),
                file_count as u64,
                file_count as u64,
                result_payload,
            );
        }
        Err(e) => {
            let error_msg = e.to_string();
            if error_msg.contains("ç”¨æˆ·å–æ¶ˆ") || error_msg.contains("Interrupted") {
                job_ctx.cancelled(Some("ç”¨æˆ·å–æ¶ˆå¯¼å…¥".to_string()));
            } else {
                error!("[data_governance] ZIP å¯¼å…¥å¤±è´¥: {}", e);
                job_ctx.fail(format!("ZIP å¯¼å…¥å¤±è´¥: {}", e));
            }

            #[cfg(feature = "data_governance")]
            {
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Backup {
                            backup_type: super::audit::BackupType::Full,
                            file_count: 0,
                            total_size: 0,
                        },
                        format!("zip_import/{}", target_backup_id),
                    )
                    .fail(error_msg.clone())
                    .with_details(serde_json::json!({
                        "job_id": job_ctx.job_id.clone(),
                        "zip_path": zip_file_path.to_string_lossy(),
                        "backup_id": target_backup_id,
                        "backup_path": target_dir.to_string_lossy(),
                        "resumed_from_checkpoint": is_resuming,
                        "subtype": "zip_import_resumable",
                    })),
                );
            }
        }
    }
}

/// æ¢å¤ç»“æœå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct RestoreResultResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// å¤‡ä»½ ID
    pub backup_id: String,
    /// æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// å·²æ¢å¤çš„æ•°æ®åº“åˆ—è¡¨
    pub databases_restored: Vec<String>,
    /// é¢„æ¢å¤å¤‡ä»½è·¯å¾„ï¼ˆç”¨äºå›æ»šï¼‰
    pub pre_restore_backup_path: Option<String>,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    pub error_message: Option<String>,
    /// æ¢å¤çš„èµ„äº§æ–‡ä»¶æ•°é‡
    #[serde(skip_serializing_if = "Option::is_none")]
    pub assets_restored: Option<usize>,
}

// ==================== èµ„äº§å¤‡ä»½ç›¸å…³å‘½ä»¤ ====================

/// æ‰«æèµ„äº§ç›®å½•
///
/// è·å–å„èµ„äº§ç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºå¤‡ä»½å‰é¢„è§ˆã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `asset_types`: è¦æ‰«æçš„èµ„äº§ç±»å‹ï¼ˆå¯é€‰ï¼Œä¸ºç©ºè¡¨ç¤ºå…¨éƒ¨ï¼‰
///
/// ## è¿”å›
/// - `AssetScanResponse`: æ‰«æç»“æœ
#[tauri::command]
pub async fn data_governance_scan_assets(
    app: tauri::AppHandle,
    asset_types: Option<Vec<String>>,
) -> Result<AssetScanResponse, String> {
    info!("[data_governance] æ‰«æèµ„äº§ç›®å½•");

    let active_dir = get_active_data_dir(&app)?;

    // è§£æèµ„äº§ç±»å‹
    let types: Vec<AssetType> = asset_types
        .map(|ts| ts.iter().filter_map(|s| AssetType::from_str(s)).collect())
        .unwrap_or_default();

    // æ‰«æèµ„äº§ï¼ˆä½¿ç”¨æ´»åŠ¨æ•°æ®ç©ºé—´ç›®å½•ï¼Œä¸ FileManager è¿è¡Œæ—¶ç»‘å®šçš„ä½ç½®ä¸€è‡´ï¼‰
    let stats = super::backup::assets::scan_assets(&active_dir, &types).map_err(|e| {
        error!("[data_governance] æ‰«æèµ„äº§å¤±è´¥: {}", e);
        format!("æ‰«æèµ„äº§å¤±è´¥: {}", e)
    })?;

    // è®¡ç®—æ€»è®¡
    let total_files: usize = stats.values().map(|s| s.file_count).sum();
    let total_size: u64 = stats.values().map(|s| s.total_size).sum();

    info!(
        "[data_governance] æ‰«æå®Œæˆ: types={}, files={}, size={}",
        stats.len(),
        total_files,
        total_size
    );

    Ok(AssetScanResponse {
        by_type: stats,
        total_files,
        total_size,
    })
}

/// èµ„äº§æ‰«æå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct AssetScanResponse {
    /// æŒ‰èµ„äº§ç±»å‹ç»Ÿè®¡
    pub by_type: std::collections::HashMap<String, AssetTypeStats>,
    /// æ€»æ–‡ä»¶æ•°
    pub total_files: usize,
    /// æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub total_size: u64,
}

/// è·å–æ”¯æŒçš„èµ„äº§ç±»å‹
///
/// è¿”å›ç³»ç»Ÿæ”¯æŒçš„æ‰€æœ‰èµ„äº§ç±»å‹åŠå…¶ä¿¡æ¯ã€‚
///
/// ## è¿”å›
/// - `Vec<AssetTypeInfo>`: èµ„äº§ç±»å‹åˆ—è¡¨
#[tauri::command]
pub fn data_governance_get_asset_types() -> Vec<AssetTypeInfo> {
    AssetType::all()
        .into_iter()
        .map(|t| AssetTypeInfo {
            id: t.as_str().to_string(),
            name: t.display_name().to_string(),
            relative_path: t.relative_path().to_string(),
            priority: t.priority(),
        })
        .collect()
}

/// èµ„äº§ç±»å‹ä¿¡æ¯
#[derive(Debug, Clone, serde::Serialize)]
pub struct AssetTypeInfo {
    /// èµ„äº§ç±»å‹ ID
    pub id: String,
    /// æ˜¾ç¤ºåç§°
    pub name: String,
    /// ç›¸å¯¹è·¯å¾„
    pub relative_path: String,
    /// ä¼˜å…ˆçº§ï¼ˆ0 ä¸ºæœ€é«˜ï¼‰
    pub priority: u8,
}

/// æ‰§è¡ŒåŒ…å«èµ„äº§çš„æ¢å¤
///
/// ä»å¤‡ä»½æ¢å¤æ•°æ®åº“å’Œèµ„äº§æ–‡ä»¶ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦æ¢å¤çš„å¤‡ä»½ ID
/// - `restore_assets`: æ˜¯å¦æ¢å¤èµ„äº§æ–‡ä»¶
///
/// ## è¿”å›
/// - `RestoreResultResponse`: æ¢å¤ç»“æœ
#[tauri::command]
pub async fn data_governance_restore_with_assets(
    app: tauri::AppHandle,
    backup_id: String,
    restore_assets: Option<bool>,
) -> Result<RestoreResultResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;
    let restore_assets = restore_assets.unwrap_or(false);
    info!(
        "[data_governance] å¼€å§‹æ¢å¤å¤‡ä»½ï¼ˆå«èµ„äº§ï¼‰: id={}, restore_assets={}",
        validated_backup_id, restore_assets
    );

    let start = Instant::now();
    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®".to_string());
    }

    // å…¨å±€äº’æ–¥ï¼šé¿å…ä¸æ­£åœ¨è¿è¡Œçš„å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _permit = BACKUP_GLOBAL_LIMITER
        .clone()
        .acquire_owned()
        .await
        .map_err(|e| format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e))?;

    // åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨
    let mut manager = BackupManager::new(backup_dir.clone());
    manager.set_app_data_dir(app_data_dir.clone());
    manager.set_app_version(env!("CARGO_PKG_VERSION").to_string());

    // è·å–å¤‡ä»½æ¸…å•
    let manifests = manager.list_backups().map_err(|e| {
        error!("[data_governance] è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e);
        format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e)
    })?;

    let manifest = manifests
        .iter()
        .find(|m| m.backup_id == validated_backup_id)
        .ok_or_else(|| format!("å¤‡ä»½ä¸å­˜åœ¨: {}", validated_backup_id))?;

    let manifest_dir = backup_dir.join(&manifest.backup_id);
    ensure_existing_path_within_backup_dir(&manifest_dir, &backup_dir)?;

    // æ¢å¤åˆ°éæ´»è·ƒæ’æ§½ï¼Œé¿å… Windows OS error 32ï¼ˆæ´»è·ƒæ’æ§½æ–‡ä»¶è¢«è¿æ¥æ± æŒæœ‰ï¼‰
    let (inactive_dir, inactive_slot) = match crate::data_space::get_data_space_manager() {
        Some(mgr) => {
            let slot = mgr.inactive_slot();
            let dir = mgr.slot_dir(slot);
            info!(
                "[data_governance] æ¢å¤ç›®æ ‡: éæ´»è·ƒæ’æ§½ {} ({})",
                slot.name(),
                dir.display()
            );
            (dir, Some(slot))
        }
        None => {
            let dir = app_data_dir.join("slots").join("slotB");
            warn!("[data_governance] DataSpaceManager æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ° slotB");
            (dir, None)
        }
    };

    // ç£ç›˜ç©ºé—´é¢„æ£€æŸ¥
    {
        let db_size: u64 = manifest.files.iter().map(|f| f.size).sum();
        let asset_size: u64 = manifest.assets.as_ref().map(|a| a.total_size).unwrap_or(0);
        let required = (db_size + asset_size).saturating_mul(2);
        match crate::backup_common::get_available_disk_space(&app_data_dir) {
            Ok(available) if available < required => {
                return Err(format!(
                    "ç£ç›˜ç©ºé—´ä¸è¶³ï¼šéœ€è¦ {:.1} MBï¼Œä»…å‰© {:.1} MBã€‚è¯·æ¸…ç†å­˜å‚¨ç©ºé—´åé‡è¯•",
                    required as f64 / 1024.0 / 1024.0,
                    available as f64 / 1024.0 / 1024.0
                ));
            }
            Err(e) => {
                warn!("[data_governance] ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥ï¼ˆç»§ç»­æ¢å¤ï¼‰: {}", e);
            }
            _ => {}
        }
    }

    // æ‰§è¡Œæ¢å¤åˆ°éæ´»è·ƒæ’æ§½ï¼ˆä¸éœ€è¦ç»´æŠ¤æ¨¡å¼ï¼Œä¸æ¶‰åŠæ´»è·ƒæ–‡ä»¶ï¼‰
    let result = manager.restore_with_assets_to_dir(manifest, restore_assets, &inactive_dir);
    let duration_ms = start.elapsed().as_millis() as u64;

    match result {
        Ok(restored_assets) => {
            let databases_restored: Vec<String> = manifest
                .files
                .iter()
                .filter_map(|f| f.database_id.clone())
                .collect();

            info!(
                "[data_governance] æ¢å¤æˆåŠŸ: id={}, databases={:?}, assets={}, duration={}ms, target={}",
                validated_backup_id, databases_restored, restored_assets, duration_ms, inactive_dir.display()
            );

            // æ ‡è®°ä¸‹æ¬¡é‡å¯æ—¶åˆ‡æ¢åˆ°æ¢å¤ç›®æ ‡æ’æ§½
            if let Some(slot) = inactive_slot {
                if let Some(mgr) = crate::data_space::get_data_space_manager() {
                    if let Err(e) = mgr.mark_pending_switch(slot) {
                        error!("[data_governance] æ ‡è®°æ’æ§½åˆ‡æ¢å¤±è´¥: {}ï¼Œæ¢å¤çš„æ•°æ®åœ¨ {} ä¸­ï¼Œéœ€æ‰‹åŠ¨åˆ‡æ¢", e, inactive_dir.display());
                    } else {
                        info!("[data_governance] å·²æ ‡è®°ä¸‹æ¬¡é‡å¯åˆ‡æ¢åˆ° {}", slot.name());
                    }
                }
            }

            Ok(RestoreResultResponse {
                success: true,
                backup_id: backup_id.clone(),
                duration_ms,
                databases_restored,
                pre_restore_backup_path: Some(
                    inactive_dir.to_string_lossy().to_string(),
                ),
                error_message: None,
                assets_restored: if restore_assets {
                    Some(restored_assets)
                } else {
                    None
                },
            })
        }
        Err(e) => {
            error!("[data_governance] æ¢å¤å¤±è´¥: {}", e);
            Err(format!(
                "æ¢å¤å¤‡ä»½å¤±è´¥: {}ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç†ã€æŸ¥çœ‹å¤‡ä»½çŠ¶æ€æˆ–é‡è¯•",
                e
            ))
        }
    }
}

/// éªŒè¯å¤‡ä»½å®Œæ•´æ€§ï¼ˆå«èµ„äº§ï¼‰
///
/// éªŒè¯å¤‡ä»½æ–‡ä»¶å’Œèµ„äº§æ–‡ä»¶çš„å®Œæ•´æ€§ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `backup_id`: è¦éªŒè¯çš„å¤‡ä»½ ID
///
/// ## è¿”å›
/// - `BackupVerifyWithAssetsResponse`: éªŒè¯ç»“æœ
#[tauri::command]
pub async fn data_governance_verify_backup_with_assets(
    app: tauri::AppHandle,
    backup_id: String,
) -> Result<BackupVerifyWithAssetsResponse, String> {
    let validated_backup_id = validate_backup_id(&backup_id)?;
    info!(
        "[data_governance] éªŒè¯å¤‡ä»½ï¼ˆå«èµ„äº§ï¼‰: {}",
        validated_backup_id
    );

    let app_data_dir = get_app_data_dir(&app)?;
    let backup_dir = get_backup_dir(&app_data_dir);

    if !backup_dir.exists() {
        return Err("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å‰å¾€ã€Œè®¾ç½® > æ•°æ®æ²»ç† > å¤‡ä»½ã€æ£€æŸ¥å¤‡ä»½ç›®å½•é…ç½®".to_string());
    }

    let mut manager = BackupManager::new(backup_dir);
    manager.set_app_data_dir(app_data_dir.clone());

    // å…¨å±€äº’æ–¥ï¼šé¿å…ä¸æ­£åœ¨è¿è¡Œçš„å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘
    let _permit = BACKUP_GLOBAL_LIMITER
        .clone()
        .acquire_owned()
        .await
        .map_err(|e| format!("è·å–å…¨å±€å¤‡ä»½é”å¤±è´¥: {}", e))?;

    // è·å–å¤‡ä»½åˆ—è¡¨å¹¶æŸ¥æ‰¾æŒ‡å®šçš„å¤‡ä»½
    let manifests = manager
        .list_backups()
        .map_err(|e| format!("è·å–å¤‡ä»½åˆ—è¡¨å¤±è´¥: {}", e))?;

    let manifest = manifests
        .iter()
        .find(|m| m.backup_id == validated_backup_id)
        .ok_or_else(|| format!("å¤‡ä»½ä¸å­˜åœ¨: {}", validated_backup_id))?;

    let manifest_dir = app_data_dir.join("backups").join(&manifest.backup_id);
    ensure_existing_path_within_backup_dir(&manifest_dir, &app_data_dir.join("backups"))?;

    // éªŒè¯å¤‡ä»½
    let verify_result = manager
        .verify_with_assets(manifest)
        .map_err(|e| format!("éªŒè¯å¤±è´¥: {}", e))?;

    let has_assets = manifest.assets.is_some();
    let asset_file_count = manifest.assets.as_ref().map(|a| a.total_files).unwrap_or(0);

    info!(
        "[data_governance] éªŒè¯å®Œæˆ: id={}, is_valid={}, db_errors={}, asset_errors={}",
        validated_backup_id,
        verify_result.is_valid,
        verify_result.database_errors.len(),
        verify_result.asset_errors.len()
    );

    Ok(BackupVerifyWithAssetsResponse {
        is_valid: verify_result.is_valid,
        database_errors: verify_result.database_errors,
        asset_errors: verify_result
            .asset_errors
            .iter()
            .map(|e| AssetVerifyErrorResponse {
                path: e.path.clone(),
                error_type: e.error_type.clone(),
                message: e.message.clone(),
            })
            .collect(),
        has_assets,
        asset_file_count,
    })
}

/// å¤‡ä»½éªŒè¯å“åº”ï¼ˆå«èµ„äº§ï¼‰
#[derive(Debug, Clone, serde::Serialize)]
pub struct BackupVerifyWithAssetsResponse {
    /// æ˜¯å¦å…¨éƒ¨æœ‰æ•ˆ
    pub is_valid: bool,
    /// æ•°æ®åº“éªŒè¯é”™è¯¯
    pub database_errors: Vec<String>,
    /// èµ„äº§éªŒè¯é”™è¯¯
    pub asset_errors: Vec<AssetVerifyErrorResponse>,
    /// æ˜¯å¦åŒ…å«èµ„äº§
    pub has_assets: bool,
    /// èµ„äº§æ–‡ä»¶æ•°é‡
    pub asset_file_count: usize,
}

/// èµ„äº§éªŒè¯é”™è¯¯å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct AssetVerifyErrorResponse {
    /// æ–‡ä»¶è·¯å¾„
    pub path: String,
    /// é”™è¯¯ç±»å‹
    pub error_type: String,
    /// é”™è¯¯ä¿¡æ¯
    pub message: String,
}

// ==================== åŒæ­¥ç›¸å…³å‘½ä»¤ ====================

use super::sync::{
    ApplyChangesResult, ChangeLogEntry, ChangeLogStats, ConflictDetectionResult, DatabaseSyncState,
    MergeApplicationResult, MergeStrategy, PendingChanges, SyncChangeWithData, SyncDirection,
    SyncExecutionResult, SyncManager, SyncManifest,
};
use crate::cloud_storage::{create_storage, CloudStorage, CloudStorageConfig};
use std::collections::HashMap;

/// è·å–åŒæ­¥çŠ¶æ€
///
/// è¿”å›å½“å‰è®¾å¤‡çš„åŒæ­¥çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¾…åŒæ­¥å˜æ›´æ•°é‡ç­‰ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
///
/// ## è¿”å›
/// - `SyncStatusResponse`: åŒæ­¥çŠ¶æ€ä¿¡æ¯
#[tauri::command]
pub async fn data_governance_get_sync_status(
    app: tauri::AppHandle,
) -> Result<SyncStatusResponse, String> {
    debug!("[data_governance] è·å–åŒæ­¥çŠ¶æ€");

    // P0-6: ç»´æŠ¤æ¨¡å¼æ£€æŸ¥â€”â€”ç¦æ­¢åœ¨å¤‡ä»½/æ¢å¤/è¿ç§»æœŸé—´è®¿é—®æ•°æ®åº“æ–‡ä»¶
    check_maintenance_mode(&app)?;

    let active_dir = get_active_data_dir(&app)?;

    let mut databases_status: Vec<DatabaseSyncStatusResponse> = Vec::new();
    let mut total_pending_changes = 0usize;
    let mut total_synced_changes = 0usize;

    // éå†æ‰€æœ‰æ•°æ®åº“è·å–åŒæ­¥çŠ¶æ€
    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            // æ‰“å¼€æ•°æ®åº“è¿æ¥
            match rusqlite::Connection::open(&db_path) {
                Ok(conn) => {
                    // æ£€æŸ¥ __change_log è¡¨æ˜¯å¦å­˜åœ¨
                    let table_exists: bool = conn
                        .query_row(
                            "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='__change_log')",
                            [],
                            |row| row.get(0),
                        )
                        .unwrap_or(false);

                    if table_exists {
                        // è·å–å˜æ›´æ—¥å¿—ç»Ÿè®¡
                        match SyncManager::get_change_log_stats(&conn) {
                            Ok(stats) => {
                                total_pending_changes += stats.pending_count;
                                total_synced_changes += stats.synced_count;

                                // è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼šå– __change_log ä¸­æœ€æ–°å·²åŒæ­¥è®°å½•çš„æ—¶é—´æˆ³
                                let last_sync: Option<String> = conn
                                    .query_row(
                                        "SELECT MAX(changed_at) FROM __change_log WHERE sync_version > 0",
                                        [],
                                        |row| row.get(0),
                                    )
                                    .ok()
                                    .flatten();

                                databases_status.push(DatabaseSyncStatusResponse {
                                    id: db_id.as_str().to_string(),
                                    has_change_log: true,
                                    pending_changes: stats.pending_count,
                                    synced_changes: stats.synced_count,
                                    last_sync_at: last_sync,
                                });
                            }
                            Err(e) => {
                                debug!(
                                    "[data_governance] è·å–æ•°æ®åº“ {:?} å˜æ›´æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {}",
                                    db_id, e
                                );
                                databases_status.push(DatabaseSyncStatusResponse {
                                    id: db_id.as_str().to_string(),
                                    has_change_log: true,
                                    pending_changes: 0,
                                    synced_changes: 0,
                                    last_sync_at: None,
                                });
                            }
                        }
                    } else {
                        databases_status.push(DatabaseSyncStatusResponse {
                            id: db_id.as_str().to_string(),
                            has_change_log: false,
                            pending_changes: 0,
                            synced_changes: 0,
                            last_sync_at: None,
                        });
                    }
                }
                Err(e) => {
                    debug!("[data_governance] æ‰“å¼€æ•°æ®åº“ {:?} å¤±è´¥: {}", db_id, e);
                }
            }
        }
    }

    let has_pending_changes = total_pending_changes > 0;

    info!(
        "[data_governance] åŒæ­¥çŠ¶æ€: pending={}, synced={}, databases={}",
        total_pending_changes,
        total_synced_changes,
        databases_status.len()
    );

    Ok(SyncStatusResponse {
        has_pending_changes,
        total_pending_changes,
        total_synced_changes,
        databases: databases_status,
        last_sync_at: None, // TODO: ä»å…¨å±€å…ƒæ•°æ®è·å–
        device_id: get_device_id(&app),
    })
}

/// è·å–è®¾å¤‡ IDï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
///
/// è®¾å¤‡ ID ä¼šè¢«æŒä¹…åŒ–ä¿å­˜åˆ°åº”ç”¨æ•°æ®ç›®å½•ä¸‹çš„ `device_id` æ–‡ä»¶ä¸­ã€‚
/// é¦–æ¬¡å¯åŠ¨æ—¶ç”Ÿæˆæ–°çš„ UUID å¹¶ä¿å­˜ï¼Œåç»­å¯åŠ¨æ—¶ä»æ–‡ä»¶è¯»å–ã€‚
/// ä½¿ç”¨ OnceLock ç¼“å­˜å·²è¯»å–çš„è®¾å¤‡ IDï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶ã€‚
fn get_device_id(app: &tauri::AppHandle) -> String {
    use std::sync::OnceLock;
    static DEVICE_ID: OnceLock<String> = OnceLock::new();

    DEVICE_ID
        .get_or_init(|| {
            // å°è¯•è·å–åº”ç”¨æ•°æ®ç›®å½•
            let app_data_dir = match app.path().app_data_dir() {
                Ok(dir) => dir,
                Err(e) => {
                    tracing::warn!("æ— æ³•è·å–åº”ç”¨æ•°æ®ç›®å½•ï¼Œä½¿ç”¨ä¸´æ—¶è®¾å¤‡ ID: {}", e);
                    return uuid::Uuid::new_v4().to_string();
                }
            };

            // ç¡®ä¿ç›®å½•å­˜åœ¨
            if let Err(e) = std::fs::create_dir_all(&app_data_dir) {
                tracing::warn!("æ— æ³•åˆ›å»ºåº”ç”¨æ•°æ®ç›®å½•ï¼Œä½¿ç”¨ä¸´æ—¶è®¾å¤‡ ID: {}", e);
                return uuid::Uuid::new_v4().to_string();
            }

            let device_id_path = app_data_dir.join("device_id");

            // å°è¯•è¯»å–ç°æœ‰è®¾å¤‡ ID
            if let Ok(id) = std::fs::read_to_string(&device_id_path) {
                let id = id.trim();
                if !id.is_empty() {
                    tracing::info!("ä»æ–‡ä»¶åŠ è½½è®¾å¤‡ ID: {}", id);
                    return id.to_string();
                }
            }

            // ç”Ÿæˆæ–°è®¾å¤‡ ID
            let new_id = uuid::Uuid::new_v4().to_string();
            tracing::info!("ç”Ÿæˆæ–°è®¾å¤‡ ID: {}", new_id);

            // ä¿å­˜åˆ°æ–‡ä»¶
            if let Err(e) = std::fs::write(&device_id_path, &new_id) {
                tracing::warn!("æ— æ³•ä¿å­˜è®¾å¤‡ ID åˆ°æ–‡ä»¶: {}", e);
            } else {
                tracing::info!("è®¾å¤‡ ID å·²ä¿å­˜åˆ°: {:?}", device_id_path);
            }

            new_id
        })
        .clone()
}

/// åŒæ­¥çŠ¶æ€å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SyncStatusResponse {
    /// æ˜¯å¦æœ‰å¾…åŒæ­¥çš„å˜æ›´
    pub has_pending_changes: bool,
    /// å¾…åŒæ­¥å˜æ›´æ€»æ•°
    pub total_pending_changes: usize,
    /// å·²åŒæ­¥å˜æ›´æ€»æ•°
    pub total_synced_changes: usize,
    /// å„æ•°æ®åº“çš„åŒæ­¥çŠ¶æ€
    pub databases: Vec<DatabaseSyncStatusResponse>,
    /// ä¸Šæ¬¡åŒæ­¥æ—¶é—´
    pub last_sync_at: Option<String>,
    /// è®¾å¤‡ ID
    pub device_id: String,
}

/// æ•°æ®åº“åŒæ­¥çŠ¶æ€å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseSyncStatusResponse {
    /// æ•°æ®åº“ ID
    pub id: String,
    /// æ˜¯å¦æœ‰å˜æ›´æ—¥å¿—è¡¨
    pub has_change_log: bool,
    /// å¾…åŒæ­¥å˜æ›´æ•°é‡
    pub pending_changes: usize,
    /// å·²åŒæ­¥å˜æ›´æ•°é‡
    pub synced_changes: usize,
    /// ä¸Šæ¬¡åŒæ­¥æ—¶é—´
    pub last_sync_at: Option<String>,
}

/// æ£€æµ‹åŒæ­¥å†²çª
///
/// æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯çš„æ•°æ®çŠ¶æ€ï¼Œæ£€æµ‹å¯èƒ½çš„å†²çªã€‚
/// æ³¨æ„ï¼šæ­¤å‘½ä»¤éœ€è¦äº‘ç«¯æ¸…å•ä½œä¸ºè¾“å…¥ï¼Œå®é™…ä½¿ç”¨ä¸­åº”è¯¥ä»äº‘ç«¯æœåŠ¡è·å–ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `cloud_manifest_json`: äº‘ç«¯åŒæ­¥æ¸…å•çš„ JSON å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
///
/// ## è¿”å›
/// - `ConflictDetectionResponse`: å†²çªæ£€æµ‹ç»“æœ
#[tauri::command]
pub async fn data_governance_detect_conflicts(
    app: tauri::AppHandle,
    cloud_manifest_json: Option<String>,
    cloud_config: Option<CloudStorageConfig>,
) -> Result<ConflictDetectionResponse, String> {
    info!("[data_governance] å¼€å§‹æ£€æµ‹åŒæ­¥å†²çª");

    // P0-6: ç»´æŠ¤æ¨¡å¼æ£€æŸ¥â€”â€”ç¦æ­¢åœ¨å¤‡ä»½/æ¢å¤/è¿ç§»æœŸé—´è®¿é—®æ•°æ®åº“æ–‡ä»¶
    check_maintenance_mode(&app)?;

    let active_dir = get_active_data_dir(&app)?;

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•
    let device_id = get_device_id(&app);
    let manager = SyncManager::new(device_id.clone());
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                // è·å–æ•°æ®åº“åŒæ­¥çŠ¶æ€
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }
            }
        }
    }

    let local_manifest = manager.create_manifest(local_databases);

    // äº‘ç«¯æ¸…å•æ¥æºä¼˜å…ˆçº§ï¼š
    // 1) æ˜¾å¼ä¼ å…¥çš„ cloud_manifest_jsonï¼ˆç”¨äºæµ‹è¯•/è°ƒè¯•ï¼‰
    // 2) ä¼ å…¥ cloud_config æ—¶ï¼Œä»äº‘ç«¯ä¸‹è½½æ¸…å•
    let cloud_manifest: Option<SyncManifest> = if let Some(cloud_json) = cloud_manifest_json {
        Some(serde_json::from_str(&cloud_json).map_err(|e| format!("è§£æäº‘ç«¯æ¸…å•å¤±è´¥: {}", e))?)
    } else if let Some(cfg) = cloud_config {
        let storage = create_storage(&cfg)
            .await
            .map_err(|e| format!("åˆ›å»ºäº‘å­˜å‚¨å¤±è´¥: {}", e))?;
        let cloud = manager
            .download_manifest(storage.as_ref())
            .await
            .map_err(|e| format!("ä»äº‘ç«¯ä¸‹è½½æ¸…å•å¤±è´¥: {}", e))?;
        Some(cloud)
    } else {
        None
    };

    // å¦‚æœæœ‰äº‘ç«¯æ¸…å•ï¼Œè¿›è¡Œæ¯”è¾ƒ
    if let Some(cloud_manifest) = cloud_manifest {
        let detection_result = SyncManager::detect_conflicts(&local_manifest, &cloud_manifest)
            .map_err(|e| format!("å†²çªæ£€æµ‹å¤±è´¥: {}", e))?;

        info!(
            "[data_governance] å†²çªæ£€æµ‹å®Œæˆ: has_conflicts={}, needs_migration={}, db_conflicts={}, record_conflicts={}",
            detection_result.has_conflicts,
            detection_result.needs_migration,
            detection_result.database_conflicts.len(),
            detection_result.record_conflicts.len()
        );

        Ok(ConflictDetectionResponse {
            has_conflicts: detection_result.has_conflicts,
            needs_migration: detection_result.needs_migration,
            database_conflicts: detection_result
                .database_conflicts
                .iter()
                .map(|c| DatabaseConflictResponse {
                    database_name: c.database_name.clone(),
                    conflict_type: format!("{:?}", c.conflict_type),
                    local_version: c.local_state.as_ref().map(|s| s.data_version),
                    cloud_version: c.cloud_state.as_ref().map(|s| s.data_version),
                    local_schema_version: c.local_state.as_ref().map(|s| s.schema_version),
                    cloud_schema_version: c.cloud_state.as_ref().map(|s| s.schema_version),
                })
                .collect(),
            record_conflict_count: detection_result.record_conflicts.len(),
            local_manifest_json: serde_json::to_string(&local_manifest).ok(),
            cloud_manifest_json: serde_json::to_string(&cloud_manifest).ok(),
        })
    } else {
        // æ²¡æœ‰äº‘ç«¯æ¸…å•ï¼Œåªè¿”å›æœ¬åœ°çŠ¶æ€
        info!("[data_governance] æ— äº‘ç«¯æ¸…å•ï¼Œè¿”å›æœ¬åœ°çŠ¶æ€");

        Ok(ConflictDetectionResponse {
            has_conflicts: false,
            needs_migration: false,
            database_conflicts: vec![],
            record_conflict_count: 0,
            local_manifest_json: serde_json::to_string(&local_manifest).ok(),
            cloud_manifest_json: None,
        })
    }
}

/// å†²çªæ£€æµ‹å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct ConflictDetectionResponse {
    /// æ˜¯å¦æœ‰å†²çª
    pub has_conflicts: bool,
    /// æ˜¯å¦éœ€è¦è¿ç§»
    pub needs_migration: bool,
    /// æ•°æ®åº“çº§å†²çªåˆ—è¡¨
    pub database_conflicts: Vec<DatabaseConflictResponse>,
    /// è®°å½•çº§å†²çªæ•°é‡
    pub record_conflict_count: usize,
    /// æœ¬åœ°æ¸…å• JSONï¼ˆç”¨äºè°ƒè¯•ï¼‰
    pub local_manifest_json: Option<String>,
    /// äº‘ç«¯æ¸…å• JSONï¼ˆç”¨äºåç»­å†²çªè§£å†³/è°ƒè¯•ï¼‰
    pub cloud_manifest_json: Option<String>,
}

/// æ•°æ®åº“å†²çªå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct DatabaseConflictResponse {
    /// æ•°æ®åº“åç§°
    pub database_name: String,
    /// å†²çªç±»å‹
    pub conflict_type: String,
    /// æœ¬åœ°æ•°æ®ç‰ˆæœ¬
    pub local_version: Option<u64>,
    /// äº‘ç«¯æ•°æ®ç‰ˆæœ¬
    pub cloud_version: Option<u64>,
    /// æœ¬åœ° Schema ç‰ˆæœ¬
    pub local_schema_version: Option<u32>,
    /// äº‘ç«¯ Schema ç‰ˆæœ¬
    pub cloud_schema_version: Option<u32>,
}

/// åº”ç”¨åˆå¹¶ç­–ç•¥è§£å†³å†²çª
///
/// æ ¹æ®æŒ‡å®šçš„åˆå¹¶ç­–ç•¥å¤„ç†æ‰€æœ‰æ£€æµ‹åˆ°çš„å†²çªã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `strategy`: åˆå¹¶ç­–ç•¥ ("keep_local", "use_cloud", "keep_latest")
/// - `cloud_manifest_json`: äº‘ç«¯åŒæ­¥æ¸…å•çš„ JSON å­—ç¬¦ä¸²
///
/// ## è¿”å›
/// - `SyncResultResponse`: åŒæ­¥ç»“æœ
#[tauri::command]
pub async fn data_governance_resolve_conflicts(
    app: tauri::AppHandle,
    strategy: String,
    cloud_manifest_json: String,
) -> Result<SyncResultResponse, String> {
    info!("[data_governance] å¼€å§‹è§£å†³å†²çªï¼Œç­–ç•¥: {}", strategy);

    // P0-6: ç»´æŠ¤æ¨¡å¼æ£€æŸ¥â€”â€”ç¦æ­¢åœ¨å¤‡ä»½/æ¢å¤/è¿ç§»æœŸé—´è®¿é—®æ•°æ®åº“æ–‡ä»¶
    check_maintenance_mode(&app)?;

    let start = Instant::now();

    // è§£æåˆå¹¶ç­–ç•¥
    let merge_strategy = match strategy.as_str() {
        "keep_local" => MergeStrategy::KeepLocal,
        "use_cloud" => MergeStrategy::UseCloud,
        "keep_latest" => MergeStrategy::KeepLatest,
        "manual" => MergeStrategy::Manual,
        _ => {
            return Err(format!(
                "æœªçŸ¥çš„åˆå¹¶ç­–ç•¥: {}ã€‚å¯é€‰å€¼: keep_local, use_cloud, keep_latest, manual",
                strategy
            ))
        }
    };

    // è§£æäº‘ç«¯æ¸…å•
    let cloud_manifest: SyncManifest = serde_json::from_str(&cloud_manifest_json)
        .map_err(|e| format!("è§£æäº‘ç«¯æ¸…å•å¤±è´¥: {}", e))?;

    let active_dir = get_active_data_dir(&app)?;

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•
    let device_id = get_device_id(&app);
    let manager = SyncManager::new(device_id.clone());
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }
            }
        }
    }

    let local_manifest = manager.create_manifest(local_databases);

    // æ£€æµ‹å†²çª
    let detection_result = SyncManager::detect_conflicts(&local_manifest, &cloud_manifest)
        .map_err(|e| format!("å†²çªæ£€æµ‹å¤±è´¥: {}", e))?;

    // å¦‚æœæ²¡æœ‰å†²çªï¼Œç›´æ¥è¿”å›æˆåŠŸ
    if !detection_result.has_conflicts {
        let duration_ms = start.elapsed().as_millis() as u64;
        info!(
            "[data_governance] æ— å†²çªï¼ŒåŒæ­¥å®Œæˆ: duration={}ms",
            duration_ms
        );

        return Ok(SyncResultResponse {
            success: true,
            strategy: strategy.clone(),
            synced_databases: detection_result.database_conflicts.len(),
            resolved_conflicts: 0,
            pending_manual_conflicts: 0,
            records_to_push: vec![],
            records_to_pull: vec![],
            duration_ms,
            error_message: None,
        });
    }

    // åº”ç”¨åˆå¹¶ç­–ç•¥å¤„ç†è®°å½•çº§å†²çª
    let merge_result =
        SyncManager::apply_merge_strategy(merge_strategy, &detection_result.record_conflicts)
            .map_err(|e| format!("åº”ç”¨åˆå¹¶ç­–ç•¥å¤±è´¥: {}", e))?;

    let duration_ms = start.elapsed().as_millis() as u64;

    info!(
        "[data_governance] å†²çªè§£å†³å®Œæˆ: kept_local={}, used_cloud={}, to_push={}, to_pull={}, duration={}ms",
        merge_result.kept_local,
        merge_result.used_cloud,
        merge_result.records_to_push.len(),
        merge_result.records_to_pull.len(),
        duration_ms
    );

    Ok(SyncResultResponse {
        success: merge_result.success,
        strategy,
        synced_databases: detection_result.database_conflicts.len(),
        resolved_conflicts: merge_result.kept_local + merge_result.used_cloud,
        pending_manual_conflicts: if merge_strategy == MergeStrategy::Manual {
            detection_result.record_conflicts.len()
        } else {
            0
        },
        records_to_push: merge_result.records_to_push,
        records_to_pull: merge_result.records_to_pull,
        duration_ms,
        error_message: if merge_result.errors.is_empty() {
            None
        } else {
            Some(merge_result.errors.join("; "))
        },
    })
}

/// åŒæ­¥ç»“æœå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SyncResultResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// ä½¿ç”¨çš„åˆå¹¶ç­–ç•¥
    pub strategy: String,
    /// åŒæ­¥çš„æ•°æ®åº“æ•°é‡
    pub synced_databases: usize,
    /// è§£å†³çš„å†²çªæ•°é‡
    pub resolved_conflicts: usize,
    /// å¾…æ‰‹åŠ¨å¤„ç†çš„å†²çªæ•°é‡
    pub pending_manual_conflicts: usize,
    /// éœ€è¦æ¨é€åˆ°äº‘ç«¯çš„è®°å½• ID åˆ—è¡¨
    pub records_to_push: Vec<String>,
    /// éœ€è¦ä»äº‘ç«¯æ‹‰å–çš„è®°å½• ID åˆ—è¡¨
    pub records_to_pull: Vec<String>,
    /// æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub error_message: Option<String>,
}

// ==================== äº‘å­˜å‚¨åŒæ­¥æ‰§è¡Œå‘½ä»¤ ====================

/// æ‰§è¡ŒåŒæ­¥
///
/// ä½¿ç”¨äº‘å­˜å‚¨æ‰§è¡Œå®é™…çš„åŒæ­¥æ“ä½œã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `direction`: åŒæ­¥æ–¹å‘ ("upload", "download", "bidirectional")
/// - `cloud_config`: äº‘å­˜å‚¨é…ç½®ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®æˆ–è¿”å›é”™è¯¯ï¼‰
/// - `strategy`: å†²çªåˆå¹¶ç­–ç•¥ ("keep_local", "use_cloud", "keep_latest")ï¼Œé»˜è®¤ä¸º "keep_latest"
///
/// ## è¿”å›
/// - `SyncExecutionResponse`: åŒæ­¥æ‰§è¡Œç»“æœ
#[tauri::command]
pub async fn data_governance_run_sync(
    app: tauri::AppHandle,
    direction: String,
    cloud_config: Option<CloudStorageConfig>,
    strategy: Option<String>,
) -> Result<SyncExecutionResponse, String> {
    info!(
        "[data_governance] å¼€å§‹æ‰§è¡ŒåŒæ­¥: direction={}, strategy={:?}",
        direction, strategy
    );

    // P0-6: ç»´æŠ¤æ¨¡å¼æ£€æŸ¥â€”â€”ç¦æ­¢åœ¨å¤‡ä»½/æ¢å¤/è¿ç§»æœŸé—´è®¿é—®æ•°æ®åº“æ–‡ä»¶
    check_maintenance_mode(&app)?;

    let start = Instant::now();

    // è§£æåŒæ­¥æ–¹å‘
    let sync_direction = SyncDirection::from_str(&direction).ok_or_else(|| {
        format!(
            "æ— æ•ˆçš„åŒæ­¥æ–¹å‘: {}ã€‚å¯é€‰å€¼: upload, download, bidirectional",
            direction
        )
    })?;

    // è§£æåˆå¹¶ç­–ç•¥
    let merge_strategy = match strategy.as_deref().unwrap_or("keep_latest") {
        "keep_local" => MergeStrategy::KeepLocal,
        "use_cloud" => MergeStrategy::UseCloud,
        "keep_latest" => MergeStrategy::KeepLatest,
        "manual" => MergeStrategy::Manual,
        s => {
            return Err(format!(
                "æ— æ•ˆçš„åˆå¹¶ç­–ç•¥: {}ã€‚å¯é€‰å€¼: keep_local, use_cloud, keep_latest, manual",
                s
            ))
        }
    };

    // è·å–äº‘å­˜å‚¨é…ç½®
    let config = match cloud_config {
        Some(cfg) => cfg,
        None => {
            // TODO: ä»åº”ç”¨é…ç½®æˆ–çŠ¶æ€ä¸­è·å–é»˜è®¤äº‘å­˜å‚¨é…ç½®
            return Err("æœªæä¾›äº‘å­˜å‚¨é…ç½®ã€‚è¯·åœ¨è°ƒç”¨å‰é…ç½®äº‘å­˜å‚¨ã€‚".to_string());
        }
    };

    // è·å–è®¾å¤‡ IDï¼ˆç”¨äºå®¡è®¡ä¸åŒæ­¥æ¸…å•ï¼‰
    let device_id = get_device_id(&app);

    #[cfg(feature = "data_governance")]
    {
        let audit_direction = match sync_direction {
            SyncDirection::Upload => super::audit::SyncDirection::Upload,
            SyncDirection::Download => super::audit::SyncDirection::Download,
            SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
        };

        // æ³¨æ„ï¼šå®¡è®¡ details ä¸åº”åŒ…å«æ•æ„Ÿå‡­æ®
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Sync {
                    direction: audit_direction,
                    records_affected: 0,
                },
                format!("cloud_sync/{}", sync_direction.as_str()),
            )
            .with_details(serde_json::json!({
                "device_id": device_id.clone(),
                "direction": direction.clone(),
                "strategy": strategy.as_deref().unwrap_or("keep_latest"),
                "provider": format!("{:?}", config.provider),
                "root": config.root.clone(),
            })),
        );
    }

    // P1-4: å…¨å±€äº’æ–¥ï¼ˆå¸¦è¶…æ—¶ï¼‰ï¼šé¿å…ä¸å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘ï¼Œé™ä½ä¸€è‡´æ€§é£é™©
    let _permit = tokio::time::timeout(
        std::time::Duration::from_secs(SYNC_LOCK_TIMEOUT_SECS),
        BACKUP_GLOBAL_LIMITER.clone().acquire_owned(),
    )
    .await
    .map_err(|_| {
        format!(
            "ç­‰å¾…å…¨å±€æ•°æ®æ²»ç†é”è¶…æ—¶ï¼ˆ{}ç§’ï¼‰ï¼Œå¯èƒ½æœ‰å…¶ä»–æ•°æ®æ²»ç†æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œè¯·ç¨åå†è¯•ã€‚",
            SYNC_LOCK_TIMEOUT_SECS
        )
    })?
    .map_err(|_| "è·å–å…¨å±€æ•°æ®æ²»ç†é”å¤±è´¥".to_string())?;

    // åˆ›å»ºäº‘å­˜å‚¨å®ä¾‹
    let storage = create_storage(&config)
        .await
        .map_err(|e| format!("åˆ›å»ºäº‘å­˜å‚¨å¤±è´¥: {}", e))?;

    let active_dir = get_active_data_dir(&app)?;

    // åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
    let manager = SyncManager::new(device_id.clone());

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•ï¼ˆéå†æ‰€æœ‰æ²»ç†æ•°æ®åº“ï¼‰
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }
            }
        }
    }

    let local_manifest = manager.create_manifest(local_databases);

    // éå†æ‰€æœ‰æ•°æ®åº“ï¼Œæ”¶é›†å¾…åŒæ­¥å˜æ›´å¹¶ç”¨ enrich_changes_with_data è¡¥å…¨å®Œæ•´è®°å½•æ•°æ®
    let mut all_enriched: Vec<SyncChangeWithData> = Vec::new();
    let mut all_change_ids: Vec<i64> = Vec::new();
    let mut db_found = false;

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);
        if !db_path.exists() {
            continue;
        }
        db_found = true;

        let conn = rusqlite::Connection::open(&db_path)
            .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“ {} å¤±è´¥: {}", db_id.as_str(), e))?;

        // æ£€æŸ¥ __change_log è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='__change_log')",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false);

        if !table_exists {
            continue;
        }

        let pending = SyncManager::get_pending_changes(&conn, None, None)
            .map_err(|e| format!("è·å–æ•°æ®åº“ {} å¾…åŒæ­¥å˜æ›´å¤±è´¥: {}", db_id.as_str(), e))?;

        if pending.has_changes() {
            // ä½¿ç”¨ enrich_changes_with_data è¡¥å…¨å®Œæ•´è®°å½•æ•°æ®ï¼ˆINSERT/UPDATE åŒ…å«çœŸå®è¡Œå†…å®¹ï¼‰
            let mut enriched = SyncManager::enrich_changes_with_data(&conn, &pending.entries, None)
                .map_err(|e| format!("è¡¥å…¨æ•°æ®åº“ {} å˜æ›´æ•°æ®å¤±è´¥: {}", db_id.as_str(), e))?;

            // ä¸ºæ¯æ¡å˜æ›´æ ‡æ³¨æ¥æºæ•°æ®åº“åç§°ï¼Œä¸‹è½½å›æ”¾æ—¶æŒ‰åº“è·¯ç”±
            for change in &mut enriched {
                change.database_name = Some(db_id.as_str().to_string());
            }

            all_change_ids.extend(pending.get_change_ids());
            all_enriched.extend(enriched);
        }
    }

    if !db_found {
        return Err("æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®åº“ã€‚è¯·å…ˆåˆå§‹åŒ–æ•°æ®åº“ã€‚".to_string());
    }

    // æ„å»ºå¸¦å®Œæ•´æ•°æ®çš„ PendingChanges ç”¨äºä¸Šä¼ 
    let enriched_pending = PendingChanges::from_entries(
        all_enriched
            .iter()
            .map(|e| ChangeLogEntry {
                id: e.change_log_id.unwrap_or(0),
                table_name: e.table_name.clone(),
                record_id: e.record_id.clone(),
                operation: e.operation,
                changed_at: e.changed_at.clone(),
                sync_version: 0,
            })
            .collect(),
    );

    // æ‰§è¡ŒåŒæ­¥ï¼ˆå¼‚æ­¥æ“ä½œï¼‰ï¼Œè¿”å› (ç»“æœ, è·³è¿‡æ•°é‡)
    let result: Result<(SyncExecutionResult, usize), String> = match sync_direction {
        SyncDirection::Upload => {
            // ä¸Šä¼ å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´
            manager
                .upload_enriched_changes(storage.as_ref(), &all_enriched)
                .await
                .map_err(|e| format!("ä¸Šä¼ åŒæ­¥å¤±è´¥: {}", e))?;

            manager
                .upload_manifest(storage.as_ref(), &local_manifest)
                .await
                .map_err(|e| format!("ä¸Šä¼ æ¸…å•å¤±è´¥: {}", e))?;

            // æ ‡è®°å˜æ›´ä¸ºå·²åŒæ­¥ï¼ˆæŒ‰æ•°æ®åº“åˆ†åˆ«æ ‡è®°ï¼‰
            for db_id in DatabaseId::all_ordered() {
                let db_path = resolve_database_path(&db_id, &active_dir);
                if !db_path.exists() {
                    continue;
                }
                let conn = rusqlite::Connection::open(&db_path)
                    .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“å¤±è´¥: {}", e))?;
                let db_change_ids: Vec<i64> = all_enriched
                    .iter()
                    .filter(|c| c.database_name.as_deref() == Some(db_id.as_str()))
                    .filter_map(|c| c.change_log_id)
                    .collect();
                if !db_change_ids.is_empty() {
                    SyncManager::mark_synced_with_timestamp(&conn, &db_change_ids)
                        .map_err(|e| format!("æ ‡è®°å˜æ›´å¤±è´¥: {}", e))?;
                }
            }

            Ok((
                SyncExecutionResult {
                    success: true,
                    direction: SyncDirection::Upload,
                    changes_uploaded: all_enriched.len(),
                    changes_downloaded: 0,
                    conflicts_detected: 0,
                    duration_ms: start.elapsed().as_millis() as u64,
                    error_message: None,
                },
                0,
            ))
        }
        SyncDirection::Download => {
            let (exec_result, downloaded_changes) = manager
                .execute_download(storage.as_ref(), &local_manifest, merge_strategy)
                .await
                .map_err(|e| format!("ä¸‹è½½åŒæ­¥å¤±è´¥: {}", e))?;

            // ä¸‹è½½çš„å˜æ›´å·²åŒ…å«å®Œæ•´æ•°æ®ï¼ŒæŒ‰æ¥æºæ•°æ®åº“è·¯ç”±å¹¶åº”ç”¨
            let mut exec_result = exec_result;
            let mut total_skipped = 0usize;
            if !downloaded_changes.is_empty() {
                let apply_agg =
                    apply_downloaded_changes_to_databases(&downloaded_changes, &active_dir)?;
                total_skipped = apply_agg.total_skipped;
                if total_skipped > 0 {
                    warn!(
                        "[data_governance] åŒæ­¥å®Œæˆä½†æœ‰ {} æ¡å˜æ›´è¢«è·³è¿‡ï¼ˆæ—§æ ¼å¼æ•°æ®ç¼ºå¤±ï¼‰ï¼Œå»ºè®®åœ¨æºè®¾å¤‡é‡æ–°æ‰§è¡Œå®Œæ•´åŒæ­¥",
                        total_skipped
                    );
                    exec_result.error_message = Some(format!(
                        "åŒæ­¥å·²å®Œæˆï¼Œä½†æœ‰ {} æ¡å˜æ›´å› æ•°æ®ä¸å®Œæ•´è¢«è·³è¿‡ã€‚å»ºè®®åœ¨æºè®¾å¤‡é‡æ–°æ‰§è¡Œå®Œæ•´åŒæ­¥ä»¥è¡¥å…¨æ•°æ®ã€‚",
                        total_skipped
                    ));
                }
            }

            Ok((exec_result, total_skipped))
        }
        SyncDirection::Bidirectional => {
            // execute_bidirectional åªè´Ÿè´£ä¸‹è½½ï¼Œä¸Šä¼ ç”±æ­¤å¤„ç»Ÿä¸€æ‰§è¡Œ
            let (exec_result, change_ids, downloaded_changes) = manager
                .execute_bidirectional(
                    storage.as_ref(),
                    &enriched_pending,
                    &local_manifest,
                    merge_strategy,
                )
                .await
                .map_err(|e| format!("åŒå‘åŒæ­¥å¤±è´¥: {}", e))?;

            // ä¸Šä¼ å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´ï¼ˆå”¯ä¸€ä¸Šä¼ ç‚¹ï¼Œé¿å…é‡å¤ï¼‰
            if !all_enriched.is_empty() {
                manager
                    .upload_enriched_changes(storage.as_ref(), &all_enriched)
                    .await
                    .map_err(|e| format!("ä¸Šä¼ å˜æ›´å¤±è´¥: {}", e))?;
            }
            manager
                .upload_manifest(storage.as_ref(), &local_manifest)
                .await
                .map_err(|e| format!("ä¸Šä¼ æ¸…å•å¤±è´¥: {}", e))?;

            // åº”ç”¨ä¸‹è½½çš„å˜æ›´ï¼ˆå·²åŒ…å«å®Œæ•´æ•°æ®ï¼Œç›´æ¥æŒ‰åº“è·¯ç”±ï¼‰
            let mut exec_result = exec_result;
            let mut total_skipped = 0usize;
            if !downloaded_changes.is_empty() {
                let apply_agg =
                    apply_downloaded_changes_to_databases(&downloaded_changes, &active_dir)?;
                total_skipped = apply_agg.total_skipped;
                if total_skipped > 0 {
                    warn!(
                        "[data_governance] åŒå‘åŒæ­¥å®Œæˆä½†æœ‰ {} æ¡å˜æ›´è¢«è·³è¿‡ï¼ˆæ—§æ ¼å¼æ•°æ®ç¼ºå¤±ï¼‰",
                        total_skipped
                    );
                    exec_result.error_message = Some(format!(
                        "åŒæ­¥å·²å®Œæˆï¼Œä½†æœ‰ {} æ¡å˜æ›´å› æ•°æ®ä¸å®Œæ•´è¢«è·³è¿‡ã€‚å»ºè®®åœ¨æºè®¾å¤‡é‡æ–°æ‰§è¡Œå®Œæ•´åŒæ­¥ä»¥è¡¥å…¨æ•°æ®ã€‚",
                        total_skipped
                    ));
                }
            }

            // ä¸‹è½½æˆåŠŸåº”ç”¨åå†æ ‡è®°æœ¬åœ°å˜æ›´å·²åŒæ­¥ï¼Œé¿å…ä¸­æ–­å¯¼è‡´â€œæ ‡è®°æˆåŠŸä½†ä¸‹è½½æœªè½åœ°â€ã€‚
            for db_id in DatabaseId::all_ordered() {
                let db_path = resolve_database_path(&db_id, &active_dir);
                if !db_path.exists() {
                    continue;
                }
                let conn = rusqlite::Connection::open(&db_path)
                    .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“å¤±è´¥: {}", e))?;
                let db_change_ids: Vec<i64> = all_enriched
                    .iter()
                    .filter(|c| c.database_name.as_deref() == Some(db_id.as_str()))
                    .filter_map(|c| c.change_log_id)
                    .collect();
                if !db_change_ids.is_empty() {
                    SyncManager::mark_synced_with_timestamp(&conn, &db_change_ids)
                        .map_err(|e| format!("æ ‡è®°å˜æ›´å¤±è´¥: {}", e))?;
                }
            }

            if !change_ids.is_empty() {
                tracing::debug!(
                    "[data_governance] åŒå‘åŒæ­¥æ ‡è®°å˜æ›´å®Œæˆ: {} æ¡",
                    change_ids.len()
                );
            }

            Ok((exec_result, total_skipped))
        }
    };

    let duration_ms = start.elapsed().as_millis() as u64;

    match result {
        Ok((exec_result, skipped)) => {
            info!(
                "[data_governance] åŒæ­¥å®Œæˆ: direction={}, uploaded={}, downloaded={}, conflicts={}, skipped={}, duration={}ms",
                exec_result.direction.as_str(),
                exec_result.changes_uploaded,
                exec_result.changes_downloaded,
                exec_result.conflicts_detected,
                skipped,
                exec_result.duration_ms
            );

            #[cfg(feature = "data_governance")]
            {
                let audit_direction = match exec_result.direction {
                    SyncDirection::Upload => super::audit::SyncDirection::Upload,
                    SyncDirection::Download => super::audit::SyncDirection::Download,
                    SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
                };
                let records_affected =
                    exec_result.changes_uploaded + exec_result.changes_downloaded;
                let base_log = AuditLog::new(
                    AuditOperation::Sync {
                        direction: audit_direction,
                        records_affected,
                    },
                    format!("cloud_sync/{}", exec_result.direction.as_str()),
                )
                .with_details(serde_json::json!({
                    "device_id": device_id.clone(),
                    "direction": exec_result.direction.as_str(),
                    "strategy": strategy.clone().unwrap_or_else(|| "keep_latest".to_string()),
                    "changes_uploaded": exec_result.changes_uploaded,
                    "changes_downloaded": exec_result.changes_downloaded,
                    "conflicts_detected": exec_result.conflicts_detected,
                }));

                if exec_result.success {
                    try_save_audit_log(&app, base_log.complete(exec_result.duration_ms));
                } else {
                    try_save_audit_log(
                        &app,
                        base_log.fail(
                            exec_result
                                .error_message
                                .clone()
                                .unwrap_or_else(|| "sync failed".to_string()),
                        ),
                    );
                }
            }

            Ok(SyncExecutionResponse {
                success: exec_result.success,
                direction: exec_result.direction.as_str().to_string(),
                changes_uploaded: exec_result.changes_uploaded,
                changes_downloaded: exec_result.changes_downloaded,
                conflicts_detected: exec_result.conflicts_detected,
                duration_ms: exec_result.duration_ms,
                device_id,
                error_message: exec_result.error_message.clone(),
                skipped_changes: skipped,
            })
        }
        Err(e) => {
            error!("[data_governance] åŒæ­¥å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                let audit_direction = match sync_direction {
                    SyncDirection::Upload => super::audit::SyncDirection::Upload,
                    SyncDirection::Download => super::audit::SyncDirection::Download,
                    SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
                };
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Sync {
                            direction: audit_direction,
                            records_affected: 0,
                        },
                        format!("cloud_sync/{}", sync_direction.as_str()),
                    )
                    .fail(e.to_string())
                    .with_details(serde_json::json!({
                        "device_id": device_id.clone(),
                        "direction": sync_direction.as_str(),
                        "strategy": strategy.clone().unwrap_or_else(|| "keep_latest".to_string()),
                    })),
                );
            }
            Ok(SyncExecutionResponse {
                success: false,
                direction: sync_direction.as_str().to_string(),
                changes_uploaded: 0,
                changes_downloaded: 0,
                conflicts_detected: 0,
                duration_ms,
                device_id,
                error_message: Some(e),
                skipped_changes: 0,
            })
        }
    }
}

/// åŒæ­¥æ‰§è¡Œå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SyncExecutionResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// åŒæ­¥æ–¹å‘
    pub direction: String,
    /// ä¸Šä¼ çš„å˜æ›´æ•°é‡
    pub changes_uploaded: usize,
    /// ä¸‹è½½çš„å˜æ›´æ•°é‡
    pub changes_downloaded: usize,
    /// æ£€æµ‹åˆ°çš„å†²çªæ•°é‡
    pub conflicts_detected: usize,
    /// æ‰§è¡Œè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// è®¾å¤‡ ID
    pub device_id: String,
    /// é”™è¯¯/è­¦å‘Šä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub error_message: Option<String>,
    /// è¢«è·³è¿‡çš„å˜æ›´æ•°é‡ï¼ˆå¦‚æ—§æ ¼å¼æ•°æ®ä¸å®Œæ•´ï¼‰
    /// å‰ç«¯å¯æ®æ­¤å±•ç¤º"éƒ¨åˆ†å®Œæˆ"çŠ¶æ€è€Œéçº¯æˆåŠŸ
    #[serde(default)]
    pub skipped_changes: usize,
}

/// å¯¼å‡ºåŒæ­¥æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶
///
/// å°†åŒæ­¥æ¸…å•å’Œå˜æ›´æ•°æ®å¯¼å‡ºä¸º JSON æ–‡ä»¶ï¼Œç”¨äºæ‰‹åŠ¨åŒæ­¥æˆ–è°ƒè¯•ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `output_path`: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºåº”ç”¨æ•°æ®ç›®å½•ä¸‹çš„ sync_export.jsonï¼‰
///
/// ## è¿”å›
/// - `SyncExportResponse`: å¯¼å‡ºç»“æœ
#[tauri::command]
pub async fn data_governance_export_sync_data(
    app: tauri::AppHandle,
    output_path: Option<String>,
) -> Result<SyncExportResponse, String> {
    info!("[data_governance] å¯¼å‡ºåŒæ­¥æ•°æ®");

    let active_dir = get_active_data_dir(&app)?;
    let app_data_dir = get_app_data_dir(&app)?;

    // è·å–è®¾å¤‡ ID
    let device_id = get_device_id(&app);

    // åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
    let manager = SyncManager::new(device_id.clone());

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•ï¼ˆä½¿ç”¨å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´ï¼‰
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();
    let mut all_enriched_changes: Vec<SyncChangeWithData> = Vec::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                // è·å–æ•°æ®åº“çŠ¶æ€
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }

                // è·å–å¾…åŒæ­¥å˜æ›´å¹¶è¡¥å…¨å®Œæ•´æ•°æ®
                if let Ok(pending) = SyncManager::get_pending_changes(&conn, None, None) {
                    if pending.has_changes() {
                        match SyncManager::enrich_changes_with_data(&conn, &pending.entries, None) {
                            Ok(mut enriched) => {
                                for change in &mut enriched {
                                    change.database_name = Some(db_id.as_str().to_string());
                                }
                                all_enriched_changes.extend(enriched);
                            }
                            Err(e) => {
                                warn!(
                                    "[data_governance] è¡¥å…¨æ•°æ®åº“ {} å˜æ›´æ•°æ®å¤±è´¥: {}",
                                    db_id.as_str(),
                                    e
                                );
                            }
                        }
                    }
                }
            }
        }
    }

    let manifest = manager.create_manifest(local_databases);

    // æ„å»ºå¯¼å‡ºæ•°æ®ï¼ˆä½¿ç”¨å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´ï¼‰
    let export_data = SyncExportData {
        manifest,
        pending_changes: all_enriched_changes.clone(),
        exported_at: chrono::Utc::now().to_rfc3339(),
    };

    // åºåˆ—åŒ–
    let json = serde_json::to_string_pretty(&export_data)
        .map_err(|e| format!("åºåˆ—åŒ–å¯¼å‡ºæ•°æ®å¤±è´¥: {}", e))?;

    // ç¡®å®šè¾“å‡ºè·¯å¾„
    let output = match output_path {
        Some(p) => {
            let user_path = std::path::PathBuf::from(&p);
            validate_user_path(&user_path, &app_data_dir)?;
            user_path
        }
        None => active_dir.join("sync_export.json"),
    };

    // ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    if let Some(parent) = output.parent() {
        std::fs::create_dir_all(parent).map_err(|e| format!("åˆ›å»ºç›®å½•å¤±è´¥: {}", e))?;
    }

    // å†™å…¥æ–‡ä»¶
    std::fs::write(&output, &json).map_err(|e| format!("å†™å…¥æ–‡ä»¶å¤±è´¥: {}", e))?;

    info!(
        "[data_governance] åŒæ­¥æ•°æ®å·²å¯¼å‡º: path={}, changes={}",
        output.display(),
        all_enriched_changes.len()
    );

    Ok(SyncExportResponse {
        success: true,
        output_path: output.to_string_lossy().to_string(),
        manifest_databases: export_data.manifest.databases.len(),
        pending_changes_count: all_enriched_changes.len(),
    })
}

/// åŒæ­¥å¯¼å‡ºæ•°æ®ï¼ˆv2ï¼šå«å®Œæ•´è®°å½•æ•°æ®ï¼‰
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct SyncExportData {
    /// åŒæ­¥æ¸…å•
    pub manifest: SyncManifest,
    /// å¾…åŒæ­¥çš„å˜æ›´ï¼ˆå«å®Œæ•´è®°å½•æ•°æ®ï¼Œæ”¯æŒè·¨è®¾å¤‡å›æ”¾ï¼‰
    pub pending_changes: Vec<SyncChangeWithData>,
    /// å¯¼å‡ºæ—¶é—´
    pub exported_at: String,
}

/// åŒæ­¥å¯¼å‡ºå“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SyncExportResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// è¾“å‡ºæ–‡ä»¶è·¯å¾„
    pub output_path: String,
    /// æ¸…å•ä¸­çš„æ•°æ®åº“æ•°é‡
    pub manifest_databases: usize,
    /// å¾…åŒæ­¥å˜æ›´æ•°é‡
    pub pending_changes_count: usize,
}

/// ä»æœ¬åœ°æ–‡ä»¶å¯¼å…¥åŒæ­¥æ•°æ®
///
/// ä» JSON æ–‡ä»¶å¯¼å…¥åŒæ­¥æ¸…å•å’Œå˜æ›´æ•°æ®ï¼Œç”¨äºæ‰‹åŠ¨åŒæ­¥æˆ–æ¢å¤ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `input_path`: è¾“å…¥æ–‡ä»¶è·¯å¾„
/// - `strategy`: å†²çªåˆå¹¶ç­–ç•¥
///
/// ## è¿”å›
/// - `SyncImportResponse`: å¯¼å…¥ç»“æœ
#[tauri::command]
pub async fn data_governance_import_sync_data(
    app: tauri::AppHandle,
    input_path: String,
    strategy: Option<String>,
) -> Result<SyncImportResponse, String> {
    info!("[data_governance] å¯¼å…¥åŒæ­¥æ•°æ®: path={}", input_path);

    let app_data_dir = get_app_data_dir(&app)?;
    let active_dir = get_active_data_dir(&app)?;

    // éªŒè¯è¾“å…¥è·¯å¾„åœ¨å®‰å…¨èŒƒå›´å†…
    let input_file = std::path::PathBuf::from(&input_path);
    validate_user_path(&input_file, &app_data_dir)?;

    // è¯»å–æ–‡ä»¶
    let json = std::fs::read_to_string(&input_path).map_err(|e| format!("è¯»å–æ–‡ä»¶å¤±è´¥: {}", e))?;

    // è§£æï¼ˆv2 æ ¼å¼å«å®Œæ•´æ•°æ®ï¼‰
    let import_data: SyncExportData =
        serde_json::from_str(&json).map_err(|e| format!("è§£æå¯¼å…¥æ•°æ®å¤±è´¥: {}", e))?;

    // åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
    let device_id = get_device_id(&app);
    let manager = SyncManager::new(device_id.clone());

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }
            }
        }
    }

    let local_manifest = manager.create_manifest(local_databases);

    // æ£€æµ‹å†²çª
    let detection = SyncManager::detect_conflicts(&local_manifest, &import_data.manifest)
        .map_err(|e| format!("å†²çªæ£€æµ‹å¤±è´¥: {}", e))?;

    // è§£æåˆå¹¶ç­–ç•¥
    let merge_strategy = match strategy.as_deref().unwrap_or("keep_latest") {
        "keep_local" => MergeStrategy::KeepLocal,
        "use_cloud" => MergeStrategy::UseCloud,
        "keep_latest" => MergeStrategy::KeepLatest,
        "manual" => MergeStrategy::Manual,
        s => {
            return Err(format!(
                "æ— æ•ˆçš„åˆå¹¶ç­–ç•¥: {}ã€‚å¯é€‰å€¼: keep_local, use_cloud, keep_latest, manual",
                s
            ))
        }
    };

    // å¦‚æœæœ‰å†²çªä¸”æ˜¯æ‰‹åŠ¨æ¨¡å¼
    if detection.has_conflicts && merge_strategy == MergeStrategy::Manual {
        return Ok(SyncImportResponse {
            success: false,
            imported_changes: 0,
            conflicts_detected: detection.total_conflicts(),
            needs_manual_resolution: true,
            error_message: Some(
                "å­˜åœ¨å†²çªï¼Œéœ€è¦æ‰‹åŠ¨è§£å†³ã€‚è¯·å‰å¾€ã€ŒåŒæ­¥ã€é¢æ¿é€‰æ‹©åˆé€‚çš„è§£å†³ç­–ç•¥".to_string(),
            ),
        });
    }

    // åº”ç”¨å˜æ›´åˆ°æœ¬åœ°æ•°æ®åº“ï¼ˆv2 æ ¼å¼å·²å«å®Œæ•´æ•°æ®ï¼ŒæŒ‰æ•°æ®åº“è·¯ç”±ï¼‰
    let mut total_applied = 0usize;
    let mut total_skipped = 0usize;
    let total_failed = 0usize;

    if !import_data.pending_changes.is_empty() {
        // å¯¼å…¥çš„å˜æ›´å·²å«å®Œæ•´è®°å½•æ•°æ®ï¼Œç›´æ¥æŒ‰æ•°æ®åº“è·¯ç”±å¹¶åº”ç”¨
        match apply_downloaded_changes_to_databases(&import_data.pending_changes, &active_dir) {
            Ok(apply_agg) => {
                total_applied = apply_agg.total_success;
                total_skipped = apply_agg.total_skipped;
                info!(
                    "[data_governance] å¯¼å…¥å˜æ›´åº”ç”¨å®Œæˆ: applied={}, skipped={}",
                    total_applied, total_skipped
                );
            }
            Err(e) => {
                error!("[data_governance] åº”ç”¨å¯¼å…¥å˜æ›´å¤±è´¥: {}", e);
                return Err(format!(
                    "åº”ç”¨å¯¼å…¥å˜æ›´å¤±è´¥: {}ã€‚è¯·æ£€æŸ¥å¯¼å…¥æ–‡ä»¶å®Œæ•´æ€§åé‡è¯•",
                    e
                ));
            }
        }
    }

    info!(
        "[data_governance] åŒæ­¥æ•°æ®å¯¼å…¥å®Œæˆ: applied={}, failed={}, conflicts={}",
        total_applied,
        total_failed,
        detection.total_conflicts()
    );

    let error_message = if total_failed > 0 {
        Some(format!("{}æ¡å˜æ›´åº”ç”¨å¤±è´¥", total_failed))
    } else if total_skipped > 0 {
        Some(format!(
            "å¯¼å…¥å·²å®Œæˆï¼Œä½†æœ‰ {} æ¡å˜æ›´å› æ•°æ®ä¸å®Œæ•´è¢«è·³è¿‡ã€‚å»ºè®®åœ¨æºè®¾å¤‡é‡æ–°å¯¼å‡ºå®Œæ•´åŒæ­¥æ•°æ®ã€‚",
            total_skipped
        ))
    } else {
        None
    };

    Ok(SyncImportResponse {
        success: total_failed == 0,
        imported_changes: total_applied,
        conflicts_detected: detection.total_conflicts(),
        needs_manual_resolution: false,
        error_message,
    })
}

/// åŒæ­¥å¯¼å…¥å“åº”
#[derive(Debug, Clone, serde::Serialize)]
pub struct SyncImportResponse {
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// å¯¼å…¥çš„å˜æ›´æ•°é‡
    pub imported_changes: usize,
    /// æ£€æµ‹åˆ°çš„å†²çªæ•°é‡
    pub conflicts_detected: usize,
    /// æ˜¯å¦éœ€è¦æ‰‹åŠ¨è§£å†³å†²çª
    pub needs_manual_resolution: bool,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub error_message: Option<String>,
}

// ==================== å¸¦è¿›åº¦å›è°ƒçš„åŒæ­¥å‘½ä»¤ ====================

use super::sync::{OptionalEmitter, SyncPhase, SyncProgress, SyncProgressEmitter};

/// æ‰§è¡Œå¸¦è¿›åº¦å›è°ƒçš„åŒæ­¥
///
/// ä¸ `data_governance_run_sync` ç±»ä¼¼ï¼Œä½†ä¼šé€šè¿‡äº‹ä»¶é€šé“å‘é€è¿›åº¦æ›´æ–°ã€‚
/// å‰ç«¯å¯ä»¥ç›‘å¬ `data-governance-sync-progress` äº‹ä»¶è·å–å®æ—¶è¿›åº¦ã€‚
///
/// ## å‚æ•°
/// - `app`: Tauri AppHandle
/// - `direction`: åŒæ­¥æ–¹å‘ ("upload", "download", "bidirectional")
/// - `cloud_config`: äº‘å­˜å‚¨é…ç½®ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®æˆ–è¿”å›é”™è¯¯ï¼‰
/// - `strategy`: å†²çªåˆå¹¶ç­–ç•¥ ("keep_local", "use_cloud", "keep_latest")ï¼Œé»˜è®¤ä¸º "keep_latest"
///
/// ## è¿›åº¦äº‹ä»¶
/// å‰ç«¯å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç›‘å¬è¿›åº¦ï¼š
/// ```javascript
/// import { listen } from '@tauri-apps/api/event';
///
/// const unlisten = await listen('data-governance-sync-progress', (event) => {
///   const progress = event.payload;
///   console.log(`Phase: ${progress.phase}, Progress: ${progress.percent}%`);
/// });
/// ```
///
/// ## è¿”å›
/// - `SyncExecutionResponse`: åŒæ­¥æ‰§è¡Œç»“æœ
#[tauri::command]
pub async fn data_governance_run_sync_with_progress(
    app: tauri::AppHandle,
    direction: String,
    cloud_config: Option<CloudStorageConfig>,
    strategy: Option<String>,
) -> Result<SyncExecutionResponse, String> {
    info!(
        "[data_governance] å¼€å§‹æ‰§è¡Œå¸¦è¿›åº¦çš„åŒæ­¥: direction={}, strategy={:?}",
        direction, strategy
    );

    // P0-6: ç»´æŠ¤æ¨¡å¼æ£€æŸ¥â€”â€”ç¦æ­¢åœ¨å¤‡ä»½/æ¢å¤/è¿ç§»æœŸé—´è®¿é—®æ•°æ®åº“æ–‡ä»¶
    check_maintenance_mode(&app)?;

    let start = Instant::now();

    // åˆ›å»ºè¿›åº¦å‘å°„å™¨
    let emitter = SyncProgressEmitter::new(app.clone());

    // å‘é€å‡†å¤‡ä¸­çŠ¶æ€
    emitter.emit_preparing().await;

    // è§£æåŒæ­¥æ–¹å‘
    let sync_direction = match SyncDirection::from_str(&direction) {
        Some(d) => d,
        None => {
            let error_msg = format!(
                "æ— æ•ˆçš„åŒæ­¥æ–¹å‘: {}ã€‚å¯é€‰å€¼: upload, download, bidirectional",
                direction
            );
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
    };

    // è§£æåˆå¹¶ç­–ç•¥
    let merge_strategy = match strategy.as_deref().unwrap_or("keep_latest") {
        "keep_local" => MergeStrategy::KeepLocal,
        "use_cloud" => MergeStrategy::UseCloud,
        "keep_latest" => MergeStrategy::KeepLatest,
        "manual" => MergeStrategy::Manual,
        s => {
            let error_msg = format!(
                "æ— æ•ˆçš„åˆå¹¶ç­–ç•¥: {}ã€‚å¯é€‰å€¼: keep_local, use_cloud, keep_latest, manual",
                s
            );
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
    };

    // è·å–äº‘å­˜å‚¨é…ç½®
    let config = match cloud_config {
        Some(cfg) => cfg,
        None => {
            let error_msg = "æœªæä¾›äº‘å­˜å‚¨é…ç½®ã€‚è¯·åœ¨è°ƒç”¨å‰é…ç½®äº‘å­˜å‚¨ã€‚".to_string();
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
    };

    // è·å–è®¾å¤‡ IDï¼ˆç”¨äºå®¡è®¡ä¸åŒæ­¥æ¸…å•ï¼‰
    let device_id = get_device_id(&app);

    #[cfg(feature = "data_governance")]
    {
        let audit_direction = match sync_direction {
            SyncDirection::Upload => super::audit::SyncDirection::Upload,
            SyncDirection::Download => super::audit::SyncDirection::Download,
            SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
        };

        // æ³¨æ„ï¼šå®¡è®¡ details ä¸åº”åŒ…å«æ•æ„Ÿå‡­æ®
        try_save_audit_log(
            &app,
            AuditLog::new(
                AuditOperation::Sync {
                    direction: audit_direction,
                    records_affected: 0,
                },
                format!("cloud_sync/{}", sync_direction.as_str()),
            )
            .with_details(serde_json::json!({
                "device_id": device_id.clone(),
                "direction": direction.clone(),
                "strategy": strategy.as_deref().unwrap_or("keep_latest"),
                "provider": format!("{:?}", config.provider),
                "root": config.root.clone(),
                "with_progress": true,
            })),
        );
    }

    // P1-4: å…¨å±€äº’æ–¥ï¼ˆå¸¦è¶…æ—¶ï¼‰ï¼šé¿å…ä¸å¤‡ä»½/æ¢å¤/ZIP å¯¼å…¥å¯¼å‡ºå¹¶å‘ï¼Œé™ä½ä¸€è‡´æ€§é£é™©
    let _permit = match tokio::time::timeout(
        std::time::Duration::from_secs(SYNC_LOCK_TIMEOUT_SECS),
        BACKUP_GLOBAL_LIMITER.clone().acquire_owned(),
    )
    .await
    {
        Ok(Ok(p)) => p,
        Ok(Err(_)) => {
            let error_msg = "è·å–å…¨å±€æ•°æ®æ²»ç†é”å¤±è´¥".to_string();
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
        Err(_) => {
            let error_msg = format!(
                "ç­‰å¾…å…¨å±€æ•°æ®æ²»ç†é”è¶…æ—¶ï¼ˆ{}ç§’ï¼‰ï¼Œå¯èƒ½æœ‰å…¶ä»–æ•°æ®æ²»ç†æ“ä½œæ­£åœ¨æ‰§è¡Œï¼Œè¯·ç¨åå†è¯•ã€‚",
                SYNC_LOCK_TIMEOUT_SECS
            );
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
    };

    // å‘é€æ£€æµ‹å˜æ›´çŠ¶æ€
    emitter.emit_detecting_changes().await;

    // åˆ›å»ºäº‘å­˜å‚¨å®ä¾‹
    let storage = match create_storage(&config).await {
        Ok(s) => s,
        Err(e) => {
            let error_msg = format!("åˆ›å»ºäº‘å­˜å‚¨å¤±è´¥: {}", e);
            emitter.emit_failed(&error_msg).await;
            return Err(error_msg);
        }
    };

    let active_dir = match get_active_data_dir(&app) {
        Ok(dir) => dir,
        Err(e) => {
            emitter.emit_failed(&e).await;
            return Err(e);
        }
    };

    // åˆ›å»ºåŒæ­¥ç®¡ç†å™¨ï¼ˆå¤ç”¨ä¸Šæ–¹å·²è·å–çš„ device_idï¼‰
    let manager = SyncManager::new(device_id.clone());

    // æ„å»ºæœ¬åœ°åŒæ­¥æ¸…å•ï¼ˆéå†æ‰€æœ‰æ²»ç†æ•°æ®åº“ï¼‰
    let mut local_databases: HashMap<String, DatabaseSyncState> = HashMap::new();

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);

        if db_path.exists() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                if let Ok(state) = SyncManager::get_database_sync_state(&conn, db_id.as_str()) {
                    local_databases.insert(db_id.as_str().to_string(), state);
                }
            }
        }
    }

    let local_manifest = manager.create_manifest(local_databases);

    // éå†æ‰€æœ‰æ•°æ®åº“ï¼Œæ”¶é›†å¾…åŒæ­¥å˜æ›´å¹¶è¡¥å…¨å®Œæ•´è®°å½•æ•°æ®
    let mut all_enriched: Vec<SyncChangeWithData> = Vec::new();
    let mut db_found = false;

    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, &active_dir);
        if !db_path.exists() {
            continue;
        }
        db_found = true;

        let conn = match rusqlite::Connection::open(&db_path) {
            Ok(c) => c,
            Err(e) => {
                let error_msg = format!("æ‰“å¼€æ•°æ®åº“ {} å¤±è´¥: {}", db_id.as_str(), e);
                emitter.emit_failed(&error_msg).await;
                return Err(error_msg);
            }
        };

        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='__change_log')",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false);

        if !table_exists {
            continue;
        }

        match SyncManager::get_pending_changes(&conn, None, None) {
            Ok(pending) if pending.has_changes() => {
                match SyncManager::enrich_changes_with_data(&conn, &pending.entries, None) {
                    Ok(mut enriched) => {
                        for change in &mut enriched {
                            change.database_name = Some(db_id.as_str().to_string());
                        }
                        all_enriched.extend(enriched);
                    }
                    Err(e) => {
                        let error_msg =
                            format!("è¡¥å…¨æ•°æ®åº“ {} å˜æ›´æ•°æ®å¤±è´¥: {}", db_id.as_str(), e);
                        emitter.emit_failed(&error_msg).await;
                        return Err(error_msg);
                    }
                }
            }
            _ => {}
        }
    }

    if !db_found {
        let error_msg = "æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®åº“ã€‚è¯·å…ˆåˆå§‹åŒ–æ•°æ®åº“ã€‚".to_string();
        emitter.emit_failed(&error_msg).await;
        return Err(error_msg);
    }

    // æ„å»º PendingChanges ç”¨äºå…¼å®¹ execute_upload æ¥å£
    let pending = PendingChanges::from_entries(
        all_enriched
            .iter()
            .map(|e| ChangeLogEntry {
                id: e.change_log_id.unwrap_or(0),
                table_name: e.table_name.clone(),
                record_id: e.record_id.clone(),
                operation: e.operation,
                changed_at: e.changed_at.clone(),
                sync_version: 0,
            })
            .collect(),
    );

    // ä½¿ç”¨ OptionalEmitter åŒ…è£…
    let opt_emitter = OptionalEmitter::with_emitter(emitter.clone());

    // æ‰§è¡ŒåŒæ­¥ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
    let result = match sync_direction {
        SyncDirection::Upload => {
            execute_upload_with_progress_v2(
                &manager,
                storage.as_ref(),
                &all_enriched,
                &pending,
                &local_manifest,
                &active_dir,
                &opt_emitter,
            )
            .await
        }
        SyncDirection::Download => {
            execute_download_with_progress_v2(
                &manager,
                storage.as_ref(),
                &local_manifest,
                merge_strategy,
                &active_dir,
                &opt_emitter,
            )
            .await
        }
        SyncDirection::Bidirectional => {
            execute_bidirectional_with_progress_v2(
                &manager,
                storage.as_ref(),
                &all_enriched,
                &pending,
                &local_manifest,
                merge_strategy,
                &active_dir,
                &opt_emitter,
            )
            .await
        }
    };

    let duration_ms = start.elapsed().as_millis() as u64;

    match result {
        Ok((exec_result, skipped)) => {
            // å‘é€å®ŒæˆçŠ¶æ€
            emitter.emit_completed().await;

            info!(
                "[data_governance] å¸¦è¿›åº¦åŒæ­¥å®Œæˆ: direction={}, uploaded={}, downloaded={}, conflicts={}, skipped={}, duration={}ms",
                exec_result.direction.as_str(),
                exec_result.changes_uploaded,
                exec_result.changes_downloaded,
                exec_result.conflicts_detected,
                skipped,
                exec_result.duration_ms
            );

            #[cfg(feature = "data_governance")]
            {
                let audit_direction = match exec_result.direction {
                    SyncDirection::Upload => super::audit::SyncDirection::Upload,
                    SyncDirection::Download => super::audit::SyncDirection::Download,
                    SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
                };
                let records_affected =
                    exec_result.changes_uploaded + exec_result.changes_downloaded;
                let base_log = AuditLog::new(
                    AuditOperation::Sync {
                        direction: audit_direction,
                        records_affected,
                    },
                    format!("cloud_sync/{}", exec_result.direction.as_str()),
                )
                .with_details(serde_json::json!({
                    "device_id": device_id.clone(),
                    "direction": exec_result.direction.as_str(),
                    "strategy": strategy.clone().unwrap_or_else(|| "keep_latest".to_string()),
                    "changes_uploaded": exec_result.changes_uploaded,
                    "changes_downloaded": exec_result.changes_downloaded,
                    "conflicts_detected": exec_result.conflicts_detected,
                    "skipped_changes": skipped,
                    "with_progress": true,
                }));

                if exec_result.success {
                    try_save_audit_log(&app, base_log.complete(exec_result.duration_ms));
                } else {
                    try_save_audit_log(
                        &app,
                        base_log.fail(
                            exec_result
                                .error_message
                                .clone()
                                .unwrap_or_else(|| "sync failed".to_string()),
                        ),
                    );
                }
            }

            Ok(SyncExecutionResponse {
                success: exec_result.success,
                direction: exec_result.direction.as_str().to_string(),
                changes_uploaded: exec_result.changes_uploaded,
                changes_downloaded: exec_result.changes_downloaded,
                conflicts_detected: exec_result.conflicts_detected,
                duration_ms: exec_result.duration_ms,
                device_id,
                error_message: exec_result.error_message.clone(),
                skipped_changes: skipped,
            })
        }
        Err(e) => {
            emitter.emit_failed(&e).await;
            error!("[data_governance] å¸¦è¿›åº¦åŒæ­¥å¤±è´¥: {}", e);
            #[cfg(feature = "data_governance")]
            {
                let audit_direction = match sync_direction {
                    SyncDirection::Upload => super::audit::SyncDirection::Upload,
                    SyncDirection::Download => super::audit::SyncDirection::Download,
                    SyncDirection::Bidirectional => super::audit::SyncDirection::Bidirectional,
                };
                try_save_audit_log(
                    &app,
                    AuditLog::new(
                        AuditOperation::Sync {
                            direction: audit_direction,
                            records_affected: 0,
                        },
                        format!("cloud_sync/{}", sync_direction.as_str()),
                    )
                    .fail(e.to_string())
                    .with_details(serde_json::json!({
                        "device_id": device_id.clone(),
                        "direction": sync_direction.as_str(),
                        "strategy": strategy.clone().unwrap_or_else(|| "keep_latest".to_string()),
                        "with_progress": true,
                    })),
                );
            }
            Ok(SyncExecutionResponse {
                success: false,
                direction: sync_direction.as_str().to_string(),
                changes_uploaded: 0,
                changes_downloaded: 0,
                conflicts_detected: 0,
                duration_ms,
                device_id,
                error_message: Some(e),
                skipped_changes: 0,
            })
        }
    }
}

// ============================================================================
// åŒæ­¥è¿›åº¦è¾…åŠ©å‡½æ•°ï¼ˆå¤šåº“ + å®Œæ•´æ•°æ®è½½è·ï¼‰
// ============================================================================

/// æ‰§è¡Œä¸Šä¼ åŒæ­¥ï¼ˆv2ï¼šå¸¦è¿›åº¦ã€å¤šåº“ã€å®Œæ•´æ•°æ®è½½è·ï¼‰
async fn execute_upload_with_progress_v2(
    manager: &SyncManager,
    storage: &dyn CloudStorage,
    enriched: &[SyncChangeWithData],
    pending: &super::sync::PendingChanges,
    local_manifest: &SyncManifest,
    active_dir: &std::path::Path,
    emitter: &OptionalEmitter,
) -> Result<(SyncExecutionResult, usize), String> {
    let start = std::time::Instant::now();
    let total = enriched.len() as u64;

    if enriched.is_empty() {
        return Ok((
            SyncExecutionResult {
                success: true,
                direction: SyncDirection::Upload,
                changes_uploaded: 0,
                changes_downloaded: 0,
                conflicts_detected: 0,
                duration_ms: start.elapsed().as_millis() as u64,
                error_message: None,
            },
            0,
        ));
    }

    emitter.emit_uploading(0, total, None).await;

    // ä¸Šä¼ å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´
    manager
        .upload_enriched_changes(storage, enriched)
        .await
        .map_err(|e| format!("ä¸Šä¼ åŒæ­¥å¤±è´¥: {}", e))?;

    manager
        .upload_manifest(storage, local_manifest)
        .await
        .map_err(|e| format!("ä¸Šä¼ æ¸…å•å¤±è´¥: {}", e))?;

    emitter.emit_uploading(total, total, None).await;

    // æŒ‰æ•°æ®åº“æ ‡è®°å˜æ›´ä¸ºå·²åŒæ­¥
    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, active_dir);
        if !db_path.exists() {
            continue;
        }

        let db_change_ids: Vec<i64> = enriched
            .iter()
            .filter(|c| c.database_name.as_deref() == Some(db_id.as_str()))
            .filter_map(|c| c.change_log_id)
            .collect();

        if !db_change_ids.is_empty() {
            let conn = rusqlite::Connection::open(&db_path)
                .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“å¤±è´¥: {}", e))?;
            SyncManager::mark_synced_with_timestamp(&conn, &db_change_ids)
                .map_err(|e| format!("æ ‡è®°å˜æ›´å¤±è´¥: {}", e))?;
        }
    }

    emitter.emit_applying(total, total, None).await;

    Ok((
        SyncExecutionResult {
            success: true,
            direction: SyncDirection::Upload,
            changes_uploaded: enriched.len(),
            changes_downloaded: 0,
            conflicts_detected: 0,
            duration_ms: start.elapsed().as_millis() as u64,
            error_message: None,
        },
        0,
    ))
}

/// æ‰§è¡Œä¸‹è½½åŒæ­¥ï¼ˆv2ï¼šå¸¦è¿›åº¦ã€å¤šåº“è·¯ç”±ï¼‰
async fn execute_download_with_progress_v2(
    manager: &SyncManager,
    storage: &dyn CloudStorage,
    local_manifest: &SyncManifest,
    merge_strategy: MergeStrategy,
    active_dir: &std::path::Path,
    emitter: &OptionalEmitter,
) -> Result<(SyncExecutionResult, usize), String> {
    let start = std::time::Instant::now();

    emitter.emit_downloading(0, 0, None).await;

    let (exec_result, downloaded_changes) = manager
        .execute_download(storage, local_manifest, merge_strategy)
        .await
        .map_err(|e| format!("ä¸‹è½½åŒæ­¥å¤±è´¥: {}", e))?;

    let total = downloaded_changes.len() as u64;
    emitter.emit_downloading(total, total, None).await;

    // ä¸‹è½½çš„å˜æ›´å·²å«å®Œæ•´æ•°æ®ï¼ŒæŒ‰æ•°æ®åº“è·¯ç”±å¹¶åº”ç”¨
    let mut exec_result = exec_result;
    let mut total_skipped = 0usize;
    if !downloaded_changes.is_empty() {
        let total_changes = downloaded_changes.len() as u64;
        emitter
            .emit_applying(0, total_changes, Some("åº”ç”¨å˜æ›´".to_string()))
            .await;

        let apply_agg = apply_downloaded_changes_to_databases(&downloaded_changes, active_dir)?;
        total_skipped = apply_agg.total_skipped;
        if total_skipped > 0 {
            exec_result.error_message = Some(format!(
                "åŒæ­¥å·²å®Œæˆï¼Œä½†æœ‰ {} æ¡å˜æ›´å› æ•°æ®ä¸å®Œæ•´è¢«è·³è¿‡ã€‚å»ºè®®åœ¨æºè®¾å¤‡é‡æ–°æ‰§è¡Œå®Œæ•´åŒæ­¥ä»¥è¡¥å…¨æ•°æ®ã€‚",
                total_skipped
            ));
        }

        emitter
            .emit_applying(total_changes, total_changes, None)
            .await;
    }

    Ok((exec_result, total_skipped))
}

/// æ‰§è¡ŒåŒå‘åŒæ­¥ï¼ˆv2ï¼šå¸¦è¿›åº¦ã€å¤šåº“ã€å®Œæ•´æ•°æ®è½½è·ï¼‰
async fn execute_bidirectional_with_progress_v2(
    manager: &SyncManager,
    storage: &dyn CloudStorage,
    enriched: &[SyncChangeWithData],
    pending: &super::sync::PendingChanges,
    local_manifest: &SyncManifest,
    merge_strategy: MergeStrategy,
    active_dir: &std::path::Path,
    emitter: &OptionalEmitter,
) -> Result<(SyncExecutionResult, usize), String> {
    let start = std::time::Instant::now();

    emitter.emit_downloading(0, 0, None).await;

    let (exec_result, change_ids, downloaded_changes) = manager
        .execute_bidirectional(storage, pending, local_manifest, merge_strategy)
        .await
        .map_err(|e| format!("åŒå‘åŒæ­¥å¤±è´¥: {}", e))?;

    // ä¸Šä¼ å¸¦å®Œæ•´æ•°æ®çš„å˜æ›´ï¼ˆå”¯ä¸€ä¸Šä¼ ç‚¹ï¼Œexecute_bidirectional ä¸å†å†…éƒ¨ä¸Šä¼ ï¼‰
    if !enriched.is_empty() {
        let upload_total = enriched.len() as u64;
        emitter.emit_uploading(0, upload_total, None).await;

        manager
            .upload_enriched_changes(storage, enriched)
            .await
            .map_err(|e| format!("ä¸Šä¼ å˜æ›´å¤±è´¥: {}", e))?;

        emitter
            .emit_uploading(upload_total, upload_total, None)
            .await;
    }
    manager
        .upload_manifest(storage, local_manifest)
        .await
        .map_err(|e| format!("ä¸Šä¼ æ¸…å•å¤±è´¥: {}", e))?;

    // åº”ç”¨ä¸‹è½½çš„å˜æ›´ï¼ˆå·²å«å®Œæ•´æ•°æ®ï¼ŒæŒ‰åº“è·¯ç”±ï¼‰
    let mut exec_result = exec_result;
    let mut total_skipped = 0usize;
    if !downloaded_changes.is_empty() {
        let total_changes = downloaded_changes.len() as u64;
        emitter
            .emit_applying(0, total_changes, Some("åº”ç”¨ä¸‹è½½å˜æ›´".to_string()))
            .await;

        let apply_agg = apply_downloaded_changes_to_databases(&downloaded_changes, active_dir)?;
        total_skipped = apply_agg.total_skipped;
        if total_skipped > 0 {
            exec_result.error_message = Some(format!(
                "åŒæ­¥å·²å®Œæˆï¼Œä½†æœ‰ {} æ¡å˜æ›´å› æ•°æ®ä¸å®Œæ•´è¢«è·³è¿‡ã€‚å»ºè®®åœ¨æºè®¾å¤‡é‡æ–°æ‰§è¡Œå®Œæ•´åŒæ­¥ä»¥è¡¥å…¨æ•°æ®ã€‚",
                total_skipped
            ));
        }

        emitter
            .emit_applying(total_changes, total_changes, None)
            .await;
    }

    // ä¸‹è½½æˆåŠŸåº”ç”¨åå†æ ‡è®°æœ¬åœ°å˜æ›´å·²åŒæ­¥ï¼Œé¿å…ä¸­æ–­å¯¼è‡´â€œæ ‡è®°æˆåŠŸä½†ä¸‹è½½æœªè½åœ°â€ã€‚
    for db_id in DatabaseId::all_ordered() {
        let db_path = resolve_database_path(&db_id, active_dir);
        if !db_path.exists() {
            continue;
        }

        let db_change_ids: Vec<i64> = enriched
            .iter()
            .filter(|c| c.database_name.as_deref() == Some(db_id.as_str()))
            .filter_map(|c| c.change_log_id)
            .collect();

        if !db_change_ids.is_empty() {
            let conn = rusqlite::Connection::open(&db_path)
                .map_err(|e| format!("æ‰“å¼€æ•°æ®åº“å¤±è´¥: {}", e))?;
            SyncManager::mark_synced_with_timestamp(&conn, &db_change_ids)
                .map_err(|e| format!("æ ‡è®°å˜æ›´å¤±è´¥: {}", e))?;
        }
    }

    if !change_ids.is_empty() {
        tracing::debug!(
            "[data_governance] åŒå‘åŒæ­¥æ ‡è®°å˜æ›´å®Œæˆ: {} æ¡",
            change_ids.len()
        );
    }

    Ok((exec_result, total_skipped))
}

#[cfg(test)]
mod tests {
    use super::{refresh_schema_registry_from_dir, resolve_target_and_pending, validate_backup_id};
    use crate::data_governance::schema_registry::{DatabaseId, DatabaseStatus, SchemaRegistry};
    use std::sync::{Arc, RwLock};
    use tempfile::TempDir;

    fn create_refinery_history_with_version(db_path: &std::path::Path, version: i32) {
        let conn = rusqlite::Connection::open(db_path).unwrap();
        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                version INTEGER PRIMARY KEY,
                name TEXT,
                applied_on TEXT,
                checksum TEXT
            )",
            [],
        )
        .unwrap();
        conn.execute(
            "INSERT OR REPLACE INTO refinery_schema_history(version, name, applied_on, checksum)
             VALUES (?1, ?2, ?3, ?4)",
            rusqlite::params![
                version,
                format!("V{}_test", version),
                "2026-02-07T00:00:00Z",
                "abc"
            ],
        )
        .unwrap();
    }

    #[test]
    fn resolve_target_and_pending_uses_migration_set_when_status_missing() {
        // Mistakes è¿ç§»é›†ï¼šV20260130, V20260131, V20260201, V20260207, V20260208, V20260209
        // ä» V20260130 å¼€å§‹ï¼Œpending = 5ï¼ˆåç»­ 5 ä¸ªè¿ç§»ï¼‰
        let (target_version, pending_count) =
            resolve_target_and_pending(&DatabaseId::Mistakes, 20260130, None);

        let expected_latest = super::super::migration::MISTAKES_MIGRATIONS.latest_version() as u32;
        let expected_pending = super::super::migration::MISTAKES_MIGRATIONS
            .pending(20260130)
            .count();

        assert_eq!(target_version, expected_latest);
        assert_eq!(pending_count, expected_pending);
    }

    #[test]
    fn resolve_target_and_pending_returns_zero_when_latest_reached() {
        let latest = super::super::migration::MISTAKES_MIGRATIONS.latest_version() as u32;
        let (target_version, pending_count) =
            resolve_target_and_pending(&DatabaseId::Mistakes, latest, None);

        assert_eq!(target_version, latest);
        assert_eq!(pending_count, 0);
    }

    #[test]
    fn resolve_target_and_pending_prefers_status_target_version() {
        let status = DatabaseStatus {
            id: DatabaseId::Mistakes,
            schema_version: 20260130,
            min_compatible_version: 1,
            max_compatible_version: 20260299,
            data_contract_version: "1.0.0".to_string(),
            migration_history: Vec::new(),
            checksum: String::new(),
            updated_at: String::new(),
        };

        let (target_version, pending_count) =
            resolve_target_and_pending(&DatabaseId::Mistakes, 20260130, Some(&status));

        let expected_pending = super::super::migration::MISTAKES_MIGRATIONS
            .pending(20260130)
            .count();

        assert_eq!(target_version, 20260299);
        assert_eq!(pending_count, expected_pending);
    }

    #[test]
    fn validate_backup_id_allows_safe_id() {
        let result = validate_backup_id("backup-20260206_120000");
        assert_eq!(result.unwrap(), "backup-20260206_120000");
    }

    #[test]
    fn validate_backup_id_rejects_parent_traversal() {
        let result = validate_backup_id("../escape");
        assert!(result.is_err());
    }

    #[test]
    fn validate_backup_id_rejects_absolute_path() {
        let result = validate_backup_id("/tmp/escape");
        assert!(result.is_err());
    }

    #[test]
    fn validate_backup_id_rejects_encoded_bypass() {
        let result = validate_backup_id("%2e%2e%2fescape");
        assert!(result.is_err());
    }

    #[test]
    fn refresh_schema_registry_from_dir_swaps_latest_live_state() {
        let temp_dir = TempDir::new().unwrap();
        let app_data_dir = temp_dir.path();
        std::fs::create_dir_all(app_data_dir.join("databases")).unwrap();

        let vfs_db = app_data_dir.join("databases").join("vfs.db");
        create_refinery_history_with_version(&vfs_db, 1);

        let registry_state = Arc::new(RwLock::new(SchemaRegistry::default()));
        let first = refresh_schema_registry_from_dir(app_data_dir, &registry_state).unwrap();
        assert_eq!(
            first.get_status(&DatabaseId::Vfs).map(|s| s.schema_version),
            Some(1)
        );

        create_refinery_history_with_version(&vfs_db, 2);

        let second = refresh_schema_registry_from_dir(app_data_dir, &registry_state).unwrap();
        assert_eq!(
            second
                .get_status(&DatabaseId::Vfs)
                .map(|s| s.schema_version),
            Some(2)
        );

        let guard = registry_state.read().unwrap();
        assert_eq!(
            guard.get_status(&DatabaseId::Vfs).map(|s| s.schema_version),
            Some(2)
        );
    }

    #[test]
    fn refresh_schema_registry_from_dir_maps_poisoned_lock_error() {
        let temp_dir = TempDir::new().unwrap();
        let app_data_dir = temp_dir.path();
        std::fs::create_dir_all(app_data_dir.join("databases")).unwrap();

        let registry_state = Arc::new(RwLock::new(SchemaRegistry::default()));
        let poison_target = registry_state.clone();
        let _ = std::panic::catch_unwind(move || {
            let _guard = poison_target.write().unwrap();
            panic!("poison registry lock");
        });

        let err = refresh_schema_registry_from_dir(app_data_dir, &registry_state).unwrap_err();
        assert!(err.contains("å†™å…¥ SchemaRegistry çŠ¶æ€å¤±è´¥"));
    }

    // ========================================================================
    // infer_database_from_table æµ‹è¯•
    // ========================================================================

    #[test]
    fn test_infer_database_chat_v2_prefix() {
        assert_eq!(
            super::infer_database_from_table("chat_v2_sessions"),
            Some("chat_v2")
        );
        assert_eq!(
            super::infer_database_from_table("chat_v2_messages"),
            Some("chat_v2")
        );
        assert_eq!(
            super::infer_database_from_table("chat_v2_blocks"),
            Some("chat_v2")
        );
    }

    #[test]
    fn test_infer_database_chat_v2_known_tables() {
        assert_eq!(
            super::infer_database_from_table("workspace_index"),
            Some("chat_v2")
        );
        assert_eq!(
            super::infer_database_from_table("sleep_block"),
            Some("chat_v2")
        );
        assert_eq!(
            super::infer_database_from_table("subagent_task"),
            Some("chat_v2")
        );
    }

    #[test]
    fn test_infer_database_resources_ambiguous_returns_none() {
        // resources è¡¨åŒæ—¶å­˜åœ¨äº chat_v2 å’Œ vfsï¼Œlegacy å˜æ›´æ— æ³•åˆ¤å®šï¼Œåº”è·³è¿‡
        assert_eq!(super::infer_database_from_table("resources"), None);
    }

    #[test]
    fn test_infer_database_mistakes() {
        assert_eq!(
            super::infer_database_from_table("mistakes"),
            Some("mistakes")
        );
        assert_eq!(
            super::infer_database_from_table("anki_cards"),
            Some("mistakes")
        );
        assert_eq!(
            super::infer_database_from_table("document_tasks"),
            Some("mistakes")
        );
        assert_eq!(
            super::infer_database_from_table("settings"),
            Some("mistakes")
        );
        assert_eq!(
            super::infer_database_from_table("review_analyses"),
            Some("mistakes")
        );
        assert_eq!(
            super::infer_database_from_table("exam_sheet_sessions"),
            Some("mistakes")
        );
    }

    #[test]
    fn test_infer_database_vfs() {
        assert_eq!(super::infer_database_from_table("notes"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("files"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("folders"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("blobs"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("questions"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("mindmaps"), Some("vfs"));
        assert_eq!(super::infer_database_from_table("essays"), Some("vfs"));
    }

    #[test]
    fn test_infer_database_llm_usage() {
        assert_eq!(
            super::infer_database_from_table("llm_usage_logs"),
            Some("llm_usage")
        );
        assert_eq!(
            super::infer_database_from_table("llm_usage_daily"),
            Some("llm_usage")
        );
    }

    #[test]
    fn test_infer_database_unknown_returns_none() {
        assert_eq!(super::infer_database_from_table("unknown_table_xyz"), None);
        assert_eq!(super::infer_database_from_table("__change_log"), None);
    }

    #[test]
    fn test_infer_database_no_cross_routing() {
        // ç¡®ä¿ mistakes è¡¨ä¸ä¼šè¢«è·¯ç”±åˆ° chat_v2
        assert_ne!(
            super::infer_database_from_table("anki_cards"),
            Some("chat_v2")
        );
        // ç¡®ä¿ vfs è¡¨ä¸ä¼šè¢«è·¯ç”±åˆ° mistakes
        assert_ne!(super::infer_database_from_table("notes"), Some("mistakes"));
    }
}
