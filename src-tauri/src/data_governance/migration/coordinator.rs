//! # Migration Coordinator (è¿ç§»åè°ƒå™¨)
//!
//! ç»Ÿä¸€åè°ƒå¤šä¸ªæ•°æ®åº“çš„è¿ç§»æ‰§è¡Œã€‚
//!
//! ## èŒè´£
//!
//! 1. æ£€æŸ¥æ‰€æœ‰æ•°æ®åº“å½“å‰ç‰ˆæœ¬
//! 2. éªŒè¯è·¨åº“ä¾èµ–å…¼å®¹æ€§
//! 3. æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œè¿ç§»
//! 4. è¿ç§»åéªŒè¯ç»“æœ
//! 5. è®°å½•å®¡è®¡æ—¥å¿—
//! 6. å¤±è´¥æ—¶åè°ƒå›æ»š

use std::collections::HashSet;
use std::path::PathBuf;
use std::sync::{Mutex, OnceLock};
use std::time::Duration;

use rusqlite::OptionalExtension;
use sha2::{Digest, Sha256};

use crate::data_governance::schema_registry::{DatabaseId, SchemaRegistry};

/// è®°å½•å¹¶è·³è¿‡è¿­ä»£ä¸­çš„é”™è¯¯ï¼Œé¿å…é™é»˜ä¸¢å¼ƒ
fn log_and_skip_err<T, E: std::fmt::Display>(result: Result<T, E>) -> Option<T> {
    match result {
        Ok(v) => Some(v),
        Err(e) => {
            tracing::warn!("[MigrationCoordinator] Row parse error (skipped): {}", e);
            None
        }
    }
}

use super::definitions::MigrationSet;
use super::verifier::MigrationVerifier;
use super::MigrationError;

// å¯¼å…¥å„æ•°æ®åº“çš„è¿ç§»é›†åˆ
use super::chat_v2::CHAT_V2_MIGRATION_SET;
use super::llm_usage::LLM_USAGE_MIGRATION_SET;
use super::mistakes::MISTAKES_MIGRATIONS;
use super::vfs::VFS_MIGRATION_SET;

const SCHEMA_FINGERPRINT_TABLE: &str = "__governance_schema_fingerprints";
const CORE_BACKUP_ROOT_DIR_NAME: &str = "migration_core_backups";
const CORE_BACKUP_RETENTION_COUNT: usize = 5;

// åŒä¸€è¿›ç¨‹ï¼ˆä¸€æ¬¡åº”ç”¨å¯åŠ¨ï¼‰ä¸­ï¼Œé’ˆå¯¹åŒä¸€æ•°æ®ç›®å½•åªåšä¸€æ¬¡â€œè¿ç§»å‰æ ¸å¿ƒåº“å¤‡ä»½â€
static STARTUP_CORE_BACKUP_GUARD: OnceLock<Mutex<HashSet<String>>> = OnceLock::new();

/// è¿ç§»åè°ƒå™¨
pub struct MigrationCoordinator {
    /// åº”ç”¨æ•°æ®ç›®å½•
    app_data_dir: PathBuf,
    /// å®¡è®¡æ•°æ®åº“è¿æ¥è·¯å¾„ï¼ˆç”¨äºè®°å½•å®¡è®¡æ—¥å¿—ï¼‰
    audit_db_path: Option<PathBuf>,
}

/// è¿ç§»æŠ¥å‘Š
#[derive(Debug)]
pub struct MigrationReport {
    /// å„æ•°æ®åº“çš„è¿ç§»ç»“æœ
    pub databases: Vec<DatabaseMigrationReport>,
    /// æ€»ä½“æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// æ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub total_duration_ms: u64,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub error: Option<String>,
}

impl MigrationReport {
    /// åˆ›å»ºæ–°çš„æŠ¥å‘Š
    pub fn new() -> Self {
        Self {
            databases: Vec::new(),
            success: true,
            total_duration_ms: 0,
            error: None,
        }
    }

    /// æ·»åŠ æ•°æ®åº“æŠ¥å‘Š
    pub fn add(&mut self, report: DatabaseMigrationReport) {
        if !report.success {
            self.success = false;
        }
        self.databases.push(report);
    }
}

impl Default for MigrationReport {
    fn default() -> Self {
        Self::new()
    }
}

/// å•ä¸ªæ•°æ®åº“çš„è¿ç§»æŠ¥å‘Š
#[derive(Debug)]
pub struct DatabaseMigrationReport {
    /// æ•°æ®åº“æ ‡è¯†
    pub id: DatabaseId,
    /// è¿ç§»å‰ç‰ˆæœ¬
    pub from_version: u32,
    /// è¿ç§»åç‰ˆæœ¬
    pub to_version: u32,
    /// åº”ç”¨çš„è¿ç§»æ•°é‡
    pub applied_count: usize,
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub duration_ms: u64,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    pub error: Option<String>,
}

impl MigrationCoordinator {
    /// åˆ›å»ºæ–°çš„è¿ç§»åè°ƒå™¨
    pub fn new(app_data_dir: PathBuf) -> Self {
        // é»˜è®¤è®¾ç½®å®¡è®¡æ•°æ®åº“è·¯å¾„
        let audit_db_path = Some(app_data_dir.join("databases").join("audit.db"));
        Self {
            app_data_dir,
            audit_db_path,
        }
    }

    /// è®¾ç½®å®¡è®¡æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    pub fn with_audit_db(mut self, path: Option<PathBuf>) -> Self {
        self.audit_db_path = path;
        self
    }

    /// æ‰§è¡Œæ‰€æœ‰æ•°æ®åº“çš„è¿ç§»
    ///
    /// æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œï¼Œä»»ä¸€æ•°æ®åº“å¤±è´¥åˆ™åœæ­¢åç»­è¿ç§»ã€‚
    /// è¿ç§»å‰æ£€æŸ¥ç£ç›˜å¯ç”¨ç©ºé—´ï¼Œç©ºé—´ä¸è¶³æ—¶ fail-fastã€‚
    pub fn run_all(&mut self) -> Result<MigrationReport, MigrationError> {
        let start = std::time::Instant::now();
        let mut report = MigrationReport::new();

        tracing::info!(
            "ğŸš€ [MigrationCoordinator] å¼€å§‹æ‰§è¡Œæ‰€æœ‰æ•°æ®åº“è¿ç§», æ•°æ®ç›®å½•: {}",
            self.app_data_dir.display()
        );

        // Issue #11 ä¿®å¤ï¼šè¿ç§»å‰æ£€æŸ¥ç£ç›˜å¯ç”¨ç©ºé—´
        self.preflight_disk_space_check()?;

        // æ ¸å¿ƒåº“è¿ç§»å‰ä¿æŠ¤ï¼šä»…åœ¨å­˜åœ¨å¾…è¿ç§»é¡¹æ—¶ï¼Œä¸”åŒä¸€å¯åŠ¨å‘¨æœŸåªå¤‡ä»½ä¸€æ¬¡åˆå§‹çŠ¶æ€
        self.maybe_backup_core_databases_before_migration()?;

        // æŒ‰ä¾èµ–é¡ºåºè·å–æ•°æ®åº“åˆ—è¡¨
        let ordered_databases = DatabaseId::all_ordered();
        tracing::info!(
            "ğŸ“‹ [MigrationCoordinator] å¾…è¿ç§»æ•°æ®åº“: {:?}",
            ordered_databases
                .iter()
                .map(|d| d.as_str())
                .collect::<Vec<_>>()
        );

        for db_id in ordered_databases {
            // fail-closeï¼šä¾èµ–ä¸æ»¡è¶³æ—¶ç«‹å³ä¸­æ–­
            if let Err(e) = self.check_dependencies(&db_id, &report) {
                tracing::error!(
                    "âŒ [MigrationCoordinator] {} ä¾èµ–æ£€æŸ¥å¤±è´¥: {}",
                    db_id.as_str(),
                    e
                );
                report.success = false;
                report.error = Some(e.to_string());
                return Err(e);
            }

            // æ‰§è¡Œè¿ç§»ï¼ˆä»»ä¸€æ•°æ®åº“å¤±è´¥å³åœæ­¢ï¼‰
            match self.migrate_database(db_id.clone()) {
                Ok(db_report) => {
                    tracing::info!(
                        "âœ… [MigrationCoordinator] {} è¿ç§»å®Œæˆ: v{} -> v{}, åº”ç”¨äº† {} ä¸ªè¿ç§»",
                        db_id.as_str(),
                        db_report.from_version,
                        db_report.to_version,
                        db_report.applied_count
                    );
                    report.add(db_report);
                }
                Err(e) => {
                    // è®°å½•å·²æˆåŠŸè¿ç§»çš„æ•°æ®åº“ï¼Œå¸®åŠ©è¿ç»´äº†è§£éƒ¨åˆ†å®ŒæˆçŠ¶æ€
                    let completed_dbs: Vec<&str> = report
                        .databases
                        .iter()
                        .filter(|r| r.success)
                        .map(|r| r.id.as_str())
                        .collect();
                    tracing::error!(
                        failed_db = db_id.as_str(),
                        error = %e,
                        completed_dbs = ?completed_dbs,
                        "âŒ [MigrationCoordinator] {} è¿ç§»å¤±è´¥ (å·²å®Œæˆ: {:?})",
                        db_id.as_str(),
                        completed_dbs,
                    );
                    report.success = false;
                    report.error = Some(format!(
                        "Database '{}' migration failed: {}. Successfully completed: [{}]",
                        db_id.as_str(),
                        e,
                        completed_dbs.join(", "),
                    ));
                    return Err(e);
                }
            }
        }

        report.total_duration_ms = start.elapsed().as_millis() as u64;
        tracing::info!(
            "ğŸ [MigrationCoordinator] è¿ç§»å®Œæˆ, æ€»è€—æ—¶: {}ms, æˆåŠŸ: {}",
            report.total_duration_ms,
            report.success
        );
        Ok(report)
    }

    fn core_backup_root_dir(&self) -> PathBuf {
        self.app_data_dir.join(CORE_BACKUP_ROOT_DIR_NAME)
    }

    fn startup_guard_key(&self) -> String {
        std::fs::canonicalize(&self.app_data_dir)
            .unwrap_or_else(|_| self.app_data_dir.clone())
            .to_string_lossy()
            .to_string()
    }

    fn maybe_backup_core_databases_before_migration(&mut self) -> Result<(), MigrationError> {
        let pending = self.pending_migrations_count()?;
        if pending == 0 {
            tracing::info!(
                "[MigrationCoordinator] å½“å‰æ— å¾…æ‰§è¡Œè¿ç§»ï¼Œè·³è¿‡æ ¸å¿ƒåº“å¿«ç…§å¤‡ä»½: {}",
                self.app_data_dir.display()
            );
            return Ok(());
        }
        self.backup_core_databases_once_per_startup()
    }

    fn backup_sqlite_consistent(src: &PathBuf, dst: &PathBuf) -> Result<(), MigrationError> {
        let src_conn = rusqlite::Connection::open(src).map_err(|e| {
            MigrationError::Database(format!("æ‰“å¼€æºæ•°æ®åº“å¤±è´¥ {}: {}", src.display(), e))
        })?;
        let mut dst_conn = rusqlite::Connection::open(dst).map_err(|e| {
            MigrationError::Database(format!("åˆ›å»ºå¤‡ä»½æ•°æ®åº“å¤±è´¥ {}: {}", dst.display(), e))
        })?;

        {
            let backup = rusqlite::backup::Backup::new(&src_conn, &mut dst_conn).map_err(|e| {
                MigrationError::Database(format!("åˆå§‹åŒ– SQLite backup å¤±è´¥: {}", e))
            })?;
            backup
                .run_to_completion(50, Duration::from_millis(20), None)
                .map_err(|e| MigrationError::Database(format!("æ‰§è¡Œ SQLite backup å¤±è´¥: {}", e)))?;
        } // drop backupï¼Œé‡Šæ”¾ dst_conn çš„å¯å˜å€Ÿç”¨

        // P1-3 ä¿®å¤ï¼šå¤‡ä»½å®ŒæˆåéªŒè¯ç›®æ ‡æ•°æ®åº“å®Œæ•´æ€§
        // ä½¿ç”¨ quick_check è€Œé integrity_checkï¼šè·³è¿‡ç´¢å¼•éªŒè¯ï¼Œé€Ÿåº¦å¿« 5-10xï¼Œ
        // ä»èƒ½æ£€æµ‹ B-tree ç»“æ„æŸåå’Œè¡Œæ ¼å¼é”™è¯¯ã€‚å¯¹å¯åŠ¨æ—¶é—´å½±å“æ›´å°ã€‚
        let integrity: String = dst_conn
            .query_row("PRAGMA quick_check", [], |row| row.get(0))
            .map_err(|e| {
                MigrationError::Database(format!("å¤‡ä»½å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ {}: {}", dst.display(), e))
            })?;
        if integrity != "ok" {
            return Err(MigrationError::Database(format!(
                "å¤‡ä»½å®Œæ•´æ€§æ ¡éªŒä¸é€šè¿‡ {}: {}",
                dst.display(),
                integrity
            )));
        }

        Ok(())
    }

    fn prune_old_core_backups(&self) -> Result<(), MigrationError> {
        let root = self.core_backup_root_dir();
        if !root.exists() {
            return Ok(());
        }

        let mut snapshot_dirs: Vec<PathBuf> = std::fs::read_dir(&root)?
            .filter_map(|e| e.ok())
            .map(|e| e.path())
            .filter(|p| p.is_dir())
            .collect();

        snapshot_dirs.sort_by(|a, b| {
            a.file_name()
                .and_then(|n| n.to_str())
                .cmp(&b.file_name().and_then(|n| n.to_str()))
        });

        if snapshot_dirs.len() <= CORE_BACKUP_RETENTION_COUNT {
            return Ok(());
        }

        let remove_count = snapshot_dirs.len() - CORE_BACKUP_RETENTION_COUNT;
        for old in snapshot_dirs.into_iter().take(remove_count) {
            if let Err(e) = std::fs::remove_dir_all(&old) {
                tracing::warn!(
                    "[MigrationCoordinator] æ¸…ç†æ—§æ ¸å¿ƒå¿«ç…§å¤±è´¥: {} ({})",
                    old.display(),
                    e
                );
            }
        }
        Ok(())
    }

    fn backup_core_databases_once_per_startup(&mut self) -> Result<(), MigrationError> {
        let guard = STARTUP_CORE_BACKUP_GUARD.get_or_init(|| Mutex::new(HashSet::new()));
        let mut sessions = guard
            .lock()
            .map_err(|_| MigrationError::Database("æ ¸å¿ƒåº“å¤‡ä»½é”å·²æŸå".to_string()))?;

        let key = self.startup_guard_key();
        if sessions.contains(&key) {
            tracing::info!(
                "[MigrationCoordinator] å·²å­˜åœ¨æœ¬æ¬¡å¯åŠ¨çš„æ ¸å¿ƒåº“å¤‡ä»½ï¼Œè·³è¿‡: {}",
                self.app_data_dir.display()
            );
            return Ok(());
        }

        std::fs::create_dir_all(self.core_backup_root_dir())?;
        let timestamp = chrono::Utc::now().format("%Y%m%dT%H%M%S%.3fZ");
        let snapshot_dir = self.core_backup_root_dir().join(format!(
            "startup_{}_{}",
            timestamp,
            std::process::id()
        ));
        std::fs::create_dir_all(&snapshot_dir)?;

        let core_files = [
            "databases/vfs.db",
            "chat_v2.db",
            "mistakes.db",
            "llm_usage.db",
        ];

        let mut copied_files: Vec<String> = Vec::new();
        for relative in core_files {
            let src = self.app_data_dir.join(relative);
            if !src.exists() {
                continue;
            }
            let dst = snapshot_dir.join(relative);
            if let Some(parent) = dst.parent() {
                std::fs::create_dir_all(parent)?;
            }
            Self::backup_sqlite_consistent(&src, &dst)?;
            copied_files.push(relative.to_string());
        }

        // P1-2 ä¿®å¤ï¼šè®°å½•å„æ•°æ®åº“çš„ schema ç‰ˆæœ¬ï¼Œä¾¿äºæ‰‹åŠ¨æ¢å¤æ—¶åˆ¤æ–­å¤‡ä»½å¯¹åº”çš„ç‰ˆæœ¬
        let mut schema_versions = serde_json::Map::new();
        for relative in &copied_files {
            let db_path = self.app_data_dir.join(relative);
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                if let Ok(version) = self.get_current_version(&conn) {
                    let db_name = std::path::Path::new(relative)
                        .file_stem()
                        .and_then(|s| s.to_str())
                        .unwrap_or(relative);
                    schema_versions.insert(db_name.to_string(), serde_json::Value::from(version));
                }
            }
        }

        let metadata = serde_json::json!({
            "created_at": chrono::Utc::now().to_rfc3339(),
            "source_dir": self.app_data_dir.display().to_string(),
            "copied_files": copied_files,
            "schema_versions": schema_versions,
            "purpose": "pre-migration core databases snapshot",
        });
        std::fs::write(
            snapshot_dir.join("metadata.json"),
            serde_json::to_string_pretty(&metadata)
                .map_err(|e| MigrationError::Database(format!("å†™å…¥å¤‡ä»½å…ƒæ•°æ®å¤±è´¥: {}", e)))?,
        )?;

        tracing::info!(
            "[MigrationCoordinator] å·²å®Œæˆè¿ç§»å‰æ ¸å¿ƒåº“å¤‡ä»½: {}",
            snapshot_dir.display()
        );

        sessions.insert(key);
        self.prune_old_core_backups()?;
        Ok(())
    }

    /// æ£€æŸ¥æ•°æ®åº“ä¾èµ–æ˜¯å¦å·²æ»¡è¶³
    pub(crate) fn check_dependencies(
        &self,
        db_id: &DatabaseId,
        report: &MigrationReport,
    ) -> Result<(), MigrationError> {
        for dep in db_id.dependencies() {
            let dep_success = report
                .databases
                .iter()
                .find(|r| &r.id == dep)
                .map(|r| r.success)
                .unwrap_or(false);

            if !dep_success {
                return Err(MigrationError::DependencyNotSatisfied {
                    database: db_id.as_str().to_string(),
                    dependency: dep.as_str().to_string(),
                });
            }
        }
        Ok(())
    }

    /// è¿ç§»å•ä¸ªæ•°æ®åº“
    ///
    /// ä½¿ç”¨ Refinery æ¡†æ¶æ‰§è¡Œ SQL è¿ç§»ï¼Œç„¶åéªŒè¯ç»“æœã€‚
    /// å¯¹äºæ—§æ•°æ®åº“ï¼ˆæœ‰æ—§è¿ç§»è¡¨ä½†æ²¡æœ‰ refinery_schema_historyï¼‰ï¼Œä¼šå…ˆåˆ›å»º baselineã€‚
    fn migrate_database(
        &mut self,
        id: DatabaseId,
    ) -> Result<DatabaseMigrationReport, MigrationError> {
        let start = std::time::Instant::now();

        // è·å–æ•°æ®åº“è·¯å¾„
        let db_path = self.get_database_path(&id);

        tracing::info!(
            "ğŸ“¦ [Migration] å¼€å§‹è¿ç§»æ•°æ®åº“ {}: {}",
            id.as_str(),
            db_path.display()
        );

        // ç¡®ä¿ç›®å½•å­˜åœ¨
        if let Some(parent) = db_path.parent() {
            std::fs::create_dir_all(parent)?;
        }

        // æ‰“å¼€æ•°æ®åº“è¿æ¥
        let mut conn = match rusqlite::Connection::open(&db_path) {
            Ok(conn) => conn,
            Err(e) => {
                let err = MigrationError::Database(e.to_string());
                self.log_migration_failure(
                    &id,
                    0,
                    &err.to_string(),
                    start.elapsed().as_millis() as u64,
                );
                return Err(err);
            }
        };

        // ğŸ”§ å¯ç”¨å¤–é”®çº¦æŸï¼ˆSQLite é»˜è®¤ç¦ç”¨ï¼Œéœ€è¦åœ¨æ¯ä¸ªè¿æ¥ä¸Šå¯ç”¨ï¼‰
        // è¿™ç¡®ä¿è¿ç§»è„šæœ¬ä¸­çš„å¤–é”®çº¦æŸèƒ½æ­£ç¡®éªŒè¯
        conn.execute("PRAGMA foreign_keys = ON", [])
            .map_err(|e| MigrationError::Database(format!("å¯ç”¨å¤–é”®çº¦æŸå¤±è´¥: {}", e)))?;

        // ğŸ”§ æ—§æ•°æ®åº“å…¼å®¹å¤„ç†ï¼šæ£€æµ‹å¹¶åˆ›å»º baseline
        if let Err(e) = self.ensure_legacy_baseline(&conn, &id) {
            self.log_migration_failure(&id, 0, &e.to_string(), start.elapsed().as_millis() as u64);
            return Err(e);
        }

        // è·å–è¿ç§»å‰ç‰ˆæœ¬
        let from_version = match self.get_current_version(&conn) {
            Ok(version) => version,
            Err(e) => {
                self.log_migration_failure(
                    &id,
                    0,
                    &e.to_string(),
                    start.elapsed().as_millis() as u64,
                );
                return Err(e);
            }
        };

        // è·å–è¿ç§»é›†åˆ
        let migration_set = self.get_migration_set(&id);

        // é¢„å¤„ç†ï¼šä¿®å¤æ ¼å¼é”™è¯¯çš„è¿ç§»è®°å½•ï¼ˆæ‰€æœ‰æ•°æ®åº“é€šç”¨ï¼‰
        if let Err(e) = self.fix_malformed_migration_records(&conn) {
            self.log_migration_failure(
                &id,
                from_version,
                &e.to_string(),
                start.elapsed().as_millis() as u64,
            );
            return Err(e);
        }

        // æ‰§è¡Œè¿ç§»
        let applied_count = match self.run_refinery_migrations(&mut conn, &id) {
            Ok(count) => count,
            Err(e) => {
                self.log_migration_failure(
                    &id,
                    from_version,
                    &e.to_string(),
                    start.elapsed().as_millis() as u64,
                );
                return Err(e);
            }
        };

        // è·å–è¿ç§»åç‰ˆæœ¬
        let to_version = self.get_current_version(&conn)?;

        // fail-closeï¼šè¿ç§»åéªŒè¯å¤±è´¥æ—¶ç«‹å³ç»ˆæ­¢
        if let Err(e) = self.verify_migrations(&conn, &id, migration_set, to_version) {
            self.log_migration_failure(
                &id,
                from_version,
                &e.to_string(),
                start.elapsed().as_millis() as u64,
            );
            return Err(e);
        }

        let duration_ms = start.elapsed().as_millis() as u64;

        // è®°å½•å®¡è®¡æ—¥å¿—ï¼ˆåŒ…å«è€—æ—¶ï¼‰
        self.log_migration_audit(&id, from_version, to_version, applied_count, duration_ms)?;

        Ok(DatabaseMigrationReport {
            id,
            from_version,
            to_version,
            applied_count,
            success: true,
            duration_ms,
            error: None,
        })
    }

    /// è·å–æ•°æ®åº“æ–‡ä»¶è·¯å¾„
    ///
    /// æ³¨æ„ï¼š`app_data_dir` å·²ç»æ˜¯æ´»åŠ¨æ•°æ®ç©ºé—´ç›®å½•ï¼ˆå¦‚ `slots/slotA`ï¼‰ï¼Œ
    /// æ‰€ä»¥è·¯å¾„åº”è¯¥ç›¸å¯¹äºå®ƒï¼Œè€Œä¸æ˜¯å†åµŒå¥— slots ç›®å½•ã€‚
    fn get_database_path(&self, id: &DatabaseId) -> PathBuf {
        match id {
            // VFS æ•°æ®åº“æ”¾åœ¨ databases å­ç›®å½•
            DatabaseId::Vfs => self.app_data_dir.join("databases").join("vfs.db"),
            // ChatV2 æ•°æ®åº“ç›´æ¥æ”¾åœ¨ app_data_dir æ ¹ç›®å½•
            DatabaseId::ChatV2 => self.app_data_dir.join("chat_v2.db"),
            // Mistakes æ•°æ®åº“ç›´æ¥æ”¾åœ¨ app_data_dir æ ¹ç›®å½•
            DatabaseId::Mistakes => self.app_data_dir.join("mistakes.db"),
            // LLM Usage æ•°æ®åº“ç›´æ¥æ”¾åœ¨ app_data_dir æ ¹ç›®å½•
            DatabaseId::LlmUsage => self.app_data_dir.join("llm_usage.db"),
        }
    }

    /// è·å–æ•°æ®åº“çš„è¿ç§»é›†åˆ
    fn get_migration_set(&self, id: &DatabaseId) -> &'static MigrationSet {
        match id {
            DatabaseId::Vfs => &VFS_MIGRATION_SET,
            DatabaseId::ChatV2 => &CHAT_V2_MIGRATION_SET,
            DatabaseId::Mistakes => &MISTAKES_MIGRATIONS,
            DatabaseId::LlmUsage => &LLM_USAGE_MIGRATION_SET,
        }
    }

    /// è·å–å½“å‰ schema ç‰ˆæœ¬
    ///
    /// ä» Refinery çš„ `refinery_schema_history` è¡¨è¯»å–æœ€æ–°ç‰ˆæœ¬ã€‚
    pub(crate) fn get_current_version(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<u32, MigrationError> {
        // æ£€æŸ¥ Refinery çš„ schema history è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='refinery_schema_history')",
                [],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        if !table_exists {
            return Ok(0);
        }

        // è·å–æœ€å¤§ç‰ˆæœ¬å·
        let version: Option<i32> = conn
            .query_row(
                "SELECT MAX(version) FROM refinery_schema_history",
                [],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        Ok(version.unwrap_or(0) as u32)
    }

    /// è·å–å·²åº”ç”¨çš„è¿ç§»æ•°é‡
    ///
    /// ä» Refinery åˆ›å»ºçš„ `refinery_schema_history` è¡¨è¯»å–è¿ç§»è®°å½•æ•°ã€‚
    fn get_migration_count(&self, conn: &rusqlite::Connection) -> Result<usize, MigrationError> {
        // æ£€æŸ¥ Refinery çš„ schema history è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='refinery_schema_history')",
                [],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        if !table_exists {
            return Ok(0);
        }

        // è·å–è¿ç§»è®°å½•æ•°é‡
        let count: i32 = conn
            .query_row("SELECT COUNT(*) FROM refinery_schema_history", [], |row| {
                row.get(0)
            })
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        Ok(count as usize)
    }

    /// ä¸ºæ—§æ•°æ®åº“åˆ›å»º Refinery baseline
    ///
    /// æ£€æµ‹æ˜¯å¦æ˜¯æ—§è¿ç§»ç³»ç»Ÿçš„æ•°æ®åº“ï¼ˆæœ‰æ—§è¿ç§»è¡¨ä½†æ²¡æœ‰ refinery_schema_historyï¼‰ï¼Œ
    /// å¦‚æœæ˜¯ï¼Œåˆ™åˆ›å»º baseline è®°å½•ä½¿ Refinery èƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å·²æœ‰æ•°æ®ã€‚
    fn ensure_legacy_baseline(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
    ) -> Result<(), MigrationError> {
        // æ£€æŸ¥æ˜¯å¦å·²æœ‰ refinery_schema_history è¡¨ä¸”æœ‰è®°å½•
        let has_refinery_with_records: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM refinery_schema_history LIMIT 1)",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false); // è¡¨ä¸å­˜åœ¨æ—¶è¿”å› false

        if has_refinery_with_records {
            // å·²æœ‰ Refinery è¡¨ä¸”æœ‰è®°å½•ï¼Œä¸éœ€è¦åˆ›å»º baseline
            return Ok(());
        }

        // æ£€æµ‹æ—§è¿ç§»ç³»ç»Ÿ
        let legacy_info = self.detect_legacy_migration(conn, id)?;

        if let Some((legacy_type, has_data)) = legacy_info {
            if has_data {
                tracing::info!(
                    "ğŸ”„ [Migration] æ£€æµ‹åˆ°æ—§æ•°æ®åº“ {} ({}), åˆ›å»º Refinery baseline",
                    id.as_str(),
                    legacy_type
                );

                // åˆ›å»º refinery_schema_history è¡¨
                conn.execute(
                    "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                        version INTEGER PRIMARY KEY,
                        name TEXT,
                        applied_on TEXT,
                        checksum TEXT
                    )",
                    [],
                )
                .map_err(|e| MigrationError::Database(e.to_string()))?;

                // è·å–åˆå§‹è¿ç§»çš„ä¿¡æ¯
                let migration_set = self.get_migration_set(id);
                if let Some(first_migration) = migration_set.migrations.first() {
                    // baseline ä»…åœ¨é¦–è¿ç§»å¥‘çº¦æ»¡è¶³æ—¶å†™å…¥ï¼Œé¿å…â€œå…ˆè®°è´¦åä¿®å¤â€çš„æ¼‚ç§»
                    match MigrationVerifier::verify(conn, first_migration) {
                        Ok(()) => {
                            let now = chrono::Utc::now().to_rfc3339();

                            // æ’å…¥ baseline è®°å½•ï¼ˆæ ‡è®°åˆå§‹è¿ç§»å·²å®Œæˆï¼‰
                            // checksum ä½¿ç”¨ "0"ï¼Œåç»­ç”± repair_refinery_checksums å¯¹é½çœŸå®å€¼
                            conn.execute(
                                "INSERT OR IGNORE INTO refinery_schema_history (version, name, applied_on, checksum)
                                 VALUES (?1, ?2, ?3, ?4)",
                                rusqlite::params![
                                    first_migration.refinery_version,
                                    first_migration.name,
                                    now,
                                    "0",
                                ],
                            )
                            .map_err(|e| MigrationError::Database(e.to_string()))?;

                            tracing::info!(
                                "âœ… [Migration] å·²ä¸º {} åˆ›å»º baseline: v{}",
                                id.as_str(),
                                first_migration.refinery_version
                            );
                        }
                        Err(err) => {
                            tracing::warn!(
                                database = id.as_str(),
                                version = first_migration.refinery_version,
                                error = %err,
                                "âš ï¸ [Migration] é¦–è¿ç§»å¥‘çº¦æœªæ»¡è¶³ï¼Œè·³è¿‡ baseline è®°è´¦ï¼Œåç»­å°†æ‰§è¡ŒçœŸå®è¿ç§»"
                            );
                        }
                    }
                }
            }
        }

        Ok(())
    }

    /// æ£€æµ‹æ—§è¿ç§»ç³»ç»Ÿç±»å‹
    ///
    /// è¿”å› Some((è¿ç§»ç±»å‹åç§°, æ˜¯å¦æœ‰å®é™…æ•°æ®)) æˆ– Noneï¼ˆä¸æ˜¯æ—§æ•°æ®åº“ï¼‰
    fn detect_legacy_migration(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
    ) -> Result<Option<(&'static str, bool)>, MigrationError> {
        match id {
            DatabaseId::ChatV2 => {
                // æ£€æŸ¥ chat_v2_migrations è¡¨
                let has_legacy: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='chat_v2_migrations')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_legacy {
                    return Ok(Some(("chat_v2_migrations", true)));
                }

                // æ£€æŸ¥æ ¸å¿ƒè¡¨
                let has_sessions: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='chat_v2_sessions')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_sessions {
                    return Ok(Some(("existing_tables", true)));
                }
            }
            DatabaseId::LlmUsage => {
                // æ£€æŸ¥ schema_version è¡¨
                let has_legacy: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='schema_version')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_legacy {
                    return Ok(Some(("schema_version", true)));
                }

                // æ£€æŸ¥æ ¸å¿ƒè¡¨
                let has_logs: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='llm_usage_logs')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_logs {
                    return Ok(Some(("existing_tables", true)));
                }
            }
            DatabaseId::Mistakes => {
                // æ£€æŸ¥ migration_progress è¡¨
                let has_legacy: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='migration_progress')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_legacy {
                    return Ok(Some(("migration_progress", true)));
                }

                // æ£€æŸ¥æ ¸å¿ƒä¸šåŠ¡è¡¨ï¼ˆæ—§åº“é€šå¸¸è‡³å°‘åŒ…å« mistakesï¼‰
                let has_mistakes: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='mistakes')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_mistakes {
                    return Ok(Some(("existing_tables", true)));
                }
            }
            DatabaseId::Vfs => {
                // VFS å·²ç»è¿ç§»åˆ° Refineryï¼Œæ£€æŸ¥æ—§è¡¨
                let has_legacy: bool = conn
                    .query_row(
                        "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='vfs_schema_history')",
                        [],
                        |row| row.get(0),
                    )
                    .map_err(|e| MigrationError::Database(e.to_string()))?;

                if has_legacy {
                    return Ok(Some(("vfs_schema_history", true)));
                }
            }
        }

        Ok(None)
    }

    /// ä½¿ç”¨ Refinery æ‰§è¡Œè¿ç§»
    ///
    /// æ­¤æ–¹æ³•åœ¨ `data_governance` feature å¯ç”¨æ—¶ä½¿ç”¨ Refinery æ¡†æ¶ï¼Œ
    /// å¦åˆ™è¿”å› NotImplemented é”™è¯¯ã€‚
    #[cfg(feature = "data_governance")]
    fn run_refinery_migrations(
        &self,
        conn: &mut rusqlite::Connection,
        id: &DatabaseId,
    ) -> Result<usize, MigrationError> {
        // è·å–è¿ç§»å‰çš„è¿ç§»è®°å½•æ•°é‡
        let before_count = self.get_migration_count(conn)?;

        // æ ¹æ®æ•°æ®åº“ ID æ‰§è¡Œå¯¹åº”çš„è¿ç§»
        let runner = match id {
            DatabaseId::Vfs => self.create_vfs_runner()?,
            DatabaseId::ChatV2 => self.create_chat_v2_runner()?,
            DatabaseId::Mistakes => self.create_mistakes_runner()?,
            DatabaseId::LlmUsage => self.create_llm_usage_runner()?,
        };

        // é¢„ä¿®å¤ï¼šå¯¹é½å·²åº”ç”¨è¿ç§»çš„ checksumï¼Œé¿å… Refiner divergent æŠ¥é”™
        self.repair_refinery_checksums(conn, id, &runner)?;

        // é…ç½® Runnerï¼š
        // - set_grouped(false): é€æ¡è¿ç§»ï¼Œæ¯æ¡æˆåŠŸç«‹å³è®°å½•åˆ° refinery_schema_historyã€‚
        //   **ä¸èƒ½ç”¨ set_grouped(true)**ï¼šSQLite å¯¹ DDLï¼ˆALTER TABLE ADD COLUMNï¼‰çš„
        //   äº‹åŠ¡å›æ»šä¸å¯é â€”â€”åˆ—å·²åŠ ä¸Šä½† refinery_schema_history è®°å½•è¢«å›æ»šï¼Œå¯¼è‡´
        //   ä¸‹æ¬¡é‡è·‘æ—¶ duplicate column æ°¸ä¹…å¡æ­»ã€‚é€æ¡æ‰§è¡Œé¿å…è¿™ä¸ªæ ¹æœ¬çŸ›ç›¾ã€‚
        // - set_abort_divergent(false): ä¸å›  checksum ä¸åŒ¹é…è€Œä¸­æ­¢ï¼ˆå…¼å®¹æ—§æ•°æ®åº“ï¼‰
        // - set_abort_missing(false): ä¸å› ç¼ºå°‘è¿ç§»æ–‡ä»¶è€Œä¸­æ­¢
        let runner = runner
            .set_grouped(false)
            .set_abort_divergent(false)
            .set_abort_missing(false);

        // è¿ç§»å‰ï¼šæ¸…ç†å¯èƒ½å­˜åœ¨çš„ä¸­é—´çŠ¶æ€è¡¨ï¼ˆä»ä¹‹å‰å¤±è´¥çš„è¿ç§»é—ç•™ï¼‰
        self.cleanup_intermediate_tables(conn, id)?;

        // ğŸ”§ é¢„ä¿®å¤ï¼šå¤„ç† schema ä¸ä¸€è‡´é—®é¢˜ï¼ˆæ—§æ•°æ®åº“å…¼å®¹ï¼‰
        // è¿™ä¼šæ£€æŸ¥å¹¶ä¿®å¤åˆ—ç¼ºå¤±/é‡å¤çš„é—®é¢˜ï¼Œé¿å…è¿ç§»å¤±è´¥
        self.pre_repair_schema(conn, id, &runner)?;

        // ğŸ”§ é€šç”¨é˜²å¾¡ï¼šå¯¹æ‰€æœ‰å¾…æ‰§è¡Œè¿ç§»ä¸­çš„ ALTER TABLE ADD COLUMN åšå¹‚ç­‰é¢„å¤„ç†
        // æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¯èƒ½ç”±ä¹‹å‰å¤±è´¥çš„ grouped äº‹åŠ¡æ®‹ç•™ï¼‰ï¼Œå·²å­˜åœ¨åˆ™é¢„æ ‡è®°è¿ç§»å®Œæˆ
        // è¿™æ˜¯æ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼Œä¸å†éœ€è¦ä¸ºæ¯ä¸ªæ–°è¿ç§»æ‰‹åŠ¨å†™ pre_repair
        self.make_alter_columns_safe(conn, &runner)?;

        // æ‰§è¡Œè¿ç§»
        runner
            .run(conn)
            .map_err(|e| MigrationError::Refinery(e.to_string()))?;

        // è·å–è¿ç§»åçš„è¿ç§»è®°å½•æ•°é‡
        let after_count = self.get_migration_count(conn)?;

        // è®¡ç®—åº”ç”¨çš„è¿ç§»æ•°é‡ï¼ˆé€šè¿‡è¿ç§»è®°å½•æ•°å·®å€¼ï¼‰
        let applied_count = after_count.saturating_sub(before_count);

        // è·å–å½“å‰ç‰ˆæœ¬ç”¨äºæ—¥å¿—
        let after_version = self.get_current_version(conn)?;

        tracing::info!(
            database = id.as_str(),
            to_version = after_version,
            applied_count = applied_count,
            "Migration completed"
        );

        Ok(applied_count)
    }

    #[cfg(not(feature = "data_governance"))]
    fn run_refinery_migrations(
        &self,
        _conn: &mut rusqlite::Connection,
        id: &DatabaseId,
    ) -> Result<usize, MigrationError> {
        Err(MigrationError::NotImplemented(format!(
            "Refinery migrations for {} (feature 'data_governance' not enabled)",
            id.as_str()
        )))
    }

    /// åˆ›å»º VFS æ•°æ®åº“çš„ Refinery Runner
    #[cfg(feature = "data_governance")]
    fn create_vfs_runner(&self) -> Result<refinery::Runner, MigrationError> {
        // ä½¿ç”¨ embed_migrations! å®åµŒå…¥è¿ç§»æ–‡ä»¶
        // è¿ç§»æ–‡ä»¶è·¯å¾„ç›¸å¯¹äº Cargo.toml æ‰€åœ¨ç›®å½•
        mod vfs_migrations {
            refinery::embed_migrations!("migrations/vfs");
        }

        Ok(vfs_migrations::migrations::runner())
    }

    /// åˆ›å»º Chat V2 æ•°æ®åº“çš„ Refinery Runner
    #[cfg(feature = "data_governance")]
    fn create_chat_v2_runner(&self) -> Result<refinery::Runner, MigrationError> {
        mod chat_v2_migrations {
            refinery::embed_migrations!("migrations/chat_v2");
        }

        Ok(chat_v2_migrations::migrations::runner())
    }

    /// åˆ›å»º Mistakes æ•°æ®åº“çš„ Refinery Runner
    #[cfg(feature = "data_governance")]
    fn create_mistakes_runner(&self) -> Result<refinery::Runner, MigrationError> {
        mod mistakes_migrations {
            refinery::embed_migrations!("migrations/mistakes");
        }

        Ok(mistakes_migrations::migrations::runner())
    }

    /// åˆ›å»º LLM Usage æ•°æ®åº“çš„ Refinery Runner
    #[cfg(feature = "data_governance")]
    fn create_llm_usage_runner(&self) -> Result<refinery::Runner, MigrationError> {
        mod llm_usage_migrations {
            refinery::embed_migrations!("migrations/llm_usage");
        }

        Ok(llm_usage_migrations::migrations::runner())
    }

    /// ä¿®å¤æ ¼å¼é”™è¯¯çš„è¿ç§»è®°å½•
    ///
    /// åˆ é™¤ä¹‹å‰ç‰ˆæœ¬æ’å…¥çš„æ ¼å¼é”™è¯¯çš„è¿ç§»è®°å½•ï¼Œ
    /// ç„¶åé‡æ–°æ’å…¥æ­£ç¡®æ ¼å¼çš„è®°å½•ã€‚
    fn fix_malformed_migration_records(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        // æ£€æŸ¥ refinery_schema_history è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='refinery_schema_history')",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false);

        if !table_exists {
            return Ok(());
        }

        // ğŸ”§ æ—§æ•°æ®åº“å…¼å®¹ï¼šåªåˆ é™¤æ˜æ˜¾æ— æ•ˆçš„è®°å½•
        // - checksum ä¸º NULL æˆ–ç©ºå­—ç¬¦ä¸²
        // - version ä¸º NULL æˆ– 0
        // ä¸å†æ£€æŸ¥ applied_on æ ¼å¼ï¼Œå› ä¸ºä¸åŒæ¥æºå¯èƒ½æœ‰ä¸åŒæ ¼å¼
        let deleted = conn
            .execute(
                "DELETE FROM refinery_schema_history WHERE
             checksum IS NULL OR checksum = '' OR
             version IS NULL OR version = 0",
                [],
            )
            .unwrap_or(0);

        if deleted > 0 {
            tracing::info!(deleted_count = deleted, "åˆ é™¤äº†æ— æ•ˆçš„è¿ç§»è®°å½•");
        }

        Ok(())
    }

    /// é€šç”¨å¹‚ç­‰é˜²å¾¡ï¼šå¯¹æ‰€æœ‰å¾…æ‰§è¡Œè¿ç§»ä¸­çš„ ALTER TABLE ADD COLUMN åšé¢„æ£€æŸ¥
    ///
    /// ## èƒŒæ™¯
    ///
    /// SQLite å¯¹ DDLï¼ˆALTER TABLE ADD COLUMNï¼‰çš„äº‹åŠ¡å›æ»šä¸å¯é ï¼š
    /// åˆ—å·²åŠ ä¸Šä½† refinery_schema_history çš„è®°å½•è¢«å›æ»šï¼Œå¯¼è‡´ä¸‹æ¬¡é‡è·‘æ—¶
    /// duplicate column æ°¸ä¹…å¡æ­»ã€‚
    ///
    /// å³ä½¿æ”¹ä¸º set_grouped(false)ï¼ˆé€æ¡è¿ç§»ï¼‰ï¼Œä»å¯èƒ½å› ä¸ºå•æ¡è¿ç§»å†…éƒ¨
    /// åŒ…å«å¤šæ¡ ALTER TABLE è€Œå‡ºç°éƒ¨åˆ†æ®‹ç•™ã€‚
    ///
    /// ## ç­–ç•¥
    ///
    /// å¯¹æ¯æ¡**æœªè®°å½•**çš„è¿ç§»ï¼Œè§£æå…¶ SQL ä¸­çš„ ALTER TABLE ADD COLUMN è¯­å¥ï¼Œ
    /// æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å·²å­˜åœ¨ã€‚å¦‚æœè¯¥è¿ç§»çš„**æ‰€æœ‰éå¹‚ç­‰ ALTER TABLE ADD COLUMN
    /// çš„ç›®æ ‡åˆ—éƒ½å·²å­˜åœ¨**ï¼Œåˆ™è®¤ä¸ºè¯¥è¿ç§»å®é™…ä¸Šå·²ç»æ‰§è¡Œè¿‡ï¼ˆåªæ˜¯è®°å½•è¢«å›æ»šäº†ï¼‰ï¼Œ
    /// é¢„å…ˆæ ‡è®°ä¸ºå·²å®Œæˆï¼Œè®© Refinery è·³è¿‡å®ƒã€‚
    ///
    /// è¿™æ˜¯æ ¹æœ¬è§£å†³æ–¹æ¡ˆï¼Œ**ä¸å†éœ€è¦ä¸ºæ¯ä¸ªæ–°è¿ç§»æ‰‹åŠ¨å†™ pre_repair**ã€‚
    #[cfg(feature = "data_governance")]
    fn make_alter_columns_safe(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        self.ensure_refinery_history_table(conn)?;

        for migration in runner.get_migrations() {
            let version = migration.version();

            // è·³è¿‡å·²è®°å½•çš„è¿ç§»
            if self.is_migration_recorded(conn, version)? {
                continue;
            }

            // è§£æ SQL ä¸­çš„ ALTER TABLE ... ADD COLUMN
            let sql = migration.sql().unwrap_or_default();
            let alter_columns = Self::parse_alter_add_columns(sql);

            if alter_columns.is_empty() {
                continue; // è¯¥è¿ç§»æ²¡æœ‰ ALTER TABLE ADD COLUMNï¼Œä¸éœ€è¦å¤„ç†
            }

            // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ ALTER TABLE ADD COLUMN çš„ç›®æ ‡åˆ—éƒ½å·²å­˜åœ¨
            let mut all_exist = true;
            let mut any_exist = false;

            for (table, column) in &alter_columns {
                if self.table_exists(conn, table)? && self.column_exists(conn, table, column)? {
                    any_exist = true;
                } else {
                    all_exist = false;
                }
            }

            if all_exist {
                // æ‰€æœ‰éå¹‚ç­‰åˆ—éƒ½å·²å­˜åœ¨ â†’ è¯¥è¿ç§»å®é™…å·²æ‰§è¡Œï¼Œæ ‡è®°å®Œæˆ
                tracing::info!(
                    version = version,
                    columns = ?alter_columns,
                    "ğŸ”§ [make_alter_columns_safe] æ£€æµ‹åˆ°æ‰€æœ‰ ALTER åˆ—å·²å­˜åœ¨ï¼Œæ ‡è®° V{} ä¸ºå·²å®Œæˆ",
                    version
                );
                self.mark_migration_complete(conn, runner, version)?;
            } else if any_exist {
                // éƒ¨åˆ†åˆ—å­˜åœ¨ â†’ ä¸­é—´çŠ¶æ€ï¼Œè¡¥é½ç¼ºå¤±çš„åˆ—
                tracing::info!(
                    version = version,
                    columns = ?alter_columns,
                    "ğŸ”§ [make_alter_columns_safe] æ£€æµ‹åˆ°éƒ¨åˆ† ALTER åˆ—å·²å­˜åœ¨ï¼ˆä¸­é—´çŠ¶æ€ï¼‰ï¼Œè¡¥é½å¹¶æ ‡è®° V{}",
                    version
                );
                for (table, column) in &alter_columns {
                    // ä» SQL ä¸­æå–è¯¥åˆ—çš„å®Œæ•´å®šä¹‰
                    let col_def = Self::extract_column_def(sql, table, column);
                    let _ = self.add_column_if_missing(conn, table, column, &col_def)?;
                }
                // æ‰§è¡Œè¿ç§»ä¸­çš„ CREATE INDEX IF NOT EXISTS / CREATE TABLE IF NOT EXISTS
                // è¿™äº›æ˜¯å¹‚ç­‰çš„ï¼Œå¯ä»¥å®‰å…¨é‡è·‘
                Self::replay_idempotent_statements(conn, sql);
                self.mark_migration_complete(conn, runner, version)?;
            }
            // å¦‚æœæ²¡æœ‰ä»»ä½•åˆ—å­˜åœ¨ï¼Œè¯´æ˜è¿ç§»ä»æœªæ‰§è¡Œè¿‡ï¼Œæ­£å¸¸è®© Refinery æ‰§è¡Œ
        }

        Ok(())
    }

    /// ä»è¿ç§» SQL ä¸­è§£æ ALTER TABLE ... ADD COLUMN è¯­å¥
    ///
    /// è¿”å› `(table_name, column_name)` åˆ—è¡¨
    #[cfg(feature = "data_governance")]
    fn parse_alter_add_columns(sql: &str) -> Vec<(String, String)> {
        let mut results = Vec::new();
        // åŒ¹é… ALTER TABLE xxx ADD COLUMN yyyï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        for line in sql.lines() {
            let trimmed = line.trim();
            let upper = trimmed.to_uppercase();
            if upper.contains("ALTER")
                && upper.contains("TABLE")
                && upper.contains("ADD")
                && upper.contains("COLUMN")
            {
                // è§£æ: ALTER TABLE <table> ADD COLUMN <column> ...
                let tokens: Vec<&str> = trimmed.split_whitespace().collect();
                // æ‰¾åˆ° TABLE åé¢çš„è¡¨åå’Œ COLUMN åé¢çš„åˆ—å
                let mut table = None;
                let mut column = None;
                for i in 0..tokens.len() {
                    let t = tokens[i].to_uppercase();
                    if t == "TABLE" && i + 1 < tokens.len() && table.is_none() {
                        table = Some(
                            tokens[i + 1].trim_matches(|c: char| !c.is_alphanumeric() && c != '_'),
                        );
                    }
                    if t == "COLUMN" && i + 1 < tokens.len() && column.is_none() {
                        column = Some(
                            tokens[i + 1].trim_matches(|c: char| !c.is_alphanumeric() && c != '_'),
                        );
                    }
                }
                if let (Some(t), Some(c)) = (table, column) {
                    if !t.is_empty() && !c.is_empty() {
                        results.push((t.to_string(), c.to_string()));
                    }
                }
            }
        }
        results
    }

    /// ä» SQL ä¸­æå–åˆ—å®šä¹‰ï¼ˆALTER TABLE xxx ADD COLUMN yyy <definition>ï¼‰
    ///
    /// è¿”å› COLUMN åç§°ä¹‹åçš„ç±»å‹å®šä¹‰éƒ¨åˆ†ï¼Œå¦‚ "TEXT DEFAULT 'pending'"
    #[cfg(feature = "data_governance")]
    fn extract_column_def(sql: &str, target_table: &str, target_column: &str) -> String {
        for line in sql.lines() {
            let trimmed = line.trim().trim_end_matches(';');
            let upper = trimmed.to_uppercase();
            if !upper.contains("ALTER") || !upper.contains("ADD") || !upper.contains("COLUMN") {
                continue;
            }
            // æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡è¡¨å’Œåˆ—
            let upper_table = target_table.to_uppercase();
            let upper_column = target_column.to_uppercase();
            if !upper.contains(&upper_table) || !upper.contains(&upper_column) {
                continue;
            }
            // æ‰¾åˆ° COLUMN <name> ä¹‹åçš„éƒ¨åˆ†ä½œä¸ºç±»å‹å®šä¹‰
            let tokens: Vec<&str> = trimmed.split_whitespace().collect();
            for i in 0..tokens.len() {
                if tokens[i].to_uppercase() == "COLUMN" && i + 1 < tokens.len() {
                    let col_name =
                        tokens[i + 1].trim_matches(|c: char| !c.is_alphanumeric() && c != '_');
                    if col_name.to_uppercase() == upper_column {
                        // COLUMN åä¹‹åçš„æ‰€æœ‰ token å°±æ˜¯ç±»å‹å®šä¹‰
                        if i + 2 < tokens.len() {
                            return tokens[i + 2..].join(" ");
                        }
                        return "TEXT".to_string(); // é»˜è®¤ç±»å‹
                    }
                }
            }
        }
        "TEXT".to_string() // å…œåº•é»˜è®¤
    }

    /// é‡æ”¾è¿ç§» SQL ä¸­çš„å¹‚ç­‰è¯­å¥ï¼ˆCREATE TABLE/INDEX IF NOT EXISTSï¼‰
    ///
    /// åœ¨ä¸­é—´çŠ¶æ€ä¿®å¤æ—¶è°ƒç”¨ï¼Œç¡®ä¿è¿ç§»ä¸­çš„å»ºè¡¨/å»ºç´¢å¼•è¯­å¥ä¹Ÿè¢«æ‰§è¡Œ
    #[cfg(feature = "data_governance")]
    fn replay_idempotent_statements(conn: &rusqlite::Connection, sql: &str) {
        for line in sql.lines() {
            let trimmed = line.trim();
            let upper = trimmed.to_uppercase();
            // åªé‡æ”¾å¹‚ç­‰çš„ CREATE è¯­å¥
            if upper.starts_with("CREATE TABLE IF NOT EXISTS")
                || upper.starts_with("CREATE INDEX IF NOT EXISTS")
                || upper.starts_with("CREATE UNIQUE INDEX IF NOT EXISTS")
                || upper.starts_with("CREATE TRIGGER IF NOT EXISTS")
            {
                if let Err(e) = conn.execute(trimmed.trim_end_matches(';'), []) {
                    tracing::warn!(
                        sql = trimmed,
                        error = %e,
                        "replay_idempotent_statements: æ‰§è¡Œå¤±è´¥ï¼ˆç»§ç»­ï¼‰"
                    );
                }
            }
        }
    }

    /// ä¿®å¤å› è¿ç§»è„šæœ¬å˜æ›´å¯¼è‡´çš„ checksum ä¸ä¸€è‡´
    ///
    /// ä»…æ›´æ–° refinery_schema_history ä¸­å·²å­˜åœ¨çš„è®°å½•ï¼Œé¿å…é‡å¤è¿ç§»æ‰§è¡Œã€‚
    ///
    /// ## å®‰å…¨é™åˆ¶
    ///
    /// - ä»…ä¿®æ”¹å·²å­˜åœ¨çš„è¿ç§»è®°å½•ï¼Œä¸æ’å…¥æ–°è®°å½•
    /// - æ¯æ¬¡ä¿®å¤éƒ½è®°å½•è¯¦ç»†å®¡è®¡æ—¥å¿—ï¼ˆå« old/new checksumï¼‰
    /// - ä¿®å¤æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶å‘å‡ºè­¦å‘Š
    fn repair_refinery_checksums(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='refinery_schema_history')",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false);

        if !table_exists {
            return Ok(());
        }

        /// å®‰å…¨é˜ˆå€¼ï¼šå•æ¬¡ä¿®å¤è¶…è¿‡æ­¤æ•°é‡å‘å‡ºè­¦å‘Š
        const REPAIR_WARN_THRESHOLD: usize = 5;

        let mut repaired = 0usize;
        let mut repair_details: Vec<String> = Vec::new();

        for migration in runner.get_migrations() {
            let version = migration.version();
            let name = migration.name().to_string();
            let checksum = migration.checksum().to_string();

            let existing: Option<(String, String)> = conn
                .query_row(
                    "SELECT name, checksum FROM refinery_schema_history WHERE version = ?1",
                    rusqlite::params![version],
                    |row| Ok((row.get(0)?, row.get(1)?)),
                )
                .optional()
                .map_err(|e| MigrationError::Database(e.to_string()))?;

            if let Some((db_name, db_checksum)) = existing {
                if db_checksum == checksum && db_name == name {
                    continue; // å·²ä¸€è‡´ï¼Œè·³è¿‡
                }

                // å®‰å…¨é™åˆ¶ï¼šä»…åœ¨ä»¥ä¸‹æƒ…å†µä¿®å¤
                // 1. baseline å¯¹é½ï¼ˆchecksum="0"ï¼Œç”± ensure_legacy_baseline å†™å…¥ï¼‰
                // 2. åŒåè¿ç§»çš„ checksum æ¼‚ç§»ï¼ˆè„šæœ¬å†…å®¹å˜æ›´ä½†åç§°ä¸€è‡´ï¼‰
                let is_baseline = db_checksum == "0";
                let is_same_name = db_name == name;

                if !is_baseline && !is_same_name {
                    tracing::warn!(
                        database = id.as_str(),
                        version = version,
                        db_name = %db_name,
                        expected_name = %name,
                        "è·³è¿‡ checksum ä¿®å¤ï¼šè¿ç§»åç§°ä¸åŒ¹é…ä¸”é baselineï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬å·å†²çª"
                    );
                    continue;
                }

                conn.execute(
                    "UPDATE refinery_schema_history SET name = ?1, checksum = ?2 WHERE version = ?3",
                    rusqlite::params![name, checksum, version],
                )
                .map_err(|e| MigrationError::Database(e.to_string()))?;

                let detail = format!(
                    "v{}: name '{}'->'{}', checksum '{}..'->'{}..', reason={}",
                    version,
                    &db_name,
                    &name,
                    &db_checksum.get(..8).unwrap_or(&db_checksum),
                    &checksum.get(..8).unwrap_or(&checksum),
                    if is_baseline {
                        "baseline_alignment"
                    } else {
                        "checksum_drift"
                    },
                );
                repair_details.push(detail);
                repaired += 1;
            }
        }

        if repaired > 0 {
            if repaired > REPAIR_WARN_THRESHOLD {
                tracing::warn!(
                    database = id.as_str(),
                    repaired = repaired,
                    threshold = REPAIR_WARN_THRESHOLD,
                    "âš ï¸ Checksum repair count exceeds safety threshold â€” review migration scripts"
                );
            }

            tracing::info!(
                database = id.as_str(),
                repaired = repaired,
                details = ?repair_details,
                "Refinery checksum records reconciled"
            );

            // å†™å…¥å®¡è®¡æ—¥å¿—
            self.log_checksum_repair_audit(id, &repair_details);
        }

        Ok(())
    }

    /// è®°å½• checksum ä¿®å¤çš„å®¡è®¡æ—¥å¿—
    fn log_checksum_repair_audit(&self, id: &DatabaseId, repair_details: &[String]) {
        use crate::data_governance::audit::AuditRepository;

        let Some(audit_db_path) = &self.audit_db_path else {
            return;
        };

        let Ok(conn) = rusqlite::Connection::open(audit_db_path) else {
            tracing::warn!("Failed to open audit db for checksum repair logging");
            return;
        };

        if AuditRepository::init(&conn).is_err() {
            return;
        }

        let details_json = serde_json::json!({
            "action": "checksum_repair",
            "database": id.as_str(),
            "repairs": repair_details,
            "count": repair_details.len(),
        });

        let log = crate::data_governance::audit::AuditLog::new(
            crate::data_governance::audit::AuditOperation::Migration {
                from_version: 0,
                to_version: 0,
                applied_count: 0,
            },
            format!("checksum_repair:{}", id.as_str()),
        )
        .with_details(details_json)
        .complete(0);

        if let Err(e) = AuditRepository::save(&conn, &log) {
            tracing::warn!(error = %e, "Failed to save checksum repair audit log");
        }
    }

    /// é¢„ä¿®å¤ schema ä¸ä¸€è‡´é—®é¢˜
    ///
    /// åœ¨æ‰§è¡Œ Refinery è¿ç§»ä¹‹å‰ï¼Œæ£€æŸ¥å¹¶ä¿®å¤ä»¥ä¸‹é—®é¢˜ï¼š
    /// 1. VFS: æ—§æ•°æ®åº“å¯èƒ½ç¼ºå°‘ `deleted_at` åˆ—ï¼ˆè™½ç„¶è¿ç§»è®°å½•æ˜¾ç¤º v20260130ï¼‰
    /// 2. chat_v2: å¦‚æœ `active_skill_ids_json` åˆ—å·²å­˜åœ¨ï¼Œæ ‡è®°è¿ç§»ä¸ºå·²å®Œæˆ
    /// 3. mistakes: å¦‚æœ `preview_data_json` åˆ—å·²å­˜åœ¨ï¼Œæ ‡è®°è¿ç§»ä¸ºå·²å®Œæˆ
    ///
    /// è¿™è§£å†³äº†æ•°æ®åº“å®é™… schema ä¸è¿ç§»è®°å½•ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
    #[cfg(feature = "data_governance")]
    fn pre_repair_schema(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        match id {
            DatabaseId::Vfs => self.pre_repair_vfs_schema(conn, runner)?,
            DatabaseId::ChatV2 => self.pre_repair_chat_v2_schema(conn, runner)?,
            DatabaseId::Mistakes => self.pre_repair_mistakes_schema(conn, runner)?,
            DatabaseId::LlmUsage => self.pre_repair_llm_usage_schema(conn, runner)?,
        }
        Ok(())
    }

    /// æ£€æŸ¥è¡¨ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåˆ—
    #[cfg(feature = "data_governance")]
    fn column_exists(
        &self,
        conn: &rusqlite::Connection,
        table_name: &str,
        column_name: &str,
    ) -> Result<bool, MigrationError> {
        let exists: bool = conn
            .query_row(
                "SELECT COUNT(*) > 0 FROM pragma_table_info(?1) WHERE name = ?2",
                rusqlite::params![table_name, column_name],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;
        Ok(exists)
    }

    /// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    #[cfg(feature = "data_governance")]
    fn table_exists(
        &self,
        conn: &rusqlite::Connection,
        table_name: &str,
    ) -> Result<bool, MigrationError> {
        let exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name=?1)",
                [table_name],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;
        Ok(exists)
    }

    /// é¢„ä¿®å¤ VFS æ•°æ®åº“çš„ schema
    ///
    /// é—®é¢˜ï¼šæ—§æ•°æ®åº“åœ¨ v20260130 ä¹‹å‰åˆ›å»ºï¼Œresources ç­‰è¡¨å¯èƒ½ç¼ºå°‘ deleted_at åˆ—ï¼Œ
    /// ä½†è¿ç§»è®°å½•æ˜¾ç¤ºä¸º v20260130ã€‚V20260201 è¿ç§»å°è¯•åˆ›å»ºå¼•ç”¨ deleted_at çš„ç´¢å¼•ä¼šå¤±è´¥ã€‚
    #[cfg(feature = "data_governance")]
    fn pre_repair_vfs_schema(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        // --- V20260131: __change_log è¡¨ä¿®å¤ï¼ˆé€šç”¨é˜²å¾¡ï¼‰ ---
        self.ensure_change_log_table(
            conn,
            "vfs",
            include_str!("../../../migrations/vfs/V20260131__add_change_log.sql"),
            "resources",
        )?;

        const TARGET_VERSION: i32 = 20260201;

        // æ–°æ•°æ®åº“ï¼ˆå°šæœªåˆ›å»ºè¡¨ï¼‰æ— éœ€é¢„ä¿®å¤
        if !self.table_exists(conn, "resources")? {
            return Ok(());
        }

        // V20260201 å·²è®°å½•ï¼šç›´æ¥è¡¥é½ç¼ºå¤±åˆ—/ç´¢å¼•ï¼Œé¿å… schema ä¸ä¸€è‡´
        let migration_recorded = self.is_migration_recorded(conn, TARGET_VERSION)?;
        if migration_recorded {
            self.apply_vfs_sync_fields_compat(conn)?;
        } else {
            // å¦‚æœä»»ä¸€åŒæ­¥å­—æ®µå·²å­˜åœ¨ï¼Œè¯´æ˜æ—§åº“éƒ¨åˆ†è¿ç§»æˆ–æ‰‹åŠ¨æ”¹åŠ¨è¿‡
            // è¿™ä¼šå¯¼è‡´ V20260201 è¿ç§»å‡ºç° duplicate column é”™è¯¯
            let would_conflict = self.vfs_sync_fields_would_conflict(conn)?;
            if would_conflict {
                self.apply_vfs_sync_fields_compat(conn)?;
                self.ensure_refinery_history_table(conn)?;
                self.mark_migration_complete(conn, runner, TARGET_VERSION)?;
            } else {
                // æ­£å¸¸æƒ…å†µï¼šè¡¥é½ deleted_atï¼ˆresources/notes/questions/foldersï¼‰
                // review_plans çš„ deleted_at ç”± V20260201 è¿ç§»æ·»åŠ ï¼Œé¿å…é‡å¤
                self.ensure_vfs_deleted_at_core(conn)?;
            }
        }

        // V20260204: PDF å¤„ç†çŠ¶æ€å­—æ®µï¼ˆ5 åˆ— + 3 ç´¢å¼•ï¼‰
        self.pre_repair_vfs_v20260204(conn, runner)?;

        // V20260205: å‹ç¼© blob hashï¼ˆ1 åˆ— + 1 ç´¢å¼•ï¼‰
        self.pre_repair_vfs_v20260205(conn, runner)?;

        // V20260209: é¢˜ç›®å›¾ç‰‡ï¼ˆ1 åˆ—ï¼‰
        self.pre_repair_vfs_v20260209(conn, runner)?;

        // V20260210: ç­”é¢˜æäº¤ï¼ˆ3 åˆ—ï¼Œanswer_submissions è¡¨å¤©ç„¶å¹‚ç­‰ï¼‰
        self.pre_repair_vfs_v20260210(conn, runner)?;

        Ok(())
    }

    /// ç¡®ä¿ __change_log è¡¨å­˜åœ¨ï¼ˆé€šç”¨é˜²å¾¡ï¼‰
    ///
    /// æ‰€æœ‰å››ä¸ªæ•°æ®åº“çš„ V20260131 éƒ½åˆ›å»º __change_log è¡¨ã€‚
    /// æ—§ç‰ˆ set_grouped(true) æ—¶ä»£ï¼ŒSQLite DDL å›æ»šåè¡¨å¯èƒ½è¢«åˆ é™¤ï¼Œ
    /// ä½† refinery_schema_history ä¸­çš„è®°å½•æœªè¢«å›æ»šï¼Œå¯¼è‡´ï¼š
    /// - è¿ç§»è®°å½•æ˜¾ç¤º V20260131 å·²å®Œæˆ
    /// - __change_log è¡¨å®é™…ä¸å­˜åœ¨
    /// - verify_migrations é˜¶æ®µ fail-closeï¼Œé˜»å¡æ‰€æœ‰åç»­è¿ç§»
    ///
    /// æ­¤æ–¹æ³•åœ¨ pre_repair é˜¶æ®µç»Ÿä¸€æ£€æµ‹å¹¶ä¿®å¤æ­¤é—®é¢˜ã€‚
    /// V20260131 SQL å…¨éƒ¨ä½¿ç”¨ IF NOT EXISTSï¼Œå¯å®‰å…¨é‡å¤æ‰§è¡Œã€‚
    #[cfg(feature = "data_governance")]
    fn ensure_change_log_table(
        &self,
        conn: &rusqlite::Connection,
        db_name: &str,
        change_log_sql: &str,
        core_table: &str,
    ) -> Result<(), MigrationError> {
        const CHANGE_LOG_VERSION: i32 = 20260131;

        // åœºæ™¯ 1ï¼šè¿ç§»å·²è®°å½•ä½†è¡¨ä¸å­˜åœ¨ï¼ˆDDL å›æ»šæ®‹ç•™ï¼‰
        if self.is_migration_recorded(conn, CHANGE_LOG_VERSION)?
            && !self.table_exists(conn, "__change_log")?
        {
            tracing::info!(
                "ğŸ”§ [PreRepair] {}: V{} å·²è®°å½•ä½† __change_log è¡¨ä¸å­˜åœ¨ï¼Œé‡æ–°æ‰§è¡Œå¹‚ç­‰ SQL",
                db_name,
                CHANGE_LOG_VERSION
            );
            conn.execute_batch(change_log_sql).map_err(|e| {
                MigrationError::Database(format!("é‡æ–°æ‰§è¡Œ {} V20260131 SQL å¤±è´¥: {}", db_name, e))
            })?;
        }

        // åœºæ™¯ 2ï¼šæ ¸å¿ƒè¡¨å­˜åœ¨ä½† __change_log ç¼ºå¤±ï¼ˆæ—§åº“ä»æœªæˆåŠŸæ‰§è¡Œè¿‡ V20260131ï¼‰
        if self.table_exists(conn, core_table)? && !self.table_exists(conn, "__change_log")? {
            tracing::info!(
                "ğŸ”§ [PreRepair] {}: æ ¸å¿ƒè¡¨å­˜åœ¨ä½† __change_log ç¼ºå¤±ï¼Œè¡¥é½",
                db_name
            );
            conn.execute_batch(change_log_sql).map_err(|e| {
                MigrationError::Database(format!("è¡¥é½ {} __change_log è¡¨å¤±è´¥: {}", db_name, e))
            })?;
        }

        Ok(())
    }

    /// ç¡®ä¿ refinery_schema_history å­˜åœ¨ï¼ˆç”¨äºæ‰‹åŠ¨æ ‡è®°è¿ç§»ï¼‰
    #[cfg(feature = "data_governance")]
    fn ensure_refinery_history_table(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        if self.table_exists(conn, "refinery_schema_history")? {
            return Ok(());
        }
        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                version INTEGER PRIMARY KEY,
                name TEXT,
                applied_on TEXT,
                checksum TEXT
            )",
            [],
        )
        .map_err(|e| MigrationError::Database(e.to_string()))?;
        Ok(())
    }

    /// æ·»åŠ åˆ—ï¼ˆè‹¥ç¼ºå¤±ï¼‰
    #[cfg(feature = "data_governance")]
    fn add_column_if_missing(
        &self,
        conn: &rusqlite::Connection,
        table_name: &str,
        column_name: &str,
        column_def: &str,
    ) -> Result<bool, MigrationError> {
        if !self.table_exists(conn, table_name)? {
            return Ok(false);
        }
        if self.column_exists(conn, table_name, column_name)? {
            return Ok(false);
        }
        let sql = format!(
            "ALTER TABLE {} ADD COLUMN {} {}",
            table_name, column_name, column_def
        );
        conn.execute(&sql, []).map_err(|e| {
            MigrationError::Database(format!(
                "ä¸º {} æ·»åŠ  {} åˆ—å¤±è´¥: {}",
                table_name, column_name, e
            ))
        })?;
        Ok(true)
    }

    /// ä»…è¡¥é½ resources/notes/questions/folders çš„ deleted_atï¼ˆé¿å…ä¸è¿ç§»å†²çªï¼‰
    ///
    /// ## deleted_at ç±»å‹è¯´æ˜
    ///
    /// æ‰€æœ‰è¡¨çš„ `deleted_at` ç»Ÿä¸€ä½¿ç”¨ `TEXT`ï¼ˆISO 8601 æ ¼å¼ï¼‰ã€‚
    ///
    /// å†å²è¯´æ˜ï¼šV20260130 init.sql ä¸­ resources è¡¨åŸæœ¬ä½¿ç”¨ INTEGER æ¯«ç§’æ—¶é—´æˆ³ï¼Œ
    /// V20260207 è¿ç§»å·²å°†å…¶ç»Ÿä¸€ä¸º TEXT ç±»å‹ã€‚æ­¤å¤„ pre-repair ä½¿ç”¨ TEXTï¼Œ
    /// å³ä½¿ resources è¡¨å°šæœªæ‰§è¡Œ V20260207ï¼ŒSQLite åŠ¨æ€ç±»å‹ä¹Ÿèƒ½å…¼å®¹ã€‚
    #[cfg(feature = "data_governance")]
    fn ensure_vfs_deleted_at_core(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        // ç»Ÿä¸€ä½¿ç”¨ TEXT ç±»å‹ï¼ˆV20260207 è¿ç§»å°† resources ä» INTEGER æ”¹ä¸º TEXTï¼‰
        let tables_with_deleted_at = ["resources", "notes", "questions", "folders"];

        for table_name in tables_with_deleted_at {
            if self.add_column_if_missing(conn, table_name, "deleted_at", "TEXT")? {
                tracing::info!(
                    "ğŸ”§ [PreRepair] VFS: ä¸º {} è¡¨æ·»åŠ ç¼ºå¤±çš„ deleted_at åˆ— (TEXT)",
                    table_name
                );
            }
        }

        Ok(())
    }

    /// åˆ¤æ–­ V20260201 è¿ç§»æ˜¯å¦ä¼šå› é‡å¤åˆ—è€Œå¤±è´¥
    #[cfg(feature = "data_governance")]
    fn vfs_sync_fields_would_conflict(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<bool, MigrationError> {
        let targets: &[(&str, &[&str])] = &[
            ("resources", &["device_id", "local_version"]),
            ("notes", &["device_id", "local_version"]),
            ("questions", &["device_id", "local_version"]),
            (
                "review_plans",
                &["device_id", "local_version", "deleted_at"],
            ),
            ("folders", &["device_id", "local_version"]),
        ];

        for (table_name, columns) in targets {
            if !self.table_exists(conn, table_name)? {
                continue;
            }
            for column in *columns {
                if self.column_exists(conn, table_name, column)? {
                    return Ok(true);
                }
            }
        }

        Ok(false)
    }

    /// å…¼å®¹å¤„ç† V20260201ï¼šè¡¥é½åˆ—ä¸ç´¢å¼•ï¼Œç„¶åæ ‡è®°è¿ç§»å®Œæˆ
    #[cfg(feature = "data_governance")]
    fn apply_vfs_sync_fields_compat(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        // å…ˆè¡¥é½ deleted_atï¼ˆæ ¸å¿ƒè¡¨ï¼‰
        self.ensure_vfs_deleted_at_core(conn)?;

        // è¡¥é½åŒæ­¥å­—æ®µ
        let _ = self.add_column_if_missing(conn, "resources", "device_id", "TEXT")?;
        let _ =
            self.add_column_if_missing(conn, "resources", "local_version", "INTEGER DEFAULT 0")?;
        let _ = self.add_column_if_missing(conn, "notes", "device_id", "TEXT")?;
        let _ = self.add_column_if_missing(conn, "notes", "local_version", "INTEGER DEFAULT 0")?;
        let _ = self.add_column_if_missing(conn, "questions", "device_id", "TEXT")?;
        let _ =
            self.add_column_if_missing(conn, "questions", "local_version", "INTEGER DEFAULT 0")?;
        let _ = self.add_column_if_missing(conn, "review_plans", "device_id", "TEXT")?;
        let _ =
            self.add_column_if_missing(conn, "review_plans", "local_version", "INTEGER DEFAULT 0")?;
        let _ = self.add_column_if_missing(conn, "review_plans", "deleted_at", "TEXT")?;
        let _ = self.add_column_if_missing(conn, "folders", "device_id", "TEXT")?;
        let _ =
            self.add_column_if_missing(conn, "folders", "local_version", "INTEGER DEFAULT 0")?;

        // åˆ›å»ºç´¢å¼•ï¼ˆå…¨éƒ¨ IF NOT EXISTSï¼Œå®‰å…¨å¹‚ç­‰ï¼‰
        let index_sqls = [
            // resources
            "CREATE INDEX IF NOT EXISTS idx_resources_local_version ON resources(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_resources_device_id ON resources(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_resources_updated_at ON resources(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_resources_device_version ON resources(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_resources_updated_not_deleted ON resources(updated_at) WHERE deleted_at IS NULL",
            // notes
            "CREATE INDEX IF NOT EXISTS idx_notes_local_version ON notes(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_notes_deleted_at_sync ON notes(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_notes_device_id ON notes(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_notes_updated_at ON notes(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_notes_device_version ON notes(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_notes_updated_not_deleted ON notes(updated_at) WHERE deleted_at IS NULL",
            // questions
            "CREATE INDEX IF NOT EXISTS idx_questions_local_version ON questions(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_questions_device_id ON questions(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_questions_updated_at ON questions(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_questions_device_version ON questions(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_questions_updated_not_deleted ON questions(updated_at) WHERE deleted_at IS NULL",
            // review_plans
            "CREATE INDEX IF NOT EXISTS idx_review_plans_local_version ON review_plans(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_review_plans_deleted_at ON review_plans(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_review_plans_device_id ON review_plans(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_review_plans_updated_at ON review_plans(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_review_plans_device_version ON review_plans(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_review_plans_updated_not_deleted ON review_plans(updated_at) WHERE deleted_at IS NULL",
            // folders
            "CREATE INDEX IF NOT EXISTS idx_folders_local_version ON folders(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_folders_device_id ON folders(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_folders_updated_at ON folders(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_folders_device_version ON folders(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_folders_updated_not_deleted ON folders(updated_at) WHERE deleted_at IS NULL",
        ];

        for sql in index_sqls {
            conn.execute(sql, [])
                .map_err(|e| MigrationError::Database(format!("åˆ›å»ºç´¢å¼•å¤±è´¥: {} ({})", sql, e)))?;
        }

        Ok(())
    }

    /// V20260204: PDF å¤„ç†çŠ¶æ€å­—æ®µé¢„ä¿®å¤
    ///
    /// æ£€æŸ¥ files è¡¨çš„ processing_status ç­‰åˆ—æ˜¯å¦å·²å­˜åœ¨ä½†è¿ç§»æœªè®°å½•ï¼Œ
    /// å¦‚æœæ˜¯åˆ™è¡¥é½æ‰€æœ‰åˆ—/ç´¢å¼•å¹¶æ ‡è®°è¿ç§»å®Œæˆã€‚
    #[cfg(feature = "data_governance")]
    fn pre_repair_vfs_v20260204(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260204;

        if !self.table_exists(conn, "files")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä¸€ PDF å¤„ç†å­—æ®µå·²å­˜åœ¨
        if !self.column_exists(conn, "files", "processing_status")? {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] VFS: æ£€æµ‹åˆ° PDF å¤„ç†å­—æ®µæ®‹ç•™ï¼Œè¡¥é½å¹¶æ ‡è®° V{}",
            VERSION
        );

        // è¡¥é½æ‰€æœ‰åˆ—
        let _ = self.add_column_if_missing(
            conn,
            "files",
            "processing_status",
            "TEXT DEFAULT 'pending'",
        )?;
        let _ = self.add_column_if_missing(conn, "files", "processing_progress", "TEXT")?;
        let _ = self.add_column_if_missing(conn, "files", "processing_error", "TEXT")?;
        let _ = self.add_column_if_missing(conn, "files", "processing_started_at", "INTEGER")?;
        let _ = self.add_column_if_missing(conn, "files", "processing_completed_at", "INTEGER")?;

        // è¡¥é½ç´¢å¼•
        let index_sqls: &[&str] = &[
            "CREATE INDEX IF NOT EXISTS idx_files_processing_status ON files(processing_status)",
            "CREATE INDEX IF NOT EXISTS idx_files_pdf_processing ON files(mime_type, processing_status) WHERE mime_type = 'application/pdf'",
            "CREATE INDEX IF NOT EXISTS idx_files_processing_started ON files(processing_started_at) WHERE processing_status IN ('text_extraction', 'page_rendering', 'ocr_processing', 'vector_indexing')",
        ];
        for sql in index_sqls {
            conn.execute(sql, []).map_err(|e| {
                MigrationError::Database(format!("VFS V20260204 ç´¢å¼•åˆ›å»ºå¤±è´¥: {} ({})", sql, e))
            })?;
        }

        // P1-1 ä¿®å¤ï¼šæ‰§è¡Œ V20260204 ä¸­çš„ UPDATE å›å¡«è¯­å¥ï¼ˆå¹‚ç­‰ï¼ŒWHERE æ¡ä»¶ç¡®ä¿ä¸é‡å¤æ›´æ–°ï¼‰
        // å¦‚æœä¸æ‰§è¡Œï¼Œå·²æœ‰ PDF çš„ processing_status ä¼šä¿æŒ 'pending' è€Œéæ ¹æ®å®é™…å†…å®¹è®¾ä¸º 'completed'
        let backfill_sqls: &[&str] = &[
            "UPDATE files SET processing_status = 'completed', processing_progress = '{\"stage\":\"completed\",\"percent\":100,\"ready_modes\":[\"text\",\"image\"]}', processing_completed_at = (strftime('%s', 'now') * 1000) WHERE mime_type = 'application/pdf' AND processing_status = 'pending' AND (preview_json IS NOT NULL OR extracted_text IS NOT NULL)",
            "UPDATE files SET processing_progress = '{\"stage\":\"completed\",\"percent\":100,\"ready_modes\":[\"text\",\"image\",\"ocr\"]}' WHERE mime_type = 'application/pdf' AND processing_status = 'completed' AND ocr_pages_json IS NOT NULL",
        ];
        for sql in backfill_sqls {
            if let Err(e) = conn.execute(sql, []) {
                tracing::warn!(
                    "VFS V20260204 å›å¡« PDF å¤„ç†çŠ¶æ€å¤±è´¥ï¼ˆç»§ç»­ï¼‰: {} ({})",
                    sql,
                    e
                );
            }
        }

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// V20260205: å‹ç¼© blob hash é¢„ä¿®å¤
    #[cfg(feature = "data_governance")]
    fn pre_repair_vfs_v20260205(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260205;

        if !self.table_exists(conn, "files")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }
        if !self.column_exists(conn, "files", "compressed_blob_hash")? {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] VFS: æ£€æµ‹åˆ° compressed_blob_hash æ®‹ç•™ï¼Œæ ‡è®° V{}",
            VERSION
        );

        conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_files_compressed_blob_hash ON files(compressed_blob_hash)",
            [],
        ).map_err(|e| MigrationError::Database(format!("VFS V20260205 ç´¢å¼•åˆ›å»ºå¤±è´¥: {}", e)))?;

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// V20260209: é¢˜ç›®å›¾ç‰‡å­—æ®µé¢„ä¿®å¤
    #[cfg(feature = "data_governance")]
    fn pre_repair_vfs_v20260209(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260209;

        if !self.table_exists(conn, "questions")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }
        if !self.column_exists(conn, "questions", "images_json")? {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] VFS: æ£€æµ‹åˆ° images_json æ®‹ç•™ï¼Œæ ‡è®° V{}",
            VERSION
        );

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// V20260210: ç­”é¢˜æäº¤å­—æ®µé¢„ä¿®å¤
    ///
    /// answer_submissions è¡¨ä½¿ç”¨ CREATE TABLE IF NOT EXISTSï¼ˆå¤©ç„¶å¹‚ç­‰ï¼‰ï¼Œ
    /// ä»…éœ€å¤„ç† questions è¡¨çš„ 3 ä¸ª ALTER TABLE ADD COLUMNã€‚
    #[cfg(feature = "data_governance")]
    fn pre_repair_vfs_v20260210(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260210;

        if !self.table_exists(conn, "questions")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä¸€ AI è¯„åˆ¤å­—æ®µå·²å­˜åœ¨
        let has_any = self.column_exists(conn, "questions", "ai_feedback")?
            || self.column_exists(conn, "questions", "ai_score")?
            || self.column_exists(conn, "questions", "ai_graded_at")?;

        if !has_any {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] VFS: æ£€æµ‹åˆ°ç­”é¢˜æäº¤å­—æ®µæ®‹ç•™ï¼Œè¡¥é½å¹¶æ ‡è®° V{}",
            VERSION
        );

        // è¡¥é½ questions è¡¨åˆ—
        let _ = self.add_column_if_missing(conn, "questions", "ai_feedback", "TEXT")?;
        let _ = self.add_column_if_missing(conn, "questions", "ai_score", "INTEGER")?;
        let _ = self.add_column_if_missing(conn, "questions", "ai_graded_at", "TEXT")?;

        // answer_submissions è¡¨å¤©ç„¶å¹‚ç­‰ï¼ˆCREATE TABLE IF NOT EXISTSï¼‰
        conn.execute_batch(
            "CREATE TABLE IF NOT EXISTS answer_submissions (
                id TEXT PRIMARY KEY NOT NULL,
                question_id TEXT NOT NULL,
                user_answer TEXT NOT NULL,
                is_correct INTEGER,
                grading_method TEXT NOT NULL DEFAULT 'auto',
                submitted_at TEXT NOT NULL,
                FOREIGN KEY (question_id) REFERENCES questions(id)
            );
            CREATE INDEX IF NOT EXISTS idx_submissions_question
                ON answer_submissions(question_id, submitted_at DESC);",
        )
        .map_err(|e| {
            MigrationError::Database(format!("VFS V20260210 answer_submissions åˆ›å»ºå¤±è´¥: {}", e))
        })?;

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// é¢„ä¿®å¤ chat_v2 æ•°æ®åº“çš„ schema
    ///
    /// å¤„ç†å¤šä¸ªç‰ˆæœ¬çš„è¿ç§»æ®‹ç•™ï¼š
    /// - V20260130: æ—§åº“ç¼ºå°‘æ–°å¢è¡¨ï¼ˆsleep_block, subagent_task, workspace_index ç­‰ï¼‰
    /// - V20260201: åŒæ­¥å­—æ®µï¼ˆdevice_id, local_version, updated_at, deleted_atï¼‰
    /// - V20260204: ä¼šè¯åˆ†ç»„ï¼ˆgroup_idï¼‰
    /// - V20260207: active_skill_ids_json
    #[cfg(feature = "data_governance")]
    fn pre_repair_chat_v2_schema(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        // --- V20260130: æ—§åº“è¡¨è¡¥é½ ---
        // æ—§åº“å¯èƒ½åªæœ‰ chat_v2_sessions/messages/blocks ç­‰æ ¸å¿ƒè¡¨ï¼Œ
        // ç¼ºå°‘åç»­æ·»åŠ åˆ° init SQL çš„è¡¨ï¼ˆsleep_block, subagent_task, workspace_index,
        // chat_v2_todo_lists, chat_v2_session_state, resources ç­‰ï¼‰ã€‚
        // V20260130 init SQL å…¨éƒ¨ä½¿ç”¨ CREATE TABLE/INDEX IF NOT EXISTSï¼Œå¤©ç„¶å¹‚ç­‰ï¼Œ
        // å¯å®‰å…¨å›æ”¾è¡¥é½ç¼ºå¤±è¡¨ï¼Œä¸å½±å“å·²æœ‰æ•°æ®ã€‚
        if self.table_exists(conn, "chat_v2_sessions")? {
            conn.execute_batch(include_str!(
                "../../../migrations/chat_v2/V20260130__init.sql"
            ))
            .map_err(|e| {
                MigrationError::Database(format!("å›æ”¾ chat_v2 init è¡¥é½ç¼ºå¤±è¡¨å¤±è´¥: {}", e))
            })?;
        }

        // --- V20260131: __change_log è¡¨ä¿®å¤ï¼ˆé€šç”¨é˜²å¾¡ï¼‰ ---
        self.ensure_change_log_table(
            conn,
            "chat_v2",
            include_str!("../../../migrations/chat_v2/V20260131__add_change_log.sql"),
            "chat_v2_sessions",
        )?;

        // --- V20260201: åŒæ­¥å­—æ®µ ---
        self.pre_repair_chat_v2_v20260201(conn, runner)?;

        // --- V20260204: ä¼šè¯åˆ†ç»„ ---
        self.pre_repair_chat_v2_v20260204(conn, runner)?;

        // --- V20260207: active_skill_ids_json ---
        {
            const TARGET_VERSION: i32 = 20260207;
            const TARGET_COLUMN: &str = "active_skill_ids_json";
            const TARGET_TABLE: &str = "chat_v2_session_state";

            if self.table_exists(conn, TARGET_TABLE)?
                && !self.is_migration_recorded(conn, TARGET_VERSION)?
            {
                // æ—§åº“å…¼å®¹ï¼šä¸»åŠ¨è¡¥é½åˆ—ï¼ˆå¹‚ç­‰ï¼‰ï¼Œç„¶åæ ‡è®°è¿ç§»å®Œæˆ
                let _ = self.add_column_if_missing(
                    conn,
                    TARGET_TABLE,
                    TARGET_COLUMN,
                    "TEXT DEFAULT '[]'",
                )?;
                tracing::info!(
                    "ğŸ”§ [PreRepair] chat_v2: {} åˆ—å·²è¡¥é½ï¼Œæ ‡è®° V{} è¿ç§»ä¸ºå·²å®Œæˆ",
                    TARGET_COLUMN,
                    TARGET_VERSION
                );
                self.ensure_refinery_history_table(conn)?;
                self.mark_migration_complete(conn, runner, TARGET_VERSION)?;
            }
        }

        Ok(())
    }

    /// V20260201: Chat V2 åŒæ­¥å­—æ®µé¢„ä¿®å¤
    ///
    /// å¤„ç† chat_v2_sessions/messages/blocks ä¸‰è¡¨çš„ 11 ä¸ª ALTER TABLE ADD COLUMN
    /// å’Œ 18 ä¸ªç´¢å¼•ã€‚
    ///
    /// ## è§¦å‘åœºæ™¯
    ///
    /// 1. **æ®‹ç•™ä¿®å¤**ï¼šéƒ¨åˆ†åŒæ­¥åˆ—å·²å­˜åœ¨ï¼ˆä¹‹å‰å¤±è´¥çš„è¿ç§»æ®‹ç•™ï¼‰ï¼Œè¡¥é½ç¼ºå¤±éƒ¨åˆ†
    /// 2. **æ—§åº“å…¼å®¹**ï¼šæ—§åº“é€šè¿‡ baseline è·³åˆ°é«˜ç‰ˆæœ¬ï¼ˆå¦‚ V20260207ï¼‰ï¼Œ
    ///    V20260201 ä»æœªæ‰§è¡Œï¼Œä½† verify_migrations ä¼šæ£€æŸ¥å…¶ç´¢å¼•ã€‚
    ///    æ­¤æ—¶ä¸»åŠ¨è¡¥é½æ‰€æœ‰åˆ—å’Œç´¢å¼•ï¼Œé¿å…éªŒè¯å¤±è´¥ã€‚
    #[cfg(feature = "data_governance")]
    fn pre_repair_chat_v2_v20260201(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260201;

        if !self.table_exists(conn, "chat_v2_sessions")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }

        // æ—§åº“å…¼å®¹ï¼šå³ä½¿åŒæ­¥åˆ—éƒ½ä¸å­˜åœ¨ï¼Œåªè¦æ˜¯æ—§åº“ï¼ˆæ ¸å¿ƒè¡¨å­˜åœ¨ä½† V20260201 æœªè®°å½•ï¼‰ï¼Œ
        // ä¹Ÿéœ€è¦ä¸»åŠ¨è¡¥é½æ‰€æœ‰åˆ—å’Œç´¢å¼•ï¼Œå› ä¸º verify_migrations ä¼šæ£€æŸ¥å®ƒä»¬ã€‚
        tracing::info!(
            "ğŸ”§ [PreRepair] chat_v2: è¡¥é½ V{} åŒæ­¥å­—æ®µå’Œç´¢å¼•ï¼ˆæ—§åº“å…¼å®¹/æ®‹ç•™ä¿®å¤ï¼‰",
            VERSION
        );

        // è¡¥é½æ‰€æœ‰åˆ—
        let sync_columns: &[(&str, &str, &str)] = &[
            ("chat_v2_sessions", "device_id", "TEXT"),
            ("chat_v2_sessions", "local_version", "INTEGER DEFAULT 0"),
            ("chat_v2_sessions", "deleted_at", "TEXT"),
            ("chat_v2_messages", "device_id", "TEXT"),
            ("chat_v2_messages", "local_version", "INTEGER DEFAULT 0"),
            ("chat_v2_messages", "updated_at", "TEXT"),
            ("chat_v2_messages", "deleted_at", "TEXT"),
            ("chat_v2_blocks", "device_id", "TEXT"),
            ("chat_v2_blocks", "local_version", "INTEGER DEFAULT 0"),
            ("chat_v2_blocks", "updated_at", "TEXT"),
            ("chat_v2_blocks", "deleted_at", "TEXT"),
        ];

        for (table, col, def) in sync_columns {
            let _ = self.add_column_if_missing(conn, table, col, def)?;
        }

        // è¡¥é½ç´¢å¼•
        let index_sqls: &[&str] = &[
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_local_version ON chat_v2_sessions(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_deleted_at ON chat_v2_sessions(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_device_id ON chat_v2_sessions(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_sync_updated_at ON chat_v2_sessions(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_local_version ON chat_v2_messages(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_deleted_at ON chat_v2_messages(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_device_id ON chat_v2_messages(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_sync_updated_at ON chat_v2_messages(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_local_version ON chat_v2_blocks(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_deleted_at ON chat_v2_blocks(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_device_id ON chat_v2_blocks(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_sync_updated_at ON chat_v2_blocks(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_device_version ON chat_v2_sessions(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_device_version ON chat_v2_messages(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_device_version ON chat_v2_blocks(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_updated_not_deleted ON chat_v2_sessions(updated_at) WHERE deleted_at IS NULL",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_messages_updated_not_deleted ON chat_v2_messages(updated_at) WHERE deleted_at IS NULL",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_blocks_updated_not_deleted ON chat_v2_blocks(updated_at) WHERE deleted_at IS NULL",
        ];

        for sql in index_sqls {
            conn.execute(sql, []).map_err(|e| {
                MigrationError::Database(format!("Chat V2 V20260201 ç´¢å¼•åˆ›å»ºå¤±è´¥: {} ({})", sql, e))
            })?;
        }

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// V20260204: Chat V2 ä¼šè¯åˆ†ç»„é¢„ä¿®å¤
    ///
    /// chat_v2_session_groups è¡¨ä½¿ç”¨ CREATE TABLE IF NOT EXISTSï¼ˆå¤©ç„¶å¹‚ç­‰ï¼‰ï¼Œ
    /// ä»…éœ€å¤„ç† chat_v2_sessions è¡¨çš„ group_id ALTER TABLE ADD COLUMNã€‚
    ///
    /// ## è§¦å‘åœºæ™¯
    ///
    /// 1. **æ®‹ç•™ä¿®å¤**ï¼šgroup_id åˆ—å·²å­˜åœ¨ä½†è¿ç§»æœªè®°å½•
    /// 2. **æ—§åº“å…¼å®¹**ï¼šæ—§åº“ baseline è·³åˆ°é«˜ç‰ˆæœ¬ï¼ŒV20260204 ä»æœªæ‰§è¡Œï¼Œ
    ///    ä¸»åŠ¨è¡¥é½åˆ—å’Œç´¢å¼•é¿å… verify_migrations å¤±è´¥
    #[cfg(feature = "data_governance")]
    fn pre_repair_chat_v2_v20260204(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const VERSION: i32 = 20260204;

        if !self.table_exists(conn, "chat_v2_sessions")? {
            return Ok(());
        }
        if self.is_migration_recorded(conn, VERSION)? {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] chat_v2: è¡¥é½ V{} ä¼šè¯åˆ†ç»„å­—æ®µå’Œç´¢å¼•ï¼ˆæ—§åº“å…¼å®¹/æ®‹ç•™ä¿®å¤ï¼‰",
            VERSION
        );

        // è¡¥é½ group_id åˆ—
        let _ = self.add_column_if_missing(conn, "chat_v2_sessions", "group_id", "TEXT")?;

        // chat_v2_session_groups è¡¨å¤©ç„¶å¹‚ç­‰
        conn.execute_batch(
            "CREATE TABLE IF NOT EXISTS chat_v2_session_groups (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                icon TEXT,
                color TEXT,
                system_prompt TEXT,
                default_skill_ids_json TEXT DEFAULT '[]',
                workspace_id TEXT,
                sort_order INTEGER DEFAULT 0,
                persist_status TEXT DEFAULT 'active',
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            );",
        )
        .map_err(|e| {
            MigrationError::Database(format!("Chat V2 V20260204 session_groups åˆ›å»ºå¤±è´¥: {}", e))
        })?;

        // è¡¥é½ç´¢å¼•
        let index_sqls: &[&str] = &[
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_session_groups_sort_order ON chat_v2_session_groups(sort_order)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_session_groups_status ON chat_v2_session_groups(persist_status)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_session_groups_workspace ON chat_v2_session_groups(workspace_id)",
            "CREATE INDEX IF NOT EXISTS idx_chat_v2_sessions_group_id ON chat_v2_sessions(group_id)",
        ];

        for sql in index_sqls {
            conn.execute(sql, []).map_err(|e| {
                MigrationError::Database(format!("Chat V2 V20260204 ç´¢å¼•åˆ›å»ºå¤±è´¥: {} ({})", sql, e))
            })?;
        }

        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, VERSION)?;
        Ok(())
    }

    /// é¢„ä¿®å¤ mistakes æ•°æ®åº“çš„ schema
    ///
    /// å¤„ç†ä¸¤ç±»å…¸å‹é—®é¢˜ï¼š
    /// 1. æ—§åº“ä¸ V20260130 å¥‘çº¦ä¸ä¸€è‡´ï¼ˆç¼ºè¡¨/ç¼ºåˆ—ï¼‰
    /// 2. preview_data_json å·²å­˜åœ¨ä½† V20260207 æœªè®°å½•ï¼Œå¯¼è‡´ duplicate column
    #[cfg(feature = "data_governance")]
    fn pre_repair_mistakes_schema(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        const SYNC_VERSION: i32 = 20260201;
        const PREVIEW_VERSION: i32 = 20260207;
        const PREVIEW_COLUMN: &str = "preview_data_json";
        const PREVIEW_TABLE: &str = "custom_anki_templates";

        let has_mistakes = self.table_exists(conn, "mistakes")?;

        // æ—§åº“å…¼å®¹ï¼šåªè¦å­˜åœ¨æ ¸å¿ƒè¡¨ï¼Œå°±å…ˆæ‰§è¡Œ V20260130 å¥‘çº¦è¡¥é½ã€‚
        // âš ï¸ å¿…é¡»å…ˆäº ensure_change_log_table æ‰§è¡Œï¼Œå› ä¸º V20260131 çš„ change_log SQL
        //    åŒ…å«å¼•ç”¨ review_analyses ç­‰è¡¨çš„è§¦å‘å™¨ï¼Œè¿™äº›è¡¨ç”± init_compat è¡¥é½ã€‚
        if has_mistakes {
            self.apply_mistakes_init_compat(conn)?;

            // --- V20260131: __change_log è¡¨ä¿®å¤ï¼ˆé€šç”¨é˜²å¾¡ï¼‰ ---
            // æ”¾åœ¨ init_compat ä¹‹åï¼Œç¡®ä¿æ‰€æœ‰è¢«è§¦å‘å™¨å¼•ç”¨çš„è¡¨å·²å­˜åœ¨
            self.ensure_change_log_table(
                conn,
                "mistakes",
                include_str!("../../../migrations/mistakes/V20260131__add_change_log.sql"),
                "mistakes",
            )?;
        } else {
            // æ–°åº“åœºæ™¯ï¼šæ ¸å¿ƒè¡¨ä¸å­˜åœ¨æ—¶ä¹Ÿå°è¯•ä¿®å¤ï¼ˆç”± Refinery æ­£å¸¸åˆ›å»ºè¡¨åè§¦å‘ï¼‰
            self.ensure_change_log_table(
                conn,
                "mistakes",
                include_str!("../../../migrations/mistakes/V20260131__add_change_log.sql"),
                "mistakes",
            )?;
        }

        if has_mistakes {
            // å¯¹æ—§åº“æå‰è¡¥é½ V20260201 åŒæ­¥å­—æ®µä¸ç´¢å¼•ï¼Œé¿å…åç»­è¿ç§»å› é‡å¤åˆ—æˆ–ç¼ºåˆ—å¤±è´¥ã€‚
            self.apply_mistakes_sync_fields_compat(conn)?;
            if !self.is_migration_recorded(conn, SYNC_VERSION)? {
                self.ensure_refinery_history_table(conn)?;
                tracing::info!(
                    "ğŸ”§ [PreRepair] mistakes: sync å­—æ®µå·²è¡¥é½ï¼Œæ ‡è®° V{} è¿ç§»ä¸ºå·²å®Œæˆ",
                    SYNC_VERSION
                );
                self.mark_migration_complete(conn, runner, SYNC_VERSION)?;
            }
        }

        // å¤„ç† V20260207 é‡å¤åˆ—é—®é¢˜ï¼ˆä»… legacy è·¯å¾„ï¼‰ã€‚
        // æ–°åº“ä¸åº”æå‰å†™å…¥é«˜ç‰ˆæœ¬è¿ç§»è®°å½•ï¼Œå¦åˆ™ä¼šè·³è¿‡ init è¿ç§»ã€‚
        if has_mistakes && self.table_exists(conn, PREVIEW_TABLE)? {
            let _ = self.add_column_if_missing(conn, PREVIEW_TABLE, PREVIEW_COLUMN, "TEXT")?;

            if !self.is_migration_recorded(conn, PREVIEW_VERSION)?
                && self.column_exists(conn, PREVIEW_TABLE, PREVIEW_COLUMN)?
            {
                self.ensure_refinery_history_table(conn)?;
                tracing::info!(
                    "ğŸ”§ [PreRepair] mistakes: {} å·²å°±ç»ªï¼Œæ ‡è®° V{} è¿ç§»ä¸ºå·²å®Œæˆ",
                    PREVIEW_COLUMN,
                    PREVIEW_VERSION
                );
                self.mark_migration_complete(conn, runner, PREVIEW_VERSION)?;
            }
        }

        Ok(())
    }

    #[cfg(feature = "data_governance")]
    fn apply_mistakes_init_compat(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        // æ—§åº“å¯èƒ½åªä¿ç•™äº†éƒ¨åˆ†åˆ—ï¼›init.sql åœ¨ååŠæ®µä¼šåˆ›å»ºç´¢å¼•/è§¦å‘å™¨ã€‚
        // å…ˆè¡¥é½â€œè¢«ç´¢å¼•/è§¦å‘å™¨å¼•ç”¨â€çš„å…³é”®åˆ—ï¼Œé¿å…å›æ”¾ init æ—¶å› ç¼ºåˆ—å¤±è´¥ã€‚
        let index_and_trigger_columns: &[(&str, &str, &str)] = &[
            ("mistakes", "irec_card_id", "TEXT"),
            ("mistakes", "updated_at", "TEXT"),
            ("chat_messages", "turn_id", "TEXT"),
            ("chat_messages", "mistake_id", "TEXT"),
            ("document_tasks", "document_id", "TEXT"),
            ("document_tasks", "status", "TEXT"),
            ("anki_cards", "task_id", "TEXT"),
            ("anki_cards", "is_error_card", "INTEGER NOT NULL DEFAULT 0"),
            ("anki_cards", "source_type", "TEXT NOT NULL DEFAULT ''"),
            ("anki_cards", "source_id", "TEXT NOT NULL DEFAULT ''"),
            ("anki_cards", "updated_at", "TEXT"),
            ("anki_cards", "text", "TEXT"),
            ("review_analyses", "updated_at", "TEXT"),
            (
                "custom_anki_templates",
                "is_active",
                "INTEGER NOT NULL DEFAULT 1",
            ),
            (
                "custom_anki_templates",
                "is_built_in",
                "INTEGER NOT NULL DEFAULT 0",
            ),
            ("document_control_states", "document_id", "TEXT"),
            ("document_control_states", "state", "TEXT"),
            (
                "document_control_states",
                "updated_at",
                "TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP",
            ),
            ("vectorized_data", "mistake_id", "TEXT"),
            ("review_session_mistakes", "session_id", "TEXT"),
            ("review_session_mistakes", "mistake_id", "TEXT"),
            (
                "search_logs",
                "created_at",
                "TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP",
            ),
            ("search_logs", "search_type", "TEXT"),
            ("exam_sheet_sessions", "status", "TEXT"),
        ];

        for (table_name, column_name, column_def) in index_and_trigger_columns {
            let _ = self.add_column_if_missing(conn, table_name, column_name, column_def)?;
        }

        // æ—§åº“ä¸­å¯èƒ½ç¼ºå°‘è¿è¡Œæ—¶æŸ¥è¯¢ä¾èµ–åˆ—ï¼Œæå‰è¡¥é½ä»¥æ»¡è¶³è¯­ä¹‰éªŒè¯ã€‚
        let runtime_compat_columns: &[(&str, &str, &str)] = &[
            ("mistakes", "mistake_summary", "TEXT"),
            ("mistakes", "user_error_analysis", "TEXT"),
            ("mistakes", "irec_status", "INTEGER DEFAULT 0"),
            ("chat_messages", "graph_sources", "TEXT"),
            ("chat_messages", "turn_seq", "SMALLINT"),
            ("chat_messages", "reply_to_msg_id", "INTEGER"),
            ("chat_messages", "message_kind", "TEXT"),
            ("chat_messages", "lifecycle", "TEXT"),
            ("chat_messages", "metadata", "TEXT"),
            ("review_chat_messages", "web_search_sources", "TEXT"),
            ("review_chat_messages", "tool_call", "TEXT"),
            ("review_chat_messages", "tool_result", "TEXT"),
            ("review_chat_messages", "overrides", "TEXT"),
            ("review_chat_messages", "relations", "TEXT"),
        ];

        for (table_name, column_name, column_def) in runtime_compat_columns {
            let _ = self.add_column_if_missing(conn, table_name, column_name, column_def)?;
        }

        // å›æ”¾ initï¼Œè¡¥é½ç¼ºå¤±è¡¨/ç´¢å¼•/è§¦å‘å™¨
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260130__init.sql"
        ))
        .map_err(|e| MigrationError::Database(format!("å›æ”¾ mistakes init å¤±è´¥: {}", e)))?;

        // æ—§åº“åœ¨ baseline è¢«è·³è¿‡æ—¶ï¼Œå¯èƒ½ç¼ºå¤± change_log è¡¨ï¼›è¯¥è„šæœ¬å¹‚ç­‰ï¼Œå¯å®‰å…¨å›æ”¾ã€‚
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260131__add_change_log.sql"
        ))
        .map_err(|e| {
            MigrationError::Database(format!("å›æ”¾ mistakes add_change_log å¤±è´¥: {}", e))
        })?;

        // å†æ¬¡å…œåº• text åˆ—åŠç´¢å¼•ï¼Œç¡®ä¿ä¿®å¤å¹‚ç­‰ä¸”å¯é‡å…¥
        let _ = self.add_column_if_missing(conn, "anki_cards", "text", "TEXT")?;
        conn.execute(
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_text ON anki_cards(text)",
            [],
        )
        .map_err(|e| MigrationError::Database(format!("åˆ›å»º idx_anki_cards_text å¤±è´¥: {}", e)))?;

        Ok(())
    }

    /// å¯¹ mistakes V20260201 åŒæ­¥å­—æ®µè¿›è¡Œå…¼å®¹è¡¥é½ï¼ˆå¹‚ç­‰ï¼‰ã€‚
    #[cfg(feature = "data_governance")]
    fn apply_mistakes_sync_fields_compat(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        let sync_columns: &[(&str, &str, &str)] = &[
            ("mistakes", "device_id", "TEXT"),
            ("mistakes", "local_version", "INTEGER DEFAULT 0"),
            ("mistakes", "deleted_at", "TEXT"),
            ("anki_cards", "device_id", "TEXT"),
            ("anki_cards", "local_version", "INTEGER DEFAULT 0"),
            ("anki_cards", "deleted_at", "TEXT"),
            ("review_analyses", "device_id", "TEXT"),
            ("review_analyses", "local_version", "INTEGER DEFAULT 0"),
            ("review_analyses", "deleted_at", "TEXT"),
        ];

        for (table_name, column_name, column_def) in sync_columns {
            let _ = self.add_column_if_missing(conn, table_name, column_name, column_def)?;
        }

        let sync_index_sqls: &[&str] = &[
            "CREATE INDEX IF NOT EXISTS idx_mistakes_local_version ON mistakes(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_mistakes_deleted_at ON mistakes(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_mistakes_device_id ON mistakes(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_mistakes_updated_at ON mistakes(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_local_version ON anki_cards(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_deleted_at ON anki_cards(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_device_id ON anki_cards(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_updated_at ON anki_cards(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_local_version ON review_analyses(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_deleted_at ON review_analyses(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_device_id ON review_analyses(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_updated_at ON review_analyses(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_mistakes_device_version ON mistakes(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_device_version ON anki_cards(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_device_version ON review_analyses(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_mistakes_updated_not_deleted ON mistakes(updated_at) WHERE deleted_at IS NULL",
            "CREATE INDEX IF NOT EXISTS idx_anki_cards_updated_not_deleted ON anki_cards(updated_at) WHERE deleted_at IS NULL",
            "CREATE INDEX IF NOT EXISTS idx_review_analyses_updated_not_deleted ON review_analyses(updated_at) WHERE deleted_at IS NULL",
        ];

        for sql in sync_index_sqls {
            conn.execute(sql, []).map_err(|e| {
                MigrationError::Database(format!("æ‰§è¡ŒåŒæ­¥ç´¢å¼• SQL å¤±è´¥: {} ({})", sql, e))
            })?;
        }

        Ok(())
    }

    /// é¢„ä¿®å¤ LLM Usage æ•°æ®åº“çš„ schema
    ///
    /// å¤„ç†ä¸¤ç±»é—®é¢˜ï¼š
    /// 1. V20260131: `__change_log` è¡¨è¢«è®°å½•ä¸ºå·²å®Œæˆä½†å®é™…ä¸å­˜åœ¨
    ///    ï¼ˆæ—§ç‰ˆ set_grouped(true) æ—¶ä»£ SQLite DDL å›æ»šæ®‹ç•™ï¼‰
    /// 2. V20260201: åŒæ­¥å­—æ®µè¿ç§»å¤±è´¥åçš„æ®‹ç•™çŠ¶æ€
    #[cfg(feature = "data_governance")]
    fn pre_repair_llm_usage_schema(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
    ) -> Result<(), MigrationError> {
        // --- V20260131: __change_log è¡¨ä¿®å¤ï¼ˆé€šç”¨é˜²å¾¡ï¼‰ ---
        self.ensure_change_log_table(
            conn,
            "llm_usage",
            include_str!("../../../migrations/llm_usage/V20260131__add_change_log.sql"),
            "llm_usage_logs",
        )?;

        const SYNC_VERSION: i32 = 20260201;

        // æ–°æ•°æ®åº“ï¼ˆå°šæœªåˆ›å»ºè¡¨ï¼‰æ— éœ€é¢„ä¿®å¤
        if !self.table_exists(conn, "llm_usage_logs")? {
            return Ok(());
        }

        // å¦‚æœè¿ç§»å·²è®°å½•ï¼Œæ— éœ€å¤„ç†
        if self.is_migration_recorded(conn, SYNC_VERSION)? {
            return Ok(());
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä¸€åŒæ­¥å­—æ®µå·²å­˜åœ¨ï¼ˆè¯´æ˜éƒ¨åˆ†è¿ç§»æ®‹ç•™ï¼‰
        let has_any_sync_field = self.column_exists(conn, "llm_usage_logs", "device_id")?
            || self.column_exists(conn, "llm_usage_logs", "local_version")?
            || self.column_exists(conn, "llm_usage_daily", "device_id")?;

        if !has_any_sync_field {
            return Ok(());
        }

        tracing::info!(
            "ğŸ”§ [PreRepair] llm_usage: æ£€æµ‹åˆ°åŒæ­¥å­—æ®µæ®‹ç•™ï¼Œè¡¥é½å¹¶æ ‡è®° V{}",
            SYNC_VERSION
        );

        // è¡¥é½æ‰€æœ‰åˆ—ï¼ˆå¹‚ç­‰ï¼‰
        let sync_columns: &[(&str, &str, &str)] = &[
            ("llm_usage_logs", "device_id", "TEXT"),
            ("llm_usage_logs", "local_version", "INTEGER DEFAULT 0"),
            ("llm_usage_logs", "updated_at", "TEXT"),
            ("llm_usage_logs", "deleted_at", "TEXT"),
            ("llm_usage_daily", "device_id", "TEXT"),
            ("llm_usage_daily", "local_version", "INTEGER DEFAULT 0"),
            ("llm_usage_daily", "deleted_at", "TEXT"),
        ];

        for (table, col, def) in sync_columns {
            let _ = self.add_column_if_missing(conn, table, col, def)?;
        }

        // è¡¥é½ç´¢å¼• â€” llm_usage_logsï¼ˆè¡¨å·²ç¡®è®¤å­˜åœ¨ï¼‰
        let logs_index_sqls: &[&str] = &[
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_local_version ON llm_usage_logs(local_version)",
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_deleted_at ON llm_usage_logs(deleted_at)",
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_device_id ON llm_usage_logs(device_id)",
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_updated_at ON llm_usage_logs(updated_at)",
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_device_version ON llm_usage_logs(device_id, local_version)",
            "CREATE INDEX IF NOT EXISTS idx_llm_usage_logs_updated_not_deleted ON llm_usage_logs(updated_at) WHERE deleted_at IS NULL",
        ];

        for sql in logs_index_sqls {
            conn.execute(sql, []).map_err(|e| {
                MigrationError::Database(format!("LLM Usage ç´¢å¼•åˆ›å»ºå¤±è´¥: {} ({})", sql, e))
            })?;
        }

        // è¡¥é½ç´¢å¼• â€” llm_usage_dailyï¼ˆéœ€å…ˆç¡®è®¤è¡¨å­˜åœ¨ï¼Œéƒ¨åˆ†å¤±è´¥åœºæ™¯ä¸‹å¯èƒ½åªæœ‰ logs è¡¨ï¼‰
        if self.table_exists(conn, "llm_usage_daily")? {
            let daily_index_sqls: &[&str] = &[
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_local_version ON llm_usage_daily(local_version)",
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_deleted_at ON llm_usage_daily(deleted_at)",
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_device_id ON llm_usage_daily(device_id)",
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_updated_at ON llm_usage_daily(updated_at)",
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_device_version ON llm_usage_daily(device_id, local_version)",
                "CREATE INDEX IF NOT EXISTS idx_llm_usage_daily_updated_not_deleted ON llm_usage_daily(updated_at) WHERE deleted_at IS NULL",
            ];

            for sql in daily_index_sqls {
                conn.execute(sql, []).map_err(|e| {
                    MigrationError::Database(format!("LLM Usage ç´¢å¼•åˆ›å»ºå¤±è´¥: {} ({})", sql, e))
                })?;
            }
        }

        // æ ‡è®°è¿ç§»å®Œæˆ
        self.ensure_refinery_history_table(conn)?;
        self.mark_migration_complete(conn, runner, SYNC_VERSION)?;

        Ok(())
    }

    fn is_migration_recorded(
        &self,
        conn: &rusqlite::Connection,
        version: i32,
    ) -> Result<bool, MigrationError> {
        let exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM refinery_schema_history WHERE version = ?1)",
                [version],
                |row| row.get(0),
            )
            .unwrap_or(false);
        Ok(exists)
    }

    /// æ‰‹åŠ¨æ ‡è®°è¿ç§»ä¸ºå·²å®Œæˆ
    ///
    /// ä» Runner ä¸­è·å–è¿ç§»ä¿¡æ¯ï¼Œæ’å…¥åˆ° refinery_schema_history è¡¨ã€‚
    #[cfg(feature = "data_governance")]
    fn mark_migration_complete(
        &self,
        conn: &rusqlite::Connection,
        runner: &refinery::Runner,
        target_version: i32,
    ) -> Result<(), MigrationError> {
        // ä» runner ä¸­æ‰¾åˆ°å¯¹åº”çš„è¿ç§»
        for migration in runner.get_migrations() {
            if migration.version() == target_version {
                let now = chrono::Utc::now().to_rfc3339();
                conn.execute(
                    "INSERT OR IGNORE INTO refinery_schema_history (version, name, applied_on, checksum)
                     VALUES (?1, ?2, ?3, ?4)",
                    rusqlite::params![
                        target_version,
                        migration.name(),
                        now,
                        migration.checksum().to_string(),
                    ],
                )
                .map_err(|e| MigrationError::Database(format!(
                    "æ ‡è®°è¿ç§» V{} ä¸ºå·²å®Œæˆå¤±è´¥: {}",
                    target_version, e
                )))?;

                tracing::info!(
                    "âœ… [PreRepair] å·²æ ‡è®°è¿ç§» V{}_{} ä¸ºå·²å®Œæˆ",
                    target_version,
                    migration.name()
                );
                return Ok(());
            }
        }

        tracing::warn!(
            "âš ï¸ [PreRepair] æœªæ‰¾åˆ°ç‰ˆæœ¬ {} çš„è¿ç§»å®šä¹‰ï¼Œè·³è¿‡æ ‡è®°",
            target_version
        );
        Ok(())
    }

    /// æ¸…ç†ä¸­é—´çŠ¶æ€çš„ä¸´æ—¶è¡¨
    ///
    /// åœ¨è¿ç§»å¤±è´¥æ—¶ï¼Œå¯èƒ½ä¼šé—ç•™ `*_new` å½¢å¼çš„ä¸­é—´è¡¨ã€‚
    /// æ­¤æ–¹æ³•åœ¨è¿ç§»å‰æ£€æµ‹å¹¶æ¸…ç†è¿™äº›è¡¨ï¼Œç¡®ä¿è¿ç§»å¯ä»¥é‡æ–°æ‰§è¡Œã€‚
    ///
    /// # å®‰å…¨è¯´æ˜
    /// - åªæ¸…ç†å·²çŸ¥çš„ä¸­é—´è¡¨æ¨¡å¼ï¼ˆå¦‚ `xxx_new`ï¼‰
    /// - åªåœ¨ `refinery_schema_history` ä¸­æ²¡æœ‰å¯¹åº”ç‰ˆæœ¬è®°å½•æ—¶æ‰æ¸…ç†
    fn cleanup_intermediate_tables(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
    ) -> Result<(), MigrationError> {
        // å®šä¹‰å„æ•°æ®åº“å¯èƒ½å­˜åœ¨çš„ä¸­é—´è¡¨
        let intermediate_tables: &[&str] = match id {
            DatabaseId::Vfs => &[
                "vfs_index_segments_new",
                "vfs_index_units_new",
                "vfs_blobs_new",
            ],
            DatabaseId::ChatV2 => &["messages_new", "variants_new", "sessions_new"],
            DatabaseId::Mistakes => &["mistakes_new"],
            DatabaseId::LlmUsage => &["llm_usage_new"],
        };

        for table_name in intermediate_tables {
            // æ£€æŸ¥ä¸­é—´è¡¨æ˜¯å¦å­˜åœ¨
            let table_exists: bool = conn
                .query_row(
                    "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name=?1)",
                    [table_name],
                    |row| row.get(0),
                )
                .unwrap_or(false);

            if table_exists {
                tracing::warn!(
                    database = id.as_str(),
                    table = table_name,
                    "æ£€æµ‹åˆ°ä¸­é—´çŠ¶æ€è¡¨ï¼ˆå¯èƒ½æ¥è‡ªå¤±è´¥çš„è¿ç§»ï¼‰ï¼Œæ­£åœ¨æ¸…ç†..."
                );

                // åˆ é™¤ä¸­é—´è¡¨
                if let Err(e) = conn.execute(&format!("DROP TABLE IF EXISTS {}", table_name), []) {
                    tracing::warn!(
                        database = id.as_str(),
                        table = table_name,
                        error = %e,
                        "æ¸…ç†ä¸­é—´çŠ¶æ€è¡¨å¤±è´¥ï¼Œç»§ç»­è¿ç§»æµç¨‹"
                    );
                } else {
                    tracing::info!(
                        database = id.as_str(),
                        table = table_name,
                        "æˆåŠŸæ¸…ç†ä¸­é—´çŠ¶æ€è¡¨"
                    );
                }
            }
        }

        Ok(())
    }

    /// éªŒè¯è¿ç§»ç»“æœ
    ///
    /// ä½¿ç”¨ MigrationVerifier æ£€æŸ¥è¡¨ã€åˆ—ã€ç´¢å¼•æ˜¯å¦æ­£ç¡®åˆ›å»ºã€‚
    fn verify_migrations(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
        migration_set: &MigrationSet,
        current_version: u32,
    ) -> Result<(), MigrationError> {
        // éªŒè¯æ‰€æœ‰å·²åº”ç”¨çš„è¿ç§»
        // æ³¨æ„ï¼šcurrent_version æ˜¯ Refinery è®°å½•çš„ç‰ˆæœ¬ï¼ˆå¦‚ 20260130ï¼‰
        for migration in migration_set.migrations.iter() {
            if migration.refinery_version <= current_version as i32 {
                MigrationVerifier::verify(conn, migration)?;
            }
        }

        let allow_rebaseline = migration_set
            .get(current_version as i32)
            .map(|m| m.idempotent)
            .unwrap_or(false);
        self.verify_schema_fingerprint(conn, id, current_version, allow_rebaseline)?;

        tracing::debug!(
            database = migration_set.database_name,
            version = current_version,
            "Migration verification passed"
        );

        Ok(())
    }

    /// éªŒè¯å¹¶è®°å½• schema fingerprintã€‚
    ///
    /// åŒç‰ˆæœ¬ä¸‹ fingerprint ä¸ä¸€è‡´è¯´æ˜å‘ç”Ÿäº†â€œè®°å½•-äº‹å®â€æ¼‚ç§»ï¼Œç›´æ¥ fail-closeã€‚
    fn verify_schema_fingerprint(
        &self,
        conn: &rusqlite::Connection,
        id: &DatabaseId,
        schema_version: u32,
        allow_rebaseline: bool,
    ) -> Result<(), MigrationError> {
        if schema_version == 0 {
            return Ok(());
        }

        self.ensure_schema_fingerprint_table(conn)?;
        let (current_fingerprint, canonical_schema) = self.compute_schema_fingerprint(conn)?;

        let select_sql = format!(
            "SELECT fingerprint FROM {} WHERE database_id = ?1 AND schema_version = ?2",
            SCHEMA_FINGERPRINT_TABLE
        );
        let existing: Option<String> = conn
            .query_row(
                &select_sql,
                rusqlite::params![id.as_str(), schema_version as i64],
                |row| row.get(0),
            )
            .optional()
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        if let Some(stored) = existing {
            if stored != current_fingerprint {
                if allow_rebaseline {
                    tracing::warn!(
                        database = id.as_str(),
                        version = schema_version,
                        "Schema fingerprint drift detected, rebaseline enabled"
                    );
                } else {
                    return Err(MigrationError::VerificationFailed {
                        version: schema_version,
                        reason: format!(
                            "Schema fingerprint drift detected at v{} (db: {}). \
                             Use the canonical_schema column in {} to diff the expected vs actual schema.",
                            schema_version,
                            id.as_str(),
                            SCHEMA_FINGERPRINT_TABLE,
                        ),
                    });
                }
            }

            // æ›´æ–° verified_atã€fingerprint å’Œ canonical_schema
            let update_sql = format!(
                "UPDATE {} SET verified_at = ?3, fingerprint = ?4, canonical_schema = ?5 WHERE database_id = ?1 AND schema_version = ?2",
                SCHEMA_FINGERPRINT_TABLE
            );
            conn.execute(
                &update_sql,
                rusqlite::params![
                    id.as_str(),
                    schema_version as i64,
                    chrono::Utc::now().to_rfc3339(),
                    current_fingerprint,
                    canonical_schema,
                ],
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;
            return Ok(());
        }

        // Issue #12: åŒæ—¶å­˜å‚¨ fingerprint hash å’Œå¯è¯»çš„ canonical schema
        let insert_sql = format!(
            "INSERT INTO {} (database_id, schema_version, fingerprint, verified_at, canonical_schema) VALUES (?1, ?2, ?3, ?4, ?5)",
            SCHEMA_FINGERPRINT_TABLE
        );
        conn.execute(
            &insert_sql,
            rusqlite::params![
                id.as_str(),
                schema_version as i64,
                current_fingerprint,
                chrono::Utc::now().to_rfc3339(),
                canonical_schema,
            ],
        )
        .map_err(|e| MigrationError::Database(e.to_string()))?;

        Ok(())
    }

    fn ensure_schema_fingerprint_table(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(), MigrationError> {
        let create_sql = format!(
            r#"CREATE TABLE IF NOT EXISTS {} (
                database_id TEXT NOT NULL,
                schema_version INTEGER NOT NULL,
                fingerprint TEXT NOT NULL,
                verified_at TEXT NOT NULL,
                PRIMARY KEY (database_id, schema_version)
            )"#,
            SCHEMA_FINGERPRINT_TABLE
        );
        conn.execute(&create_sql, [])
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        // Issue #12: æ·»åŠ  canonical_schema åˆ—å­˜å‚¨ç»“æ„åŒ– schema æ–‡æœ¬ï¼ˆå¯è¯»ï¼Œä¾¿äºè°ƒè¯•æ¼‚ç§»ï¼‰
        // ä½¿ç”¨ ALTER TABLE ... ADD COLUMNï¼Œå¯¹å·²æœ‰è¡¨å®‰å…¨
        let alter_sql = format!(
            "ALTER TABLE {} ADD COLUMN canonical_schema TEXT",
            SCHEMA_FINGERPRINT_TABLE
        );
        // åˆ—å·²å­˜åœ¨æ—¶ SQLite è¿”å› "duplicate column" é”™è¯¯ï¼Œå¿½ç•¥å³å¯
        // ä½†å…¶ä»–é”™è¯¯ï¼ˆç£ç›˜æ»¡ã€æƒé™ä¸è¶³ç­‰ï¼‰åº”è®°å½•è­¦å‘Š
        if let Err(e) = conn.execute(&alter_sql, []) {
            let err_msg = e.to_string();
            if !err_msg.contains("duplicate column") {
                tracing::warn!(
                    error = %e,
                    "Failed to add canonical_schema column to {} (non-duplicate error)",
                    SCHEMA_FINGERPRINT_TABLE
                );
            }
        }

        Ok(())
    }

    /// è®¡ç®— schema fingerprint
    ///
    /// è¿”å› `(fingerprint_hash, canonical_schema_text)` å…ƒç»„ã€‚
    /// - `fingerprint_hash`: SHA256 hashï¼ˆç”¨äºå¿«é€Ÿæ¯”è¾ƒï¼‰
    /// - `canonical_schema_text`: ç»“æ„åŒ– schema æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•æ¼‚ç§»åŸå› ï¼‰
    ///
    /// ## Issue #12 æ”¹è¿›
    ///
    /// ä¹‹å‰ä»…è¿”å› hashï¼Œæ— æ³•ç¡®å®šæ¼‚ç§»å‘ç”Ÿåœ¨å“ªä¸ªè¡¨/åˆ—ã€‚
    /// ç°åœ¨åŒæ—¶ä¿ç•™ canonical æ–‡æœ¬ï¼Œæ¼‚ç§»å‘ç”Ÿæ—¶å¯é€šè¿‡ diff å¿«é€Ÿå®šä½ã€‚
    fn compute_schema_fingerprint(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<(String, String), MigrationError> {
        let mut canonical = String::new();

        let tables_sql = format!(
            r#"SELECT name FROM sqlite_master
               WHERE type='table'
                 AND name NOT LIKE 'sqlite_%'
                 AND name != 'refinery_schema_history'
                 AND name != '{}'
               ORDER BY name"#,
            SCHEMA_FINGERPRINT_TABLE
        );

        let mut tables_stmt = conn
            .prepare(&tables_sql)
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        let tables = tables_stmt
            .query_map([], |row| row.get::<_, String>(0))
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        for table in tables {
            let table = table.map_err(|e| MigrationError::Database(e.to_string()))?;
            canonical.push_str("table:");
            canonical.push_str(&table);
            canonical.push('\n');

            let escaped_table = table.replace('\'', "''");
            let pragma_sql = format!("PRAGMA table_info('{}')", escaped_table);
            let mut columns_stmt = conn
                .prepare(&pragma_sql)
                .map_err(|e| MigrationError::Database(e.to_string()))?;
            let columns = columns_stmt
                .query_map([], |row| {
                    Ok((
                        row.get::<_, i32>(0)?,
                        row.get::<_, String>(1)?,
                        row.get::<_, Option<String>>(2)?.unwrap_or_default(),
                        row.get::<_, i32>(3)?,
                        row.get::<_, Option<String>>(4)?.unwrap_or_default(),
                        row.get::<_, i32>(5)?,
                    ))
                })
                .map_err(|e| MigrationError::Database(e.to_string()))?;

            for column in columns {
                let (cid, name, ty, not_null, default_val, pk) =
                    column.map_err(|e| MigrationError::Database(e.to_string()))?;
                canonical.push_str(&format!(
                    "col:{}:{}:{}:{}:{}:{}\n",
                    cid, name, ty, not_null, default_val, pk
                ));
            }

            let mut indexes_stmt = conn
                .prepare(
                    "SELECT name, IFNULL(sql, '') FROM sqlite_master                     WHERE type='index' AND tbl_name = ?1 AND name NOT LIKE 'sqlite_autoindex%'                     ORDER BY name",
                )
                .map_err(|e| MigrationError::Database(e.to_string()))?;
            let indexes = indexes_stmt
                .query_map([table.as_str()], |row| {
                    Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?))
                })
                .map_err(|e| MigrationError::Database(e.to_string()))?;

            for index in indexes {
                let (name, sql) = index.map_err(|e| MigrationError::Database(e.to_string()))?;
                canonical.push_str(&format!("idx:{}:{}\n", name, sql));
            }

            let mut triggers_stmt = conn
                .prepare(
                    "SELECT name, IFNULL(sql, '') FROM sqlite_master                     WHERE type='trigger' AND tbl_name = ?1                     ORDER BY name",
                )
                .map_err(|e| MigrationError::Database(e.to_string()))?;
            let triggers = triggers_stmt
                .query_map([table.as_str()], |row| {
                    Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?))
                })
                .map_err(|e| MigrationError::Database(e.to_string()))?;

            for trigger in triggers {
                let (name, sql) = trigger.map_err(|e| MigrationError::Database(e.to_string()))?;
                canonical.push_str(&format!("trg:{}:{}\n", name, sql));
            }
        }

        let mut hasher = Sha256::new();
        hasher.update(canonical.as_bytes());
        let fingerprint = format!("{:x}", hasher.finalize());

        Ok((fingerprint, canonical))
    }

    /// è®°å½•è¿ç§»å®¡è®¡æ—¥å¿—
    fn log_migration_audit(
        &self,
        id: &DatabaseId,
        from_version: u32,
        to_version: u32,
        applied_count: usize,
        duration_ms: u64,
    ) -> Result<(), MigrationError> {
        use crate::data_governance::audit::AuditRepository;

        // å¦‚æœæ²¡æœ‰é…ç½®å®¡è®¡æ•°æ®åº“ï¼Œä»…è®°å½•æ—¥å¿—
        let Some(audit_db_path) = &self.audit_db_path else {
            tracing::debug!(
                database = id.as_str(),
                from_version = from_version,
                to_version = to_version,
                applied_count = applied_count,
                "Migration audit (no audit db configured)"
            );
            return Ok(());
        };

        // å°è¯•æ‰“å¼€å®¡è®¡æ•°æ®åº“å¹¶å†™å…¥æ—¥å¿—
        match rusqlite::Connection::open(audit_db_path) {
            Ok(conn) => {
                // ç¡®ä¿å®¡è®¡è¡¨å­˜åœ¨
                if let Err(e) = AuditRepository::init(&conn) {
                    tracing::warn!(
                        error = %e,
                        "Failed to init audit table, skipping audit log"
                    );
                    return Ok(()); // ä¸å½±å“è¿ç§»
                }

                // å†™å…¥å®¡è®¡æ—¥å¿—
                match AuditRepository::log_migration_complete(
                    &conn,
                    id.as_str(),
                    from_version,
                    to_version,
                    applied_count,
                    duration_ms,
                ) {
                    Ok(audit_id) => {
                        tracing::info!(
                            database = id.as_str(),
                            from_version = from_version,
                            to_version = to_version,
                            applied_count = applied_count,
                            audit_id = %audit_id,
                            "Migration audit log saved to database"
                        );
                    }
                    Err(e) => {
                        tracing::warn!(
                            error = %e,
                            database = id.as_str(),
                            "Failed to save migration audit log"
                        );
                    }
                }
            }
            Err(e) => {
                tracing::warn!(
                    error = %e,
                    path = %audit_db_path.display(),
                    "Failed to open audit database for logging"
                );
            }
        }

        Ok(())
    }

    /// è®°å½•è¿ç§»å¤±è´¥å®¡è®¡æ—¥å¿—
    fn log_migration_failure(
        &self,
        id: &DatabaseId,
        from_version: u32,
        error_message: &str,
        duration_ms: u64,
    ) {
        use crate::data_governance::audit::{AuditLog, AuditOperation, AuditRepository};

        let Some(audit_db_path) = &self.audit_db_path else {
            tracing::warn!(
                database = id.as_str(),
                error = error_message,
                "Migration failed (no audit db configured)"
            );
            return;
        };

        let mut log = AuditLog::new(
            AuditOperation::Migration {
                from_version,
                to_version: from_version,
                applied_count: 0,
            },
            id.as_str(),
        )
        .fail(error_message.to_string())
        .with_details(serde_json::json!({
            "database": id.as_str(),
            "from_version": from_version,
            "error": error_message,
        }));
        log.duration_ms = Some(duration_ms);

        match rusqlite::Connection::open(audit_db_path) {
            Ok(conn) => {
                if let Err(e) = AuditRepository::init(&conn) {
                    tracing::warn!(
                        error = %e,
                        "Failed to init audit table for migration failure"
                    );
                    return;
                }
                if let Err(e) = AuditRepository::save(&conn, &log) {
                    tracing::warn!(
                        error = %e,
                        database = id.as_str(),
                        "Failed to save migration failure audit log"
                    );
                }
            }
            Err(e) => {
                tracing::warn!(
                    error = %e,
                    path = %audit_db_path.display(),
                    "Failed to open audit database for migration failure logging"
                );
            }
        }
    }

    /// ç£ç›˜ç©ºé—´é¢„æ£€æŸ¥
    ///
    /// è¿ç§»è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦åˆ›å»ºä¸´æ—¶è¡¨ï¼ˆCREATE-COPY-SWAP æ¨¡å¼ï¼‰ï¼Œ
    /// ç£ç›˜ç©ºé—´ä¸è¶³ä¼šå¯¼è‡´è¿ç§»ä¸­é€”å¤±è´¥å¹¶å¯èƒ½æŸåæ•°æ®åº“ã€‚
    /// æ­¤æ–¹æ³•åœ¨è¿ç§»å‰æ£€æŸ¥å¯ç”¨ç©ºé—´ï¼Œä¸è¶³æ—¶æå‰ fail-fast å¹¶ç»™å‡ºå¯æ“ä½œæç¤ºã€‚
    ///
    /// ## æ£€æŸ¥ç­–ç•¥
    ///
    /// - è®¡ç®—æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶æ€»å¤§å°
    /// - è¦æ±‚å¯ç”¨ç©ºé—´è‡³å°‘ä¸ºæ•°æ®åº“æ€»å¤§å°çš„ 2 å€ + 50MB ä½™é‡
    ///   ï¼ˆCREATE-COPY-SWAP éœ€è¦ä¸€ä»½å®Œæ•´æ‹·è´ï¼‰
    fn preflight_disk_space_check(&self) -> Result<(), MigrationError> {
        use std::fs;

        // è®¡ç®—æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶æ€»å¤§å°
        let mut total_db_size: u64 = 0;
        for db_id in DatabaseId::all_ordered() {
            let db_path = self.get_database_path(&db_id);
            if db_path.exists() {
                if let Ok(metadata) = fs::metadata(&db_path) {
                    total_db_size += metadata.len();
                }
                // ä¹Ÿè®¡ç®— WAL æ–‡ä»¶å¤§å°
                let wal_path = db_path.with_extension("db-wal");
                if wal_path.exists() {
                    if let Ok(metadata) = fs::metadata(&wal_path) {
                        total_db_size += metadata.len();
                    }
                }
            }
        }

        // éœ€è¦çš„æœ€å°ç©ºé—´ = æ•°æ®åº“æ€»å¤§å° * 2 + 50MB ä½™é‡
        let min_margin_bytes: u64 = 50 * 1024 * 1024; // 50MB
        let required_bytes = total_db_size
            .saturating_mul(2)
            .saturating_add(min_margin_bytes);

        // è·å–ç£ç›˜å¯ç”¨ç©ºé—´ï¼ˆä½¿ç”¨å·²æœ‰çš„è·¨å¹³å°å®ç°ï¼‰
        let available =
            crate::backup_common::get_available_disk_space(&self.app_data_dir).unwrap_or(u64::MAX);

        let required_mb = required_bytes / (1024 * 1024);
        let available_mb = available / (1024 * 1024);

        if available < required_bytes {
            tracing::error!(
                available_mb = available_mb,
                required_mb = required_mb,
                total_db_size_mb = total_db_size / (1024 * 1024),
                "ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ— æ³•å®‰å…¨æ‰§è¡Œè¿ç§»"
            );
            return Err(MigrationError::InsufficientDiskSpace {
                available_mb,
                required_mb,
            });
        }

        tracing::debug!(
            available_mb = available_mb,
            required_mb = required_mb,
            "ç£ç›˜ç©ºé—´é¢„æ£€æŸ¥é€šè¿‡"
        );

        Ok(())
    }

    /// è·å–åº”ç”¨æ•°æ®ç›®å½•
    pub fn app_data_dir(&self) -> &PathBuf {
        &self.app_data_dir
    }

    /// èšåˆå½“å‰ Schema çŠ¶æ€
    ///
    /// ä»æ‰€æœ‰æ•°æ®åº“è¯»å–å½“å‰ç‰ˆæœ¬ä¿¡æ¯ï¼Œç”Ÿæˆç»Ÿä¸€çš„ SchemaRegistryã€‚
    /// æ”¯æŒå¤šç§è¿ç§»ç³»ç»Ÿï¼šRefineryã€ChatV2ã€LLM Usage ç­‰ã€‚
    pub fn aggregate_schema_registry(&self) -> Result<SchemaRegistry, MigrationError> {
        use crate::data_governance::schema_registry::{get_data_contract_version, DatabaseStatus};

        tracing::info!("ğŸ“Š [SchemaAggregation] å¼€å§‹èšåˆæ•°æ®åº“ Schema çŠ¶æ€...");
        let mut registry = SchemaRegistry::new();

        for db_id in DatabaseId::all_ordered() {
            let db_path = self.get_database_path(&db_id);

            // å¦‚æœæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè®°å½•å¹¶è·³è¿‡
            if !db_path.exists() {
                tracing::debug!(
                    "  â­ï¸ [SchemaAggregation] {}: æ–‡ä»¶ä¸å­˜åœ¨ ({})",
                    db_id.as_str(),
                    db_path.display()
                );
                continue;
            }

            let conn = rusqlite::Connection::open(&db_path)
                .map_err(|e| MigrationError::Database(e.to_string()))?;

            let version = self.get_current_version(&conn)?;
            let migration_set = self.get_migration_set(&db_id);

            // è¯»å–è¿ç§»å†å²ï¼ˆåŒ…å« Refinery è®°å½•çš„ checksumï¼‰
            let history = self.read_migration_history(&conn)?;

            // ä½¿ç”¨ Refinery è®°å½•çš„æœ€æ–° checksumï¼ˆæƒå¨æ¥æºï¼‰
            let checksum = history
                .iter()
                .filter(|r| r.version == version)
                .map(|r| r.checksum.clone())
                .next()
                .unwrap_or_default();

            tracing::info!(
                "  âœ… [SchemaAggregation] {}: v{} (è·¯å¾„: {})",
                db_id.as_str(),
                version,
                db_path.display()
            );

            let status = DatabaseStatus {
                id: db_id.clone(),
                schema_version: version,
                min_compatible_version: 1,
                max_compatible_version: migration_set.latest_version() as u32,
                data_contract_version: get_data_contract_version(version),
                migration_history: history,
                checksum,
                updated_at: chrono::Utc::now().to_rfc3339(),
            };

            registry.databases.insert(db_id, status);
        }

        registry.global_version = registry.calculate_global_version();
        registry.aggregated_at = chrono::Utc::now().to_rfc3339();

        tracing::info!(
            "ğŸ“Š [SchemaAggregation] èšåˆå®Œæˆ: å…¨å±€ç‰ˆæœ¬={}, æ•°æ®åº“æ•°é‡={}",
            registry.global_version,
            registry.databases.len()
        );

        Ok(registry)
    }

    /// è¯»å–æ•°æ®åº“çš„è¿ç§»å†å²
    fn read_migration_history(
        &self,
        conn: &rusqlite::Connection,
    ) -> Result<Vec<crate::data_governance::schema_registry::MigrationRecord>, MigrationError> {
        use crate::data_governance::schema_registry::MigrationRecord;

        // æ£€æŸ¥ Refinery çš„ schema history è¡¨æ˜¯å¦å­˜åœ¨
        let table_exists: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='refinery_schema_history')",
                [],
                |row| row.get(0),
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        if !table_exists {
            return Ok(Vec::new());
        }

        // è¯»å–è¿ç§»å†å²
        let mut stmt = conn
            .prepare(
                "SELECT version, name, checksum, applied_on FROM refinery_schema_history ORDER BY version",
            )
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        let records = stmt
            .query_map([], |row| {
                Ok(MigrationRecord {
                    version: row.get::<_, i32>(0)? as u32,
                    name: row.get(1)?,
                    checksum: row.get(2)?,
                    applied_at: row.get(3)?,
                    duration_ms: None, // Refinery ä¸è®°å½•è€—æ—¶
                    success: true,
                })
            })
            .map_err(|e| MigrationError::Database(e.to_string()))?
            .filter_map(log_and_skip_err)
            .collect();

        Ok(records)
    }

    /// æ‰§è¡Œå•ä¸ªæ•°æ®åº“çš„è¿ç§»ï¼ˆå…¬å¼€æ–¹æ³•ï¼‰
    ///
    /// ç”¨äºå•ç‹¬è¿ç§»æŸä¸ªæ•°æ®åº“ï¼Œä¸æ£€æŸ¥ä¾èµ–å…³ç³»ã€‚
    pub fn migrate_single(
        &mut self,
        id: DatabaseId,
    ) -> Result<DatabaseMigrationReport, MigrationError> {
        self.migrate_database(id)
    }

    /// æ£€æŸ¥æ•°æ®åº“æ˜¯å¦éœ€è¦è¿ç§»
    pub fn needs_migration(&self, id: &DatabaseId) -> Result<bool, MigrationError> {
        let db_path = self.get_database_path(id);

        // å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œéœ€è¦è¿ç§»
        if !db_path.exists() {
            return Ok(true);
        }

        let conn = rusqlite::Connection::open(&db_path)
            .map_err(|e| MigrationError::Database(e.to_string()))?;

        let current_version = self.get_current_version(&conn)? as i32;
        let migration_set = self.get_migration_set(id);
        let latest_version = migration_set.latest_version();

        Ok(current_version < latest_version)
    }

    /// è·å–æ‰€æœ‰å¾…æ‰§è¡Œçš„è¿ç§»æ•°é‡
    pub fn pending_migrations_count(&self) -> Result<usize, MigrationError> {
        let mut total = 0;

        for db_id in DatabaseId::all_ordered() {
            let db_path = self.get_database_path(&db_id);

            let current_version = if db_path.exists() {
                let conn = rusqlite::Connection::open(&db_path)
                    .map_err(|e| MigrationError::Database(e.to_string()))?;
                self.get_current_version(&conn)? as i32
            } else {
                0
            };

            let migration_set = self.get_migration_set(&db_id);
            total += migration_set.pending(current_version).count();
        }

        Ok(total)
    }
}

// ============================================================================
// æµ‹è¯•
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    use crate::data_governance::migration::{
        CHAT_V2_MIGRATION_SET, LLM_USAGE_MIGRATION_SET, VFS_MIGRATION_SET,
    };
    use tempfile::TempDir;

    fn create_test_coordinator() -> (MigrationCoordinator, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let coordinator =
            MigrationCoordinator::new(temp_dir.path().to_path_buf()).with_audit_db(None); // æµ‹è¯•æ—¶ä¸éœ€è¦å®¡è®¡æ—¥å¿—
        (coordinator, temp_dir)
    }

    fn create_test_sqlite_db(path: &std::path::Path) {
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent).unwrap();
        }
        let conn = rusqlite::Connection::open(path).unwrap();
        conn.execute(
            "CREATE TABLE IF NOT EXISTS test_data (id INTEGER PRIMARY KEY, value TEXT NOT NULL)",
            [],
        )
        .unwrap();
        conn.execute("INSERT INTO test_data (value) VALUES ('ok')", [])
            .unwrap();
    }

    fn mark_latest_version(path: &std::path::Path, version: u32) {
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent).unwrap();
        }
        let conn = rusqlite::Connection::open(path).unwrap();
        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                version INTEGER PRIMARY KEY,
                name TEXT,
                applied_on TEXT,
                checksum TEXT
            )",
            [],
        )
        .unwrap();
        conn.execute(
            "INSERT OR REPLACE INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (?1, 'latest', '2026-02-11T00:00:00Z', 'x')",
            [version],
        )
        .unwrap();
    }

    #[test]
    fn test_new_coordinator() {
        let (coordinator, temp_dir) = create_test_coordinator();
        assert_eq!(coordinator.app_data_dir(), temp_dir.path());
    }

    #[test]
    fn test_database_paths() {
        let (coordinator, temp_dir) = create_test_coordinator();

        // VFS æ•°æ®åº“åœ¨ databases å­ç›®å½•
        assert_eq!(
            coordinator.get_database_path(&DatabaseId::Vfs),
            temp_dir.path().join("databases").join("vfs.db")
        );

        // ChatV2, Mistakes, LlmUsage æ•°æ®åº“åœ¨æ ¹ç›®å½•
        assert_eq!(
            coordinator.get_database_path(&DatabaseId::ChatV2),
            temp_dir.path().join("chat_v2.db")
        );

        assert_eq!(
            coordinator.get_database_path(&DatabaseId::Mistakes),
            temp_dir.path().join("mistakes.db")
        );

        assert_eq!(
            coordinator.get_database_path(&DatabaseId::LlmUsage),
            temp_dir.path().join("llm_usage.db")
        );
    }

    #[test]
    fn test_migration_report() {
        let mut report = MigrationReport::new();
        assert!(report.success);
        assert!(report.databases.is_empty());

        report.add(DatabaseMigrationReport {
            id: DatabaseId::Vfs,
            from_version: 0,
            to_version: 1,
            applied_count: 1,
            success: true,
            duration_ms: 100,
            error: None,
        });

        assert!(report.success);
        assert_eq!(report.databases.len(), 1);

        report.add(DatabaseMigrationReport {
            id: DatabaseId::ChatV2,
            from_version: 0,
            to_version: 0,
            applied_count: 0,
            success: false,
            duration_ms: 50,
            error: Some("Test error".to_string()),
        });

        assert!(!report.success);
        assert_eq!(report.databases.len(), 2);
    }

    #[test]
    fn test_needs_migration_nonexistent_db() {
        let (coordinator, _temp_dir) = create_test_coordinator();

        // ä¸å­˜åœ¨çš„æ•°æ®åº“åº”è¯¥éœ€è¦è¿ç§»
        assert!(coordinator.needs_migration(&DatabaseId::Vfs).unwrap());
        assert!(coordinator.needs_migration(&DatabaseId::ChatV2).unwrap());
        assert!(coordinator.needs_migration(&DatabaseId::Mistakes).unwrap());
        assert!(coordinator.needs_migration(&DatabaseId::LlmUsage).unwrap());
    }

    #[test]
    fn test_pending_migrations_count_empty() {
        let (coordinator, _temp_dir) = create_test_coordinator();

        // æ‰€æœ‰æ•°æ®åº“éƒ½ä¸å­˜åœ¨æ—¶ï¼Œå¾…æ‰§è¡Œè¿ç§»æ•°é‡åº”ç­‰äºå…¨éƒ¨è¿ç§»æ¡ç›®æ•°
        let expected: usize = crate::data_governance::migration::ALL_MIGRATION_SETS
            .iter()
            .map(|set| set.count())
            .sum();
        let count = coordinator.pending_migrations_count().unwrap();
        assert_eq!(count, expected);
    }

    #[test]
    fn test_get_current_version_no_table() {
        let (coordinator, temp_dir) = create_test_coordinator();

        // åˆ›å»ºä¸€ä¸ªç©ºæ•°æ®åº“
        let db_path = temp_dir.path().join("test.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        // æ²¡æœ‰ refinery_schema_history è¡¨æ—¶åº”è¯¥è¿”å› 0
        let version = coordinator.get_current_version(&conn).unwrap();
        assert_eq!(version, 0);
    }

    #[test]
    fn test_check_dependencies_success() {
        let (coordinator, _temp_dir) = create_test_coordinator();
        let mut report = MigrationReport::new();

        // VFS æ²¡æœ‰ä¾èµ–ï¼Œåº”è¯¥æˆåŠŸ
        assert!(coordinator
            .check_dependencies(&DatabaseId::Vfs, &report)
            .is_ok());

        // æ·»åŠ  VFS æˆåŠŸæŠ¥å‘Š
        report.add(DatabaseMigrationReport {
            id: DatabaseId::Vfs,
            from_version: 0,
            to_version: 1,
            applied_count: 1,
            success: true,
            duration_ms: 100,
            error: None,
        });

        // ChatV2 ä¾èµ– VFSï¼Œç°åœ¨åº”è¯¥æˆåŠŸ
        assert!(coordinator
            .check_dependencies(&DatabaseId::ChatV2, &report)
            .is_ok());
    }

    #[test]
    fn test_legacy_baseline_skips_when_init_contract_missing() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute(
            "CREATE TABLE migration_progress (category TEXT PRIMARY KEY, status TEXT NOT NULL)",
            [],
        )
        .unwrap();
        conn.execute(
            "CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL)",
            [],
        )
        .unwrap();

        coordinator
            .ensure_legacy_baseline(&conn, &DatabaseId::Mistakes)
            .unwrap();

        let recorded: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM refinery_schema_history WHERE version = 20260130",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(recorded, 0, "invalid legacy schema must not be baselined");
    }

    #[test]
    fn test_legacy_baseline_writes_record_when_init_contract_satisfied() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260130__init.sql"
        ))
        .unwrap();
        conn.execute(
            "CREATE TABLE IF NOT EXISTS migration_progress (category TEXT PRIMARY KEY, status TEXT NOT NULL)",
            [],
        )
        .unwrap();

        coordinator
            .ensure_legacy_baseline(&conn, &DatabaseId::Mistakes)
            .unwrap();

        let recorded: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM refinery_schema_history WHERE version = 20260130",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(
            recorded, 1,
            "valid legacy schema should be baselined exactly once"
        );
    }

    #[cfg(feature = "data_governance")]
    #[test]
    fn test_apply_mistakes_init_compat_repairs_legacy_schema() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(
            "
            CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL, status TEXT NOT NULL, question_images TEXT NOT NULL);
            CREATE TABLE document_tasks (id TEXT PRIMARY KEY);
            CREATE TABLE anki_cards (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                front TEXT NOT NULL,
                back TEXT NOT NULL,
                source_type TEXT NOT NULL DEFAULT '',
                source_id TEXT NOT NULL DEFAULT ''
            );
            CREATE TABLE chat_messages (id INTEGER PRIMARY KEY, mistake_id TEXT NOT NULL, role TEXT NOT NULL, content TEXT NOT NULL, timestamp TEXT NOT NULL, stable_id TEXT);
            CREATE TABLE review_chat_messages (id INTEGER PRIMARY KEY, review_analysis_id TEXT NOT NULL, role TEXT NOT NULL, content TEXT NOT NULL, timestamp TEXT NOT NULL);
            ",
        )
        .unwrap();

        coordinator.apply_mistakes_init_compat(&conn).unwrap();

        let has_text: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM pragma_table_info('anki_cards') WHERE name='text')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(has_text, "anki_cards.text should be repaired");

        let has_review_sessions: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='review_sessions')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_review_sessions,
            "missing review_sessions table should be created"
        );
    }

    #[test]
    fn test_verify_migrations_persists_schema_fingerprint() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260130__init.sql"
        ))
        .unwrap();
        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (version INTEGER PRIMARY KEY, name TEXT, applied_on TEXT, checksum TEXT)",
            [],
        )
        .unwrap();
        conn.execute(
            "INSERT INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (20260130, 'init', '2026-02-07T00:00:00Z', '0')",
            [],
        )
        .unwrap();

        coordinator
            .verify_migrations(&conn, &DatabaseId::Mistakes, &MISTAKES_MIGRATIONS, 20260130)
            .unwrap();

        let check_sql = format!(
            "SELECT COUNT(*) FROM {} WHERE database_id = ?1 AND schema_version = ?2",
            SCHEMA_FINGERPRINT_TABLE
        );
        let count: i64 = conn
            .query_row(
                &check_sql,
                rusqlite::params!["mistakes", 20260130_i64],
                |row| row.get(0),
            )
            .unwrap();
        assert_eq!(
            count, 1,
            "fingerprint should be recorded for the verified version"
        );
    }

    #[test]
    fn test_verify_migrations_detects_schema_fingerprint_drift() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260130__init.sql"
        ))
        .unwrap();

        // é¦–æ¬¡è®°å½• fingerprintï¼ˆallow_rebaseline=falseï¼‰
        coordinator
            .verify_schema_fingerprint(&conn, &DatabaseId::Mistakes, 20260130, false)
            .unwrap();

        // åˆ¶é€  schema æ¼‚ç§»
        conn.execute("ALTER TABLE anki_cards ADD COLUMN drift_marker INTEGER", [])
            .unwrap();

        // allow_rebaseline=false æ—¶åº”æ£€æµ‹åˆ°æ¼‚ç§»å¹¶æŠ¥é”™
        let err = coordinator
            .verify_schema_fingerprint(&conn, &DatabaseId::Mistakes, 20260130, false)
            .unwrap_err();

        match err {
            MigrationError::VerificationFailed { reason, .. } => {
                assert!(reason.contains("Schema fingerprint drift detected"));
            }
            other => panic!("unexpected error: {:?}", other),
        }

        // allow_rebaseline=true æ—¶æ¼‚ç§»åº”è¢«å®¹å¿ï¼ˆä¸æŠ¥é”™ï¼‰
        coordinator
            .verify_schema_fingerprint(&conn, &DatabaseId::Mistakes, 20260130, true)
            .unwrap();
    }

    #[cfg(feature = "data_governance")]
    #[test]
    fn test_apply_mistakes_init_compat_is_idempotent_on_sparse_legacy_schema() {
        let (coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(
            "
            CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL, status TEXT NOT NULL, question_images TEXT NOT NULL, updated_at TEXT NOT NULL DEFAULT '');
            CREATE TABLE document_tasks (
                id TEXT PRIMARY KEY,
                document_id TEXT NOT NULL DEFAULT '',
                original_document_name TEXT NOT NULL DEFAULT '',
                segment_index INTEGER NOT NULL DEFAULT 0,
                content_segment TEXT NOT NULL DEFAULT '',
                status TEXT NOT NULL DEFAULT 'Pending',
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                anki_generation_options_json TEXT NOT NULL DEFAULT '{}'
            );
            CREATE TABLE anki_cards (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                front TEXT NOT NULL,
                back TEXT NOT NULL,
                source_type TEXT NOT NULL DEFAULT '',
                source_id TEXT NOT NULL DEFAULT '',
                card_order_in_task INTEGER DEFAULT 0,
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                template_id TEXT,
                text TEXT
            );
            CREATE TABLE chat_messages (
                id INTEGER PRIMARY KEY,
                mistake_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                stable_id TEXT
            );
            CREATE TABLE review_chat_messages (
                id INTEGER PRIMARY KEY,
                review_analysis_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp TEXT NOT NULL
            );
            CREATE TABLE custom_anki_templates (
                id TEXT PRIMARY KEY,
                name TEXT,
                generation_prompt TEXT,
                front_template TEXT,
                back_template TEXT,
                css_style TEXT
            );
            ",
        )
        .unwrap();

        coordinator.apply_mistakes_init_compat(&conn).unwrap();
        coordinator.apply_mistakes_init_compat(&conn).unwrap();

        let has_irec_card_id: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM pragma_table_info('mistakes') WHERE name='irec_card_id')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_irec_card_id,
            "mistakes.irec_card_id should exist after compat repair"
        );

        let has_turn_id: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM pragma_table_info('chat_messages') WHERE name='turn_id')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_turn_id,
            "chat_messages.turn_id should exist after compat repair"
        );

        let has_text_idx: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='index' AND name='idx_anki_cards_text')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_text_idx,
            "idx_anki_cards_text should exist after compat repair"
        );
    }

    #[cfg(feature = "data_governance")]
    #[test]
    fn test_migrate_single_mistakes_recovers_partial_legacy_database() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(
            "
            CREATE TABLE migration_progress (category TEXT PRIMARY KEY, status TEXT NOT NULL);
            CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL, status TEXT NOT NULL, question_images TEXT NOT NULL, updated_at TEXT NOT NULL DEFAULT '');
            CREATE TABLE document_tasks (
                id TEXT PRIMARY KEY,
                document_id TEXT NOT NULL DEFAULT '',
                original_document_name TEXT NOT NULL DEFAULT '',
                segment_index INTEGER NOT NULL DEFAULT 0,
                content_segment TEXT NOT NULL DEFAULT '',
                status TEXT NOT NULL DEFAULT 'Pending',
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                anki_generation_options_json TEXT NOT NULL DEFAULT '{}'
            );
            CREATE TABLE anki_cards (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                front TEXT NOT NULL,
                back TEXT NOT NULL,
                source_type TEXT NOT NULL DEFAULT '',
                source_id TEXT NOT NULL DEFAULT '',
                card_order_in_task INTEGER DEFAULT 0,
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                template_id TEXT,
                text TEXT
            );
            CREATE TABLE chat_messages (
                id INTEGER PRIMARY KEY,
                mistake_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                stable_id TEXT
            );
            CREATE TABLE review_chat_messages (
                id INTEGER PRIMARY KEY,
                review_analysis_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp TEXT NOT NULL
            );
            ",
        )
        .unwrap();

        drop(conn);

        let report = coordinator.migrate_single(DatabaseId::Mistakes).unwrap();
        assert!(report.success);
        assert_eq!(
            report.to_version,
            MISTAKES_MIGRATIONS.latest_version() as u32
        );

        let verify_conn = rusqlite::Connection::open(&db_path).unwrap();
        let has_review_sessions: bool = verify_conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='review_sessions')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_review_sessions,
            "review_sessions should exist after migration recovery"
        );

        let has_anki_text: bool = verify_conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM pragma_table_info('anki_cards') WHERE name='text')",
                [],
                |row| row.get(0),
            )
            .unwrap();
        assert!(
            has_anki_text,
            "anki_cards.text should exist after migration recovery"
        );
    }

    #[cfg(feature = "data_governance")]
    #[test]
    fn test_migrate_single_mistakes_reentrant_after_recovery() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");
        let conn = rusqlite::Connection::open(&db_path).unwrap();

        conn.execute_batch(
            "
            CREATE TABLE migration_progress (category TEXT PRIMARY KEY, status TEXT NOT NULL);
            CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL, status TEXT NOT NULL, question_images TEXT NOT NULL, updated_at TEXT NOT NULL DEFAULT '');
            CREATE TABLE document_tasks (
                id TEXT PRIMARY KEY,
                document_id TEXT NOT NULL DEFAULT '',
                original_document_name TEXT NOT NULL DEFAULT '',
                segment_index INTEGER NOT NULL DEFAULT 0,
                content_segment TEXT NOT NULL DEFAULT '',
                status TEXT NOT NULL DEFAULT 'Pending',
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                anki_generation_options_json TEXT NOT NULL DEFAULT '{}'
            );
            CREATE TABLE anki_cards (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                front TEXT NOT NULL,
                back TEXT NOT NULL,
                source_type TEXT NOT NULL DEFAULT '',
                source_id TEXT NOT NULL DEFAULT '',
                card_order_in_task INTEGER DEFAULT 0,
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                template_id TEXT,
                text TEXT
            );
            ",
        )
        .unwrap();

        drop(conn);

        let first = coordinator.migrate_single(DatabaseId::Mistakes).unwrap();
        let second = coordinator.migrate_single(DatabaseId::Mistakes).unwrap();

        assert!(first.success);
        assert!(second.success);
        assert_eq!(
            second.applied_count, 0,
            "second migration should be idempotent"
        );
        assert_eq!(
            second.to_version,
            MISTAKES_MIGRATIONS.latest_version() as u32,
            "second migration should stay at latest version"
        );
    }

    #[test]
    fn test_check_dependencies_failure() {
        let (coordinator, _temp_dir) = create_test_coordinator();
        let report = MigrationReport::new();

        // ChatV2 ä¾èµ– VFSï¼Œä½† VFS æœªè¿ç§»
        let result = coordinator.check_dependencies(&DatabaseId::ChatV2, &report);
        assert!(result.is_err());

        if let Err(MigrationError::DependencyNotSatisfied {
            database,
            dependency,
        }) = result
        {
            assert_eq!(database, "chat_v2");
            assert_eq!(dependency, "vfs");
        } else {
            panic!("Expected DependencyNotSatisfied error");
        }
    }

    #[test]
    fn test_core_backup_creates_snapshot_for_four_core_dbs() {
        let (mut coordinator, temp_dir) = create_test_coordinator();

        // å‡†å¤‡å››ä¸ªæ ¸å¿ƒåº“ï¼ˆçœŸå® SQLiteï¼‰
        create_test_sqlite_db(&temp_dir.path().join("databases").join("vfs.db"));
        create_test_sqlite_db(&temp_dir.path().join("chat_v2.db"));
        create_test_sqlite_db(&temp_dir.path().join("mistakes.db"));
        create_test_sqlite_db(&temp_dir.path().join("llm_usage.db"));

        coordinator
            .backup_core_databases_once_per_startup()
            .unwrap();

        let backup_root = coordinator.core_backup_root_dir();
        let snapshots: Vec<_> = std::fs::read_dir(&backup_root)
            .unwrap()
            .filter_map(|e| e.ok())
            .collect();
        assert_eq!(snapshots.len(), 1, "é¦–æ¬¡åº”ç”Ÿæˆä¸€ä¸ªå¿«ç…§ç›®å½•");

        let snapshot_dir = snapshots[0].path();
        assert!(snapshot_dir.join("databases").join("vfs.db").exists());
        assert!(snapshot_dir.join("chat_v2.db").exists());
        assert!(snapshot_dir.join("mistakes.db").exists());
        assert!(snapshot_dir.join("llm_usage.db").exists());
    }

    #[test]
    fn test_core_backup_only_once_in_same_process_for_same_data_dir() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        create_test_sqlite_db(&temp_dir.path().join("databases").join("vfs.db"));
        create_test_sqlite_db(&temp_dir.path().join("chat_v2.db"));
        create_test_sqlite_db(&temp_dir.path().join("mistakes.db"));
        create_test_sqlite_db(&temp_dir.path().join("llm_usage.db"));

        coordinator
            .backup_core_databases_once_per_startup()
            .unwrap();
        coordinator
            .backup_core_databases_once_per_startup()
            .unwrap();

        let backup_root = coordinator.core_backup_root_dir();
        let snapshot_count = std::fs::read_dir(&backup_root)
            .unwrap()
            .filter_map(|e| e.ok())
            .count();
        assert_eq!(snapshot_count, 1, "åŒä¸€å¯åŠ¨å‘¨æœŸåŒä¸€ç›®å½•ä»…å…è®¸ä¸€æ¬¡å¤‡ä»½");
    }

    #[test]
    fn test_core_backup_skips_when_no_pending_migrations() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        let vfs_db = temp_dir.path().join("databases").join("vfs.db");
        let chat_db = temp_dir.path().join("chat_v2.db");
        let mistakes_db = temp_dir.path().join("mistakes.db");
        let llm_db = temp_dir.path().join("llm_usage.db");

        mark_latest_version(&vfs_db, VFS_MIGRATION_SET.latest_version() as u32);
        mark_latest_version(&chat_db, CHAT_V2_MIGRATION_SET.latest_version() as u32);
        mark_latest_version(&mistakes_db, MISTAKES_MIGRATIONS.latest_version() as u32);
        mark_latest_version(&llm_db, LLM_USAGE_MIGRATION_SET.latest_version() as u32);

        // æ¸…ç†è¯¥ç›®å½•å¯èƒ½è¢«å‰åºæµ‹è¯•å†™å…¥çš„å¯åŠ¨ guard
        let key = coordinator.startup_guard_key();
        if let Some(guard) = STARTUP_CORE_BACKUP_GUARD.get() {
            let mut sessions = guard.lock().unwrap();
            sessions.remove(&key);
        }

        coordinator
            .maybe_backup_core_databases_before_migration()
            .unwrap();

        assert!(
            !coordinator.core_backup_root_dir().exists(),
            "æ— å¾…è¿ç§»æ—¶ä¸åº”åˆ›å»ºæ ¸å¿ƒå¿«ç…§ç›®å½•"
        );
    }

    /// å¤ç° V20260202 (llm_usage) è¿ç§»å¤±è´¥åœºæ™¯
    ///
    /// æ¨¡æ‹Ÿå·²å®Œæˆ V20260130+V20260131+V20260201 çš„æ•°æ®åº“ï¼Œ
    /// éªŒè¯ V20260202 èƒ½å¦æˆåŠŸæ‰§è¡Œã€‚
    #[cfg(feature = "data_governance")]
    #[test]
    fn test_reproduce_llm_usage_v20260202_failure() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("llm_usage.db");

        // æŒ‰é¡ºåºæ‰§è¡Œå‰ä¸‰ä¸ªè¿ç§»çš„ SQLï¼Œå»ºç«‹ v20260201 çŠ¶æ€
        let conn = rusqlite::Connection::open(&db_path).unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/llm_usage/V20260130__init.sql"
        ))
        .unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/llm_usage/V20260131__add_change_log.sql"
        ))
        .unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/llm_usage/V20260201__add_sync_fields.sql"
        ))
        .unwrap();

        // æ‰‹åŠ¨æ ‡è®°å‰ä¸‰ä¸ªè¿ç§»å·²å®Œæˆ
        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (version INTEGER PRIMARY KEY, name TEXT, applied_on TEXT, checksum TEXT)",
            [],
        ).unwrap();
        conn.execute(
            "INSERT INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (20260130, 'init', '2026-01-30T00:00:00Z', '0')",
            [],
        ).unwrap();
        conn.execute(
            "INSERT INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (20260131, 'add_change_log', '2026-01-31T00:00:00Z', '0')",
            [],
        ).unwrap();
        conn.execute(
            "INSERT INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (20260201, 'add_sync_fields', '2026-02-01T00:00:00Z', '0')",
            [],
        ).unwrap();
        drop(conn);

        // æ‰§è¡Œè¿ç§» â€” åº”æ‰§è¡Œ V20260202
        let result = coordinator.migrate_single(DatabaseId::LlmUsage);
        match &result {
            Ok(report) => {
                eprintln!(
                    "[llm_usage V20260202] SUCCESS: from={} to={} applied={}",
                    report.from_version, report.to_version, report.applied_count
                );
            }
            Err(e) => {
                eprintln!("[llm_usage V20260202] FAILED: {}", e);
                eprintln!("[llm_usage V20260202] Debug: {:?}", e);
            }
        }
        assert!(
            result.is_ok(),
            "V20260202 migration should succeed: {:?}",
            result.err()
        );

        let report = result.unwrap();
        assert_eq!(
            report.to_version,
            LLM_USAGE_MIGRATION_SET.latest_version() as u32
        );
    }

    /// å¤ç° V20260208+V20260209 (mistakes) è¿ç§»å¤±è´¥åœºæ™¯
    #[cfg(feature = "data_governance")]
    #[test]
    fn test_reproduce_mistakes_v20260208_v20260209_failure() {
        let (mut coordinator, temp_dir) = create_test_coordinator();
        let db_path = temp_dir.path().join("mistakes.db");

        let conn = rusqlite::Connection::open(&db_path).unwrap();
        // æ‰§è¡Œå‰å››ä¸ªè¿ç§»å»ºç«‹ v20260207 çŠ¶æ€
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260130__init.sql"
        ))
        .unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260131__add_change_log.sql"
        ))
        .unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260201__add_sync_fields.sql"
        ))
        .unwrap();
        conn.execute_batch(include_str!(
            "../../../migrations/mistakes/V20260207__add_template_preview_data.sql"
        ))
        .unwrap();

        conn.execute(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (version INTEGER PRIMARY KEY, name TEXT, applied_on TEXT, checksum TEXT)",
            [],
        ).unwrap();
        for (v, n) in [
            (20260130, "init"),
            (20260131, "add_change_log"),
            (20260201, "add_sync_fields"),
            (20260207, "add_template_preview_data"),
        ] {
            conn.execute(
                "INSERT INTO refinery_schema_history (version, name, applied_on, checksum) VALUES (?1, ?2, '2026-02-07T00:00:00Z', '0')",
                rusqlite::params![v, n],
            ).unwrap();
        }
        drop(conn);

        let result = coordinator.migrate_single(DatabaseId::Mistakes);
        match &result {
            Ok(report) => {
                eprintln!(
                    "[mistakes V20260208+9] SUCCESS: from={} to={} applied={}",
                    report.from_version, report.to_version, report.applied_count
                );
            }
            Err(e) => {
                eprintln!("[mistakes V20260208+9] FAILED: {}", e);
                eprintln!("[mistakes V20260208+9] Debug: {:?}", e);
            }
        }
        assert!(
            result.is_ok(),
            "V20260208+V20260209 migration should succeed: {:?}",
            result.err()
        );

        let report = result.unwrap();
        assert_eq!(
            report.to_version,
            MISTAKES_MIGRATIONS.latest_version() as u32
        );
    }
}
