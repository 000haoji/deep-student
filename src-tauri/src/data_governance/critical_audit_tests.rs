//! # æ•°æ®æ²»ç†ç³»ç»Ÿæ‰¹åˆ¤æ€§å®¡é˜… - é—®é¢˜ç¡®è®¤æµ‹è¯•
//!
//! æœ¬æµ‹è¯•æ–‡ä»¶æ ¹æ®å®¡é˜…æŠ¥å‘Šåˆ›å»ºï¼Œç”¨äºç¡®è®¤æŠ¥å‘Šä¸­å‘ç°çš„æ‰€æœ‰é—®é¢˜ã€‚
//! æ¯ä¸ªæµ‹è¯•å¯¹åº”æŠ¥å‘Šä¸­çš„ä¸€ä¸ªå…·ä½“é—®é¢˜ç¼–å·ã€‚
//!
//! ## è¿è¡Œæ–¹å¼
//!
//! ```bash
//! cargo test --features data_governance critical_audit_tests
//! ```

#[cfg(test)]
#[cfg(feature = "data_governance")]
mod critical_audit_tests {
    use rusqlite::Connection;
    use std::path::PathBuf;
    use tempfile::TempDir;

    use crate::data_governance::audit::{AuditFilter, AuditRepository, AuditStatus};
    use crate::data_governance::backup::{BackupConfig, BackupManager, BackupManifest};
    use crate::data_governance::init::{initialize_with_report, needs_initialization};
    use crate::data_governance::migration::{
        DatabaseMigrationReport, MigrationCoordinator, MigrationError, MigrationReport,
        MigrationVerifier, CHAT_V2_MIGRATION_SET, LLM_USAGE_MIGRATION_SET, MISTAKES_MIGRATIONS,
        VFS_MIGRATION_SET,
    };
    use crate::data_governance::schema_registry::DatabaseId;

    // ============================================================================
    // è¾…åŠ©å‡½æ•°
    // ============================================================================

    fn create_test_dir() -> TempDir {
        TempDir::new().expect("Failed to create temp dir")
    }

    fn create_test_coordinator(temp_dir: &TempDir) -> MigrationCoordinator {
        MigrationCoordinator::new(temp_dir.path().to_path_buf()).with_audit_db(None)
    }

    fn create_test_coordinator_with_audit(temp_dir: &TempDir) -> MigrationCoordinator {
        let audit_db_path = temp_dir.path().join("databases").join("audit.db");
        MigrationCoordinator::new(temp_dir.path().to_path_buf()).with_audit_db(Some(audit_db_path))
    }

    fn setup_test_directories(temp_dir: &TempDir) {
        let databases_dir = temp_dir.path().join("databases");
        std::fs::create_dir_all(&databases_dir).expect("Failed to create databases dir");
    }

    fn get_database_path(temp_dir: &TempDir, db_id: &DatabaseId) -> PathBuf {
        match db_id {
            DatabaseId::Vfs => temp_dir.path().join("databases").join("vfs.db"),
            DatabaseId::ChatV2 => temp_dir.path().join("chat_v2.db"),
            DatabaseId::Mistakes => temp_dir.path().join("mistakes.db"),
            DatabaseId::LlmUsage => temp_dir.path().join("llm_usage.db"),
        }
    }

    fn init_audit_database(temp_dir: &TempDir) -> PathBuf {
        let audit_db_path = temp_dir.path().join("databases").join("audit.db");
        let conn = Connection::open(&audit_db_path).expect("Failed to create audit database");
        AuditRepository::init(&conn).expect("Audit init should succeed");
        audit_db_path
    }

    /// æ‰§è¡Œå®Œæ•´è¿ç§»å¹¶è¿”å›åè°ƒå™¨
    fn migrate_all(temp_dir: &TempDir) -> MigrationCoordinator {
        setup_test_directories(temp_dir);
        let mut coordinator = create_test_coordinator(temp_dir);
        coordinator
            .run_all()
            .expect("Full migration should succeed");
        coordinator
    }

    // ============================================================================
    // ğŸ”´ é—®é¢˜ 1: ç¼ºå°‘"å¤‡ä»½æ¢å¤åé‡æ–°è¿ç§»"æµ‹è¯•
    //
    // éªŒè¯ï¼šæ¢å¤æ—§ç‰ˆæœ¬å¤‡ä»½åï¼Œneeds_migration åº”è¿”å› trueï¼Œ
    // ä¸”æ‰§è¡Œè¿ç§»å schema åº”è¾¾åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚
    // å½“å‰çŠ¶æ€ï¼šBackupManager::restore() ä¸ä¼šè§¦å‘è¿ç§»ã€‚
    // ============================================================================

    /// æµ‹è¯•ï¼šåˆ›å»ºv2æ•°æ®åº“ â†’ æ‰‹åŠ¨æ¨¡æ‹Ÿæ¢å¤åˆ°v1 schema â†’ éªŒè¯ needs_migration è¿”å› true
    ///
    /// é—®é¢˜ 1: æ¢å¤æ—§ç‰ˆæœ¬å¤‡ä»½åä¸è§¦å‘è¿ç§»
    #[test]
    fn test_issue1_restore_old_backup_needs_migration_is_true() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // æ­¥éª¤ 1: å®Œæ•´è¿ç§»åˆ°æœ€æ–°ç‰ˆæœ¬
        coordinator
            .run_all()
            .expect("Initial migration should succeed");

        // æ­¥éª¤ 2: éªŒè¯è¿ç§»åä¸éœ€è¦å†æ¬¡è¿ç§»
        for db_id in DatabaseId::all_ordered() {
            let needs = coordinator
                .needs_migration(&db_id)
                .expect("needs_migration should succeed");
            assert!(
                !needs,
                "After full migration, {:?} should not need migration",
                db_id
            );
        }

        // æ­¥éª¤ 3: æ¨¡æ‹Ÿæ¢å¤æ—§ç‰ˆæœ¬æ•°æ®åº“ â€”â€” åˆ é™¤ VFS å¹¶åˆ›å»ºä¸€ä¸ªåªæœ‰ init schema çš„æ•°æ®åº“
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        std::fs::remove_file(&vfs_path).expect("Failed to remove VFS db");
        // åˆ é™¤ WAL/SHM æ–‡ä»¶
        let _ = std::fs::remove_file(vfs_path.with_extension("db-wal"));
        let _ = std::fs::remove_file(vfs_path.with_extension("db-shm"));

        // åˆ›å»ºä¸€ä¸ªåªæœ‰ v20260130 (init) çš„æ•°æ®åº“ï¼Œæ¨¡æ‹Ÿæ—§ç‰ˆå¤‡ä»½
        let conn = Connection::open(&vfs_path).expect("Failed to create old VFS db");
        conn.execute_batch(include_str!("../../migrations/vfs/V20260130__init.sql"))
            .expect("Failed to apply init schema");
        // æ‰‹åŠ¨å†™å…¥ refinery_schema_history åªåˆ° v20260130
        conn.execute_batch(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                version INTEGER PRIMARY KEY,
                name TEXT,
                applied_on TEXT,
                checksum TEXT
            );
            INSERT INTO refinery_schema_history (version, name, applied_on, checksum)
            VALUES (20260130, 'init', '2026-01-30T00:00:00Z', '0');",
        )
        .expect("Failed to create refinery history");
        drop(conn);

        // æ­¥éª¤ 4: éªŒè¯ needs_migration è¿”å› trueï¼ˆæ—§ç‰ˆæœ¬åº”éœ€è¦è¿ç§»ï¼‰
        let needs = coordinator
            .needs_migration(&DatabaseId::Vfs)
            .expect("needs_migration should succeed");
        assert!(
            needs,
            "BUG CONFIRMED (Issue 1): After restoring old backup, VFS should need migration, \
             but needs_migration returned false. This means restore does not trigger re-migration."
        );
    }

    /// æµ‹è¯•ï¼šæ¢å¤æ—§å¤‡ä»½åæ‰§è¡Œè¿ç§»ï¼ŒéªŒè¯ schema è¾¾åˆ°æœ€æ–°ç‰ˆæœ¬
    ///
    /// é—®é¢˜ 1: å¤‡ä»½æ¢å¤åé‡æ–°è¿ç§»çš„ç«¯åˆ°ç«¯æµç¨‹
    #[test]
    fn test_issue1_restore_old_backup_then_migrate_reaches_latest() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // æ­¥éª¤ 1: å®Œæ•´è¿ç§»
        coordinator
            .run_all()
            .expect("Initial migration should succeed");

        // æ­¥éª¤ 2: æ¨¡æ‹Ÿæ¢å¤æ—§ Mistakes æ•°æ®åº“ï¼ˆåªæœ‰ initï¼‰
        let mistakes_path = get_database_path(&temp_dir, &DatabaseId::Mistakes);
        std::fs::remove_file(&mistakes_path).expect("Failed to remove mistakes db");
        let _ = std::fs::remove_file(mistakes_path.with_extension("db-wal"));
        let _ = std::fs::remove_file(mistakes_path.with_extension("db-shm"));

        let conn = Connection::open(&mistakes_path).expect("Failed to create old mistakes db");
        conn.execute_batch(include_str!(
            "../../migrations/mistakes/V20260130__init.sql"
        ))
        .expect("Failed to apply init schema");
        conn.execute_batch(
            "CREATE TABLE IF NOT EXISTS refinery_schema_history (
                version INTEGER PRIMARY KEY, name TEXT, applied_on TEXT, checksum TEXT
            );
            INSERT INTO refinery_schema_history (version, name, applied_on, checksum)
            VALUES (20260130, 'init', '2026-01-30T00:00:00Z', '0');",
        )
        .expect("Failed to create refinery history");
        drop(conn);

        // æ­¥éª¤ 3: é‡æ–°æ‰§è¡Œè¿ç§»
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let result = coordinator2.run_all();
        assert!(
            result.is_ok(),
            "Re-migration after restore should succeed: {:?}",
            result.err()
        );

        let report = result.unwrap();
        assert!(report.success, "Re-migration should be successful");

        // æ­¥éª¤ 4: éªŒè¯ Mistakes è¾¾åˆ°æœ€æ–°ç‰ˆæœ¬
        let mistakes_report = report
            .databases
            .iter()
            .find(|r| r.id == DatabaseId::Mistakes);
        assert!(mistakes_report.is_some(), "Mistakes should be in report");
        let mr = mistakes_report.unwrap();
        assert_eq!(
            mr.to_version,
            MISTAKES_MIGRATIONS.latest_version() as u32,
            "Mistakes should reach latest version after re-migration"
        );
    }

    // ============================================================================
    // ğŸ”´ é—®é¢˜ 2: è¿ç§»è¿‡ç¨‹ä¸­ä¸éªŒè¯ç”¨æˆ·æ•°æ®å­˜æ´»
    //
    // éªŒè¯ï¼šæ’å…¥çœŸå®ä¸šåŠ¡æ•°æ® â†’ æ‰§è¡Œè¿ç§» â†’ éªŒè¯æ•°æ®ä»ç„¶å®Œå¥½ã€‚
    // å½“å‰æµ‹è¯•ä»…æ£€æŸ¥ schema ç»“æ„ï¼Œä»æœªéªŒè¯æ•°æ®ã€‚
    // ============================================================================

    /// æµ‹è¯•ï¼šåœ¨ VFS æ•°æ®åº“ä¸­æ’å…¥æµ‹è¯•æ•°æ® â†’ è¿ç§» â†’ éªŒè¯æ•°æ®å®Œæ•´
    ///
    /// é—®é¢˜ 2: è¿ç§»æµ‹è¯•ä»æœªéªŒè¯ç”¨æˆ·æ•°æ®å­˜æ´»
    #[test]
    fn test_issue2_vfs_data_survives_migration() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // æ‰§è¡Œ VFS è¿ç§»
        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("VFS migration should succeed");

        // æ’å…¥æµ‹è¯•æ•°æ®åˆ° resources è¡¨ï¼ˆæ³¨æ„ï¼šresources æ—  title åˆ—ï¼Œä½¿ç”¨çœŸå® schemaï¼‰
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");

        conn.execute(
            "INSERT INTO resources (id, type, hash, storage_mode, data, metadata_json, ref_count, created_at, updated_at)
             VALUES ('res-001', 'note', 'hash_alpha_unique', 'inline', 'Test Resource Alpha Content', '{}', 1, 1706745600000, 1706745600000)",
            [],
        ).expect("Failed to insert resource");

        conn.execute(
            "INSERT INTO resources (id, type, hash, storage_mode, data, metadata_json, ref_count, created_at, updated_at)
             VALUES ('res-002', 'file', 'hash_beta_unique', 'inline', 'Test Resource Beta Content', '{}', 1, 1706832000000, 1706832000000)",
            [],
        ).expect("Failed to insert resource");

        // æ’å…¥ notes æ•°æ®ï¼ˆnotes æœ‰ titleï¼Œresource_id å¼•ç”¨ resourcesï¼‰
        conn.execute(
            "INSERT INTO notes (id, resource_id, title, tags, is_favorite, created_at, updated_at)
             VALUES ('note-001', 'res-001', 'Test Note Title', '[]', 0, '2026-01-01T00:00:00Z', '2026-01-01T00:00:00Z')",
            [],
        ).expect("Failed to insert note");

        // æ’å…¥ folders æ•°æ®ï¼ˆçœŸå® schemaï¼štitle é nameï¼Œcreated_at/updated_at ä¸º INTEGERï¼‰
        conn.execute(
            "INSERT INTO folders (id, title, sort_order, is_expanded, created_at, updated_at)
             VALUES ('folder-001', 'My Study Folder', 0, 1, 1706745600000, 1706745600000)",
            [],
        )
        .expect("Failed to insert folder");

        drop(conn);

        // é‡æ–°è¿è¡Œè¿ç§»ï¼ˆåº”ä¸ºå¹‚ç­‰ï¼‰
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let report = coordinator2
            .migrate_single(DatabaseId::Vfs)
            .expect("Re-migration should succeed");
        assert!(report.success);

        // éªŒè¯æ•°æ®ä»ç„¶å­˜åœ¨ä¸”å®Œæ•´
        let conn = Connection::open(&vfs_path).expect("Failed to reopen VFS db");

        let resource_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM resources", [], |row| row.get(0))
            .expect("Failed to count resources");
        assert_eq!(
            resource_count, 2,
            "Issue 2: Resources should survive migration (expected 2, got {})",
            resource_count
        );

        let data_value: String = conn
            .query_row(
                "SELECT data FROM resources WHERE id = 'res-001'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query resource data");
        assert_eq!(
            data_value, "Test Resource Alpha Content",
            "Issue 2: Resource data should be preserved"
        );

        let note_title: String = conn
            .query_row("SELECT title FROM notes WHERE id = 'note-001'", [], |row| {
                row.get(0)
            })
            .expect("Failed to query note title");
        assert_eq!(
            note_title, "Test Note Title",
            "Issue 2: Note title should be preserved after migration"
        );

        let folder_title: String = conn
            .query_row(
                "SELECT title FROM folders WHERE id = 'folder-001'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query folder title");
        assert_eq!(
            folder_title, "My Study Folder",
            "Issue 2: Folder title should be preserved"
        );
    }

    /// æµ‹è¯•ï¼šåœ¨ Mistakes æ•°æ®åº“ä¸­æ’å…¥ anki_cards æ•°æ® â†’ è¿ç§» â†’ éªŒè¯æ•°æ®å®Œæ•´
    ///
    /// é—®é¢˜ 2: ä»æœªéªŒè¯ Mistakes ç”¨æˆ·æ•°æ®è¿ç§»åçš„å®Œæ•´æ€§
    #[test]
    fn test_issue2_mistakes_anki_cards_data_survives_migration() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // å…ˆè¿ç§» VFSï¼ˆä¾èµ–ï¼‰
        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("VFS migration should succeed");

        // è¿ç§» Mistakes
        coordinator
            .migrate_single(DatabaseId::Mistakes)
            .expect("Mistakes migration should succeed");

        let mistakes_path = get_database_path(&temp_dir, &DatabaseId::Mistakes);
        let conn = Connection::open(&mistakes_path).expect("Failed to open mistakes db");

        // å…ˆæ’å…¥ document_tasksï¼ˆanki_cards çš„å¤–é”®ä¾èµ–ï¼‰
        for i in 1..=10 {
            conn.execute(
                "INSERT INTO document_tasks (id, document_id, original_document_name, segment_index, content_segment, status, anki_generation_options_json)
                 VALUES (?1, ?2, ?3, ?4, ?5, 'Completed', '{}')",
                rusqlite::params![
                    format!("task-{:03}", i),
                    format!("doc-{:03}", i),
                    format!("document_{}.pdf", i),
                    i,
                    format!("Content segment {}", i),
                ],
            )
            .expect(&format!("Failed to insert document_task {}", i));
        }

        // æ’å…¥ 10 è¡Œ anki_cards æµ‹è¯•æ•°æ®ï¼ˆtask_id å¼•ç”¨ document_tasksï¼‰
        for i in 1..=10 {
            conn.execute(
                "INSERT INTO anki_cards (id, task_id, front, back, source_type, source_id, text)
                 VALUES (?1, ?2, ?3, ?4, 'manual', ?5, ?6)",
                rusqlite::params![
                    format!("card-{:03}", i),
                    format!("task-{:03}", i),
                    format!("Front of card {}", i),
                    format!("Back of card {}", i),
                    format!("src-{:03}", i),
                    format!("Full text for card {}", i),
                ],
            )
            .expect(&format!("Failed to insert anki_card {}", i));
        }

        // æ’å…¥ review_sessions æ•°æ®ï¼ˆçœŸå® schemaï¼šid, title, start_date, end_dateï¼‰
        conn.execute(
            "INSERT INTO review_sessions (id, title, start_date, end_date)
             VALUES ('session-001', 'Daily Review', '2026-01-01', '2026-01-07')",
            [],
        )
        .expect("Failed to insert review_session");

        drop(conn);

        // é‡æ–°æ‰§è¡Œè¿ç§»ï¼ˆå¹‚ç­‰æ€§æµ‹è¯• + æ•°æ®å­˜æ´»ï¼‰
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let report = coordinator2
            .migrate_single(DatabaseId::Mistakes)
            .expect("Re-migration should succeed");
        assert!(report.success);

        // éªŒè¯æ‰€æœ‰ 10 è¡Œ anki_cards æ•°æ®å­˜æ´»
        let conn = Connection::open(&mistakes_path).expect("Failed to reopen mistakes db");

        let card_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM anki_cards", [], |row| row.get(0))
            .expect("Failed to count anki_cards");
        assert_eq!(
            card_count, 10,
            "Issue 2: All 10 anki_cards should survive migration (got {})",
            card_count
        );

        // éªŒè¯å…·ä½“å­—æ®µå€¼
        let front: String = conn
            .query_row(
                "SELECT front FROM anki_cards WHERE id = 'card-005'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query card front");
        assert_eq!(
            front, "Front of card 5",
            "Issue 2: anki_card front field should be preserved"
        );

        let back: String = conn
            .query_row(
                "SELECT back FROM anki_cards WHERE id = 'card-005'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query card back");
        assert_eq!(
            back, "Back of card 5",
            "Issue 2: anki_card back field should be preserved"
        );

        // éªŒè¯ review_sessions å­˜æ´»
        let session_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM review_sessions", [], |row| row.get(0))
            .expect("Failed to count review_sessions");
        assert_eq!(
            session_count, 1,
            "Issue 2: review_sessions should survive migration"
        );
    }

    /// æµ‹è¯•ï¼šChat V2 æ•°æ®åœ¨è¿ç§»åå­˜æ´»
    ///
    /// é—®é¢˜ 2: chat_messages ç­‰æ ¸å¿ƒæ•°æ®è¿ç§»åçš„å®Œæ•´æ€§
    #[test]
    fn test_issue2_chat_v2_data_survives_migration() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // å…ˆè¿ç§»ä¾èµ–
        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("VFS migration failed");
        coordinator
            .migrate_single(DatabaseId::ChatV2)
            .expect("ChatV2 migration failed");

        let chat_path = get_database_path(&temp_dir, &DatabaseId::ChatV2);
        let conn = Connection::open(&chat_path).expect("Failed to open chat_v2 db");

        // æ’å…¥ sessionsï¼ˆä½¿ç”¨çœŸå® schema åˆ—åï¼‰
        conn.execute(
            "INSERT INTO chat_v2_sessions (id, mode, created_at, updated_at, persist_status)
             VALUES ('sess-001', 'general_chat', '2026-01-01T00:00:00Z', '2026-01-01T00:00:00Z', 'active')",
            [],
        ).expect("Failed to insert session");

        // æ’å…¥ messagesï¼ˆtimestamp æ˜¯ INTEGER æ¯«ç§’ï¼Œé created_at TEXTï¼‰
        for i in 1..=5 {
            conn.execute(
                "INSERT INTO chat_v2_messages (id, session_id, role, timestamp)
                 VALUES (?1, 'sess-001', ?2, ?3)",
                rusqlite::params![
                    format!("msg-{:03}", i),
                    if i % 2 == 0 { "assistant" } else { "user" },
                    1706745600000i64 + (i as i64 * 1000),
                ],
            )
            .expect(&format!("Failed to insert message {}", i));
        }

        drop(conn);

        // é‡æ–°è¿ç§»
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let report = coordinator2
            .migrate_single(DatabaseId::ChatV2)
            .expect("Re-migration failed");
        assert!(report.success);

        // éªŒè¯æ•°æ®å­˜æ´»
        let conn = Connection::open(&chat_path).expect("Failed to reopen chat_v2 db");

        let msg_count: i64 = conn
            .query_row(
                "SELECT COUNT(*) FROM chat_v2_messages WHERE session_id = 'sess-001'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to count messages");
        assert_eq!(
            msg_count, 5,
            "Issue 2: All 5 chat messages should survive migration"
        );

        let session_mode: String = conn
            .query_row(
                "SELECT mode FROM chat_v2_sessions WHERE id = 'sess-001'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query session mode");
        assert_eq!(
            session_mode, "general_chat",
            "Issue 2: Session mode should be preserved"
        );
    }

    /// æµ‹è¯•ï¼šLLM Usage æ•°æ®åœ¨è¿ç§»åå­˜æ´»
    ///
    /// é—®é¢˜ 2: llm_usage token è®°å½•è¿ç§»åå®Œæ•´æ€§
    #[test]
    fn test_issue2_llm_usage_data_survives_migration() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        coordinator
            .migrate_single(DatabaseId::LlmUsage)
            .expect("LlmUsage migration failed");

        let llm_path = get_database_path(&temp_dir, &DatabaseId::LlmUsage);
        let conn = Connection::open(&llm_path).expect("Failed to open llm_usage db");

        // æ’å…¥ token ä½¿ç”¨è®°å½•ï¼ˆçœŸå®è¡¨å llm_usage_logsï¼Œä½¿ç”¨çœŸå® schemaï¼‰
        for i in 1..=8 {
            conn.execute(
                "INSERT INTO llm_usage_logs (id, timestamp, provider, model, prompt_tokens, completion_tokens, total_tokens, caller_type, status)
                 VALUES (?1, '2026-01-01T00:00:00Z', 'openai', 'gpt-4o', ?2, ?3, ?4, 'chat_v2', 'success')",
                rusqlite::params![
                    format!("usage-{:03}", i),
                    100 * i as i64,
                    50 * i as i64,
                    150 * i as i64,
                ],
            ).expect(&format!("Failed to insert llm_usage_logs {}", i));
        }

        drop(conn);

        // é‡æ–°è¿ç§»
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let report = coordinator2
            .migrate_single(DatabaseId::LlmUsage)
            .expect("Re-migration failed");
        assert!(report.success);

        let conn = Connection::open(&llm_path).expect("Failed to reopen llm_usage db");
        let count: i64 = conn
            .query_row("SELECT COUNT(*) FROM llm_usage_logs", [], |row| row.get(0))
            .expect("Failed to count llm_usage_logs");
        assert_eq!(
            count, 8,
            "Issue 2: All 8 llm_usage_logs records should survive migration"
        );

        let tokens: i64 = conn
            .query_row(
                "SELECT prompt_tokens FROM llm_usage_logs WHERE id = 'usage-005'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query tokens");
        assert_eq!(
            tokens, 500,
            "Issue 2: Token count should be preserved exactly"
        );
    }

    // ============================================================================
    // ğŸ”´ é—®é¢˜ 3: è·¨ç‰ˆæœ¬æ¢å¤æ— ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
    //
    // éªŒè¯ï¼šBackupManifest æœ‰ schema_versions å­—æ®µï¼Œ
    // ä½† restore() ä»ä¸æ£€æŸ¥è¯¥å­—æ®µã€‚
    // ============================================================================

    /// æµ‹è¯•ï¼šrestore() å¯¹å«æœªæ¥ schema ç‰ˆæœ¬çš„ manifest ä¸æ‹’ç»
    ///
    /// é—®é¢˜ 3: æ¢å¤æ—¶ä¸æ£€æŸ¥ schema ç‰ˆæœ¬å…¼å®¹æ€§
    /// çœŸæ­£çš„éªŒè¯é€»è¾‘ï¼šæ„å»ºä¸€ä¸ªå£°ç§°æ¥è‡ªæœªæ¥ç‰ˆæœ¬çš„å¤‡ä»½ç›®å½• â†’ è°ƒç”¨ restore() â†’
    /// éªŒè¯ restore æ²¡æœ‰å› ç‰ˆæœ¬ä¸å…¼å®¹è€ŒæŠ¥é”™ï¼ˆè€Œæ˜¯å› ä¸ºåˆ«çš„åŸå› ï¼‰
    #[test]
    fn test_issue3_restore_does_not_check_schema_version_compatibility() {
        let backup_dir = TempDir::new().unwrap();
        let manager = BackupManager::new(backup_dir.path().to_path_buf());

        // æ„å»ºä¸€ä¸ªå£°ç§°æ¥è‡ªæœªæ¥ç‰ˆæœ¬çš„å¤‡ä»½
        let future_version = VFS_MIGRATION_SET.latest_version() as u32 + 99999;
        let mut manifest = BackupManifest::new("99.0.0");
        manifest.backup_id = "future_backup".to_string();
        manifest.set_schema_version("vfs", future_version);

        // åˆ›å»ºå¤‡ä»½ç›®å½•å’Œä¸€ä¸ªå‡çš„ .db æ–‡ä»¶ + manifestï¼Œè®© restore èµ°åˆ°ç‰ˆæœ¬æ£€æŸ¥åº”è¯¥å‘ç”Ÿçš„ä½ç½®
        let sub = backup_dir.path().join("future_backup");
        std::fs::create_dir_all(&sub).expect("Failed to create backup subdir");
        // å†™ä¸€ä¸ªæœ‰æ•ˆçš„ç©º SQLite æ–‡ä»¶
        let fake_db = sub.join("vfs.db");
        let conn = Connection::open(&fake_db).expect("create fake db");
        conn.execute_batch("CREATE TABLE t(x)").unwrap();
        drop(conn);
        let sha = crate::data_governance::backup::BackupFile {
            path: "vfs.db".to_string(),
            size: std::fs::metadata(&fake_db).unwrap().len(),
            sha256: {
                use sha2::{Digest, Sha256};
                let bytes = std::fs::read(&fake_db).unwrap();
                let hash = Sha256::digest(&bytes);
                hex::encode(hash)
            },
            database_id: Some("vfs".to_string()),
        };
        manifest.add_file(sha);
        manifest.save_to_file(&sub.join("manifest.json")).unwrap();

        // è°ƒç”¨ restore
        let result = manager.restore(&manifest);

        // Issue 3 å·²ä¿®å¤ï¼šrestore() åº”æ‹’ç»æ¥è‡ªæœªæ¥ç‰ˆæœ¬çš„å¤‡ä»½
        assert!(
            result.is_err(),
            "Issue 3 FIXED: restore() should reject future-version backup"
        );
        let msg = format!("{}", result.unwrap_err());
        assert!(
            msg.contains("version")
                || msg.contains("incompatible")
                || msg.contains("å…¼å®¹")
                || msg.contains("Version"),
            "Issue 3 FIXED: error should mention version incompatibility, got: {}",
            msg
        );
    }

    // ============================================================================
    // ğŸ”´ é—®é¢˜ 4: æ•°æ®åº“æŸååœºæ™¯æ— æµ‹è¯•
    //
    // éªŒè¯ï¼šå†™å…¥éšæœºå­—èŠ‚åˆ° .db æ–‡ä»¶ â†’ è°ƒç”¨ migrate_single â†’ åº”è¿”å›å‹å¥½é”™è¯¯
    // ============================================================================

    /// æµ‹è¯•ï¼šæŸåçš„æ•°æ®åº“æ–‡ä»¶åº”è¿”å›å‹å¥½çš„é”™è¯¯
    ///
    /// é—®é¢˜ 4: å®Œå…¨æŸåçš„ SQLite æ–‡ä»¶
    #[test]
    fn test_issue4_corrupted_database_file_returns_friendly_error() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // åˆ›å»ºä¸€ä¸ªå®Œå…¨æŸåçš„ VFS æ•°æ®åº“æ–‡ä»¶
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        std::fs::write(
            &vfs_path,
            b"THIS IS NOT A SQLITE DATABASE - RANDOM CORRUPTED BYTES 12345",
        )
        .expect("Failed to write corrupted file");

        // å°è¯•è¿ç§»åº”è¯¥è¿”å›é”™è¯¯
        let result = coordinator.migrate_single(DatabaseId::Vfs);

        assert!(
            result.is_err(),
            "Issue 4: Migrating a corrupted database should return an error, not crash"
        );

        // éªŒè¯é”™è¯¯ç±»å‹æ˜¯ Database é”™è¯¯ï¼Œè€Œé panic
        let err = result.unwrap_err();
        let err_msg = format!("{:?}", err);
        assert!(
            err_msg.contains("Database")
                || err_msg.contains("not a database")
                || err_msg.contains("Refinery"),
            "Issue 4: Error should mention database-related problem, got: {}",
            err_msg
        );
    }

    /// æµ‹è¯•ï¼šæˆªæ–­çš„æ•°æ®åº“æ–‡ä»¶åº”è¿”å›å‹å¥½é”™è¯¯
    ///
    /// é—®é¢˜ 4: ç£ç›˜æ–­ç”µå¯¼è‡´çš„æˆªæ–­æ–‡ä»¶
    #[test]
    fn test_issue4_truncated_database_file() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // å…ˆæ­£å¸¸è¿ç§»åˆ›å»ºæ•°æ®åº“
        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("Initial migration should succeed");

        // è¯»å–å‰ 16 å­—èŠ‚ï¼ˆSQLite header ä¸å®Œæ•´ï¼‰ç„¶åæˆªæ–­
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let data = std::fs::read(&vfs_path).expect("Failed to read VFS db");
        let truncated = &data[..std::cmp::min(16, data.len())];
        std::fs::write(&vfs_path, truncated).expect("Failed to write truncated file");

        // å°è¯•è¿ç§»æˆªæ–­çš„æ•°æ®åº“
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let result = coordinator2.migrate_single(DatabaseId::Vfs);

        assert!(
            result.is_err(),
            "Issue 4: Migrating a truncated database should return an error"
        );
    }

    /// æµ‹è¯•ï¼šæƒé™è¢«æ‹’ç»æ—¶åº”è¿”å›å‹å¥½é”™è¯¯ï¼ˆä»… Unixï¼‰
    ///
    /// é—®é¢˜ 4: æ•°æ®åº“æ–‡ä»¶æƒé™å˜æ›´
    #[cfg(unix)]
    #[test]
    fn test_issue4_readonly_database_file() {
        use std::os::unix::fs::PermissionsExt;

        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // å…ˆåˆ›å»ºæ•°æ®åº“
        coordinator
            .migrate_single(DatabaseId::LlmUsage)
            .expect("Initial migration should succeed");

        // è®¾ç½®ä¸ºåªè¯»
        let llm_path = get_database_path(&temp_dir, &DatabaseId::LlmUsage);
        let metadata = std::fs::metadata(&llm_path).expect("Failed to get metadata");
        let mut perms = metadata.permissions();
        perms.set_mode(0o444); // åªè¯»
        std::fs::set_permissions(&llm_path, perms).expect("Failed to set permissions");

        // å°è¯•å†æ¬¡è¿ç§»ï¼ˆåº”è¯¥ä¸éœ€è¦è¿ç§»ï¼Œä½†å¦‚æœéœ€è¦å†™å…¥ä¼šå¤±è´¥ï¼‰
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        // æ³¨æ„ï¼šå¦‚æœä¸éœ€è¦è¿ç§»ï¼Œè¿™ä¸ä¼šå¤±è´¥ã€‚ä½†å¦‚æœåº•å±‚éœ€è¦å†™æ“ä½œåˆ™ä¼šå¤±è´¥ã€‚
        let result = coordinator2.migrate_single(DatabaseId::LlmUsage);

        // æ¢å¤æƒé™ï¼ˆæ¸…ç†ï¼‰
        let metadata = std::fs::metadata(&llm_path).expect("Failed to get metadata");
        let mut perms = metadata.permissions();
        perms.set_mode(0o644);
        let _ = std::fs::set_permissions(&llm_path, perms);

        // åªè¯»æ–‡ä»¶çš„è¿ç§»ç»“æœå–å†³äºæ˜¯å¦éœ€è¦å†™æ“ä½œ
        // ä½†è‡³å°‘ä¸åº”è¯¥ panic
        if let Err(e) = &result {
            let err_msg = format!("{:?}", e);
            assert!(
                !err_msg.contains("panic"),
                "Issue 4: Read-only database should not cause panic"
            );
        }
    }

    // ============================================================================
    // ğŸ”´ é—®é¢˜ 5: repair_refinery_checksums é™é»˜è¦†ç›– checksum
    //
    // éªŒè¯ï¼šä¿®æ”¹å·²æœ‰è¿ç§»è®°å½•çš„ checksum â†’ è°ƒç”¨è¿ç§» â†’
    // checksum è¢«é™é»˜ä¿®å¤è€ŒéæŠ¥é”™
    // ============================================================================

    /// æµ‹è¯•ï¼šç¯¡æ”¹å·²æœ‰è¿ç§»è®°å½•çš„ checksum åï¼Œç³»ç»Ÿé™é»˜ä¿®å¤è€ŒéæŠ¥è­¦
    ///
    /// é—®é¢˜ 5: repair_refinery_checksums ç»•è¿‡äº† Refinery çš„å®‰å…¨æ£€æŸ¥
    #[test]
    fn test_issue5_tampered_checksum_is_silently_repaired() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        // æ­¥éª¤ 1: æ­£å¸¸è¿ç§»
        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("Initial VFS migration should succeed");

        // æ­¥éª¤ 2: ç¯¡æ”¹ä¸€ä¸ªå·²æœ‰è¿ç§»çš„ checksumï¼ˆæ¨¡æ‹Ÿæ¶æ„/æ„å¤–ä¿®æ”¹ï¼‰
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");

        // è®°å½•åŸå§‹ checksum
        let original_checksum: String = conn
            .query_row(
                "SELECT checksum FROM refinery_schema_history WHERE version = 20260130",
                [],
                |row| row.get(0),
            )
            .expect("Failed to get original checksum");

        // ç¯¡æ”¹ checksumï¼ˆæ¨¡æ‹Ÿè¿ç§»è„šæœ¬è¢«ä¿®æ”¹åçš„çŠ¶æ€ï¼‰
        let tampered_checksum = "TAMPERED_MALICIOUS_CHECKSUM_12345";
        conn.execute(
            "UPDATE refinery_schema_history SET checksum = ?1 WHERE version = 20260130",
            [tampered_checksum],
        )
        .expect("Failed to tamper checksum");

        drop(conn);

        // æ­¥éª¤ 3: å†æ¬¡æ‰§è¡Œè¿ç§»
        let mut coordinator2 = create_test_coordinator(&temp_dir);
        let result = coordinator2.migrate_single(DatabaseId::Vfs);

        // æ­¥éª¤ 4: æ— æ¡ä»¶æ£€æŸ¥ checksum æ˜¯å¦è¢«é™é»˜ä¿®å¤
        // è¿ç§»åº”è¯¥æˆåŠŸï¼ˆrepair_refinery_checksums ä¼šåœ¨ Refinery æ‰§è¡Œå‰ä¿®å¤ checksumï¼‰
        assert!(
            result.is_ok(),
            "Issue 5: Migration should succeed because repair_refinery_checksums \
             silently fixes the tampered checksum before Refinery sees it. Got error: {:?}",
            result.err()
        );

        let conn = Connection::open(&vfs_path).expect("Failed to reopen VFS db");
        let current_checksum: String = conn
            .query_row(
                "SELECT checksum FROM refinery_schema_history WHERE version = 20260130",
                [],
                |row| row.get(0),
            )
            .expect("Failed to get current checksum");

        // æ ¸å¿ƒæ–­è¨€ï¼šç¯¡æ”¹çš„ checksum ä¸€å®šè¢«é™é»˜ä¿®å¤å›äº†æ­£ç¡®å€¼
        assert_ne!(
            current_checksum, tampered_checksum,
            "Issue 5 CONFIRMED: Tampered checksum was silently repaired. \
             repair_refinery_checksums bypasses Refinery's divergent detection."
        );
        // ä¿®å¤åçš„å€¼åº”è¯¥ç­‰äºåŸå§‹å€¼
        assert_eq!(
            current_checksum, original_checksum,
            "Issue 5: Checksum was repaired back to the original value"
        );
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 6: å¢é‡å¤‡ä»½æ¢å¤è·¯å¾„å®Œå…¨ç¼ºå¤±
    //
    // éªŒè¯ï¼šbackup_incremental() å­˜åœ¨ä½†æ²¡æœ‰ restore_incremental()
    // ============================================================================

    /// æµ‹è¯•ï¼šå¯¹å¢é‡å¤‡ä»½ manifest è°ƒç”¨ restore()ï¼Œè¯æ˜ä¸ä¼šæ¢å¤ä»»ä½•æ•°æ®åº“
    ///
    /// é—®é¢˜ 6: å¢é‡å¤‡ä»½å¯ä»¥åˆ›å»ºä½†æ— æ³•æ¢å¤
    #[test]
    fn test_issue6_incremental_restore_silently_restores_nothing() {
        let backup_dir = TempDir::new().unwrap();
        let manager = BackupManager::new(backup_dir.path().to_path_buf());

        // æ„å»ºä¸€ä¸ªå¢é‡å¤‡ä»½ manifestï¼Œå…¶ä¸­åªæœ‰ _changes.json æ–‡ä»¶
        let mut manifest = BackupManifest::new("1.0.0");
        manifest.backup_id = "incr_test".to_string();
        manifest.is_incremental = true;
        manifest.incremental_base = Some("20260101_000000".to_string());
        manifest.add_file(crate::data_governance::backup::BackupFile {
            path: "vfs_changes.json".to_string(),
            size: 128,
            sha256: "aaa".to_string(),
            database_id: Some("vfs".to_string()),
        });

        // åˆ›å»ºå¤‡ä»½ç›®å½•åŠæ–‡ä»¶
        let sub = backup_dir.path().join("incr_test");
        std::fs::create_dir_all(&sub).unwrap();
        std::fs::write(
            sub.join("vfs_changes.json"),
            r#"[{"table":"resources","op":"INSERT"}]"#,
        )
        .unwrap();
        // é‡æ–°è®¡ç®— sha è®© verify é€šè¿‡
        let real_sha = {
            use sha2::{Digest, Sha256};
            let bytes = std::fs::read(sub.join("vfs_changes.json")).unwrap();
            hex::encode(Sha256::digest(&bytes))
        };
        manifest.files[0].sha256 = real_sha;
        manifest.files[0].size = std::fs::metadata(sub.join("vfs_changes.json"))
            .unwrap()
            .len();
        manifest.save_to_file(&sub.join("manifest.json")).unwrap();

        // è°ƒç”¨ restore â€” restore() å†…éƒ¨åªå¤„ç† .db ç»“å°¾çš„æ–‡ä»¶
        let result = manager.restore(&manifest);

        // å…³é”®æ–­è¨€ï¼šrestore å¯¹äºå¢é‡ manifest "æˆåŠŸ"äº†ï¼Œä½†å®é™…ä¸Šä»€ä¹ˆæ•°æ®åº“ä¹Ÿæ²¡æ¢å¤
        // Issue 6 å·²ä¿®å¤ï¼šrestore() åº”æ‹’ç»å¢é‡å¤‡ä»½å¹¶è¿”å›æ˜ç¡®é”™è¯¯
        assert!(
            result.is_err(),
            "Issue 6 FIXED: restore() should reject incremental backup"
        );
        let msg = format!("{}", result.unwrap_err());
        assert!(
            msg.contains("incremental") || msg.contains("å¢é‡"),
            "Issue 6 FIXED: error should mention incremental restore not supported, got: {}",
            msg
        );
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 7: å®¡è®¡æ—¥å¿—æ— å®¹é‡æ§åˆ¶
    //
    // éªŒè¯ï¼šAuditRepository åªæœ‰ INSERT/QUERYï¼Œæ— æ¸…ç†/è½®è½¬æœºåˆ¶
    // ============================================================================

    /// æµ‹è¯•ï¼šç¡®è®¤å®¡è®¡æ—¥å¿—æ— å®¹é‡æ§åˆ¶ï¼Œå¤§é‡æ—¥å¿—ä¸ä¼šè¢«æ¸…ç†
    ///
    /// é—®é¢˜ 7: audit.db ä¼šæ— é™å¢é•¿
    #[test]
    fn test_issue7_audit_log_grows_unbounded() {
        let temp_dir = create_test_dir();
        let audit_db_path = temp_dir.path().join("audit.db");

        let conn = Connection::open(&audit_db_path).expect("Failed to create audit database");
        AuditRepository::init(&conn).expect("Audit init should succeed");

        // å†™å…¥ 500 æ¡å®¡è®¡æ—¥å¿—æ¨¡æ‹Ÿé•¿æœŸè¿è¡Œ
        for i in 0..500 {
            AuditRepository::log_migration_complete(
                &conn,
                &format!("db_{}", i % 4),
                0,
                20260130,
                1,
                100,
            )
            .expect("Failed to log migration");
        }

        // éªŒè¯æ‰€æœ‰ 500 æ¡éƒ½åœ¨
        let total_count =
            AuditRepository::count_by_type(&conn, "Migration").expect("Count should succeed");
        assert_eq!(total_count, 500, "All 500 logs should exist");

        // é—®é¢˜ 7 ç¡®è®¤ï¼šæ²¡æœ‰ cleanup_old_logs æˆ–ç±»ä¼¼æ–¹æ³•
        // ä»¥ä¸‹æ³¨é‡Šçš„ä»£ç åº”è¯¥å­˜åœ¨ä½†ä¸å­˜åœ¨ï¼š
        // AuditRepository::cleanup_old_logs(&conn, 90).expect("Cleanup should succeed");

        // å†æ¬¡è®¡æ•° - ä»ç„¶æ˜¯ 500ï¼ˆæ²¡æœ‰æ¸…ç†æœºåˆ¶ï¼‰
        let after_count =
            AuditRepository::count_by_type(&conn, "Migration").expect("Count should succeed");
        assert_eq!(
            after_count, 500,
            "Issue 7 CONFIRMED: After attempting cleanup, all 500 logs still exist. \
             AuditRepository has no cleanup/rotation mechanism. \
             audit.db will grow unboundedly over time."
        );
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 8: pre_restore_backup å›æ»šè·¯å¾„æœªæµ‹è¯•
    //
    // éªŒè¯ï¼šrollback_from_pre_restore æ–¹æ³•å­˜åœ¨ä½†æµ‹è¯•å¥—ä»¶ä»æœªè°ƒç”¨
    // ============================================================================

    /// æµ‹è¯•ï¼šå«æŸåæ–‡ä»¶çš„ manifest è°ƒç”¨ restore()ï¼ŒéªŒè¯æ˜¯å¦è§¦å‘å›æ»šå¹¶æŠ¥é”™
    ///
    /// é—®é¢˜ 8: pre_restore å›æ»šè·¯å¾„æ— æµ‹è¯•è¦†ç›–
    #[test]
    fn test_issue8_restore_with_corrupted_backup_triggers_rollback_error() {
        let backup_dir = TempDir::new().unwrap();
        let manager = BackupManager::new(backup_dir.path().to_path_buf());

        // æ„å»ºä¸€ä¸ª manifestï¼Œå£°ç§°æœ‰ä¸¤ä¸ªæ•°æ®åº“æ–‡ä»¶
        let mut manifest = BackupManifest::new("1.0.0");
        manifest.backup_id = "rollback_test".to_string();

        // vfs.db æ­£å¸¸
        manifest.add_file(crate::data_governance::backup::BackupFile {
            path: "vfs.db".to_string(),
            size: 100,
            sha256: "will_be_fixed".to_string(),
            database_id: Some("vfs".to_string()),
        });
        // chat_v2.db å°†ä¼šæŸå
        manifest.add_file(crate::data_governance::backup::BackupFile {
            path: "chat_v2.db".to_string(),
            size: 50,
            sha256: "intentionally_wrong_checksum".to_string(),
            database_id: Some("chat_v2".to_string()),
        });

        // åˆ›å»ºå¤‡ä»½ç›®å½•
        let sub = backup_dir.path().join("rollback_test");
        std::fs::create_dir_all(&sub).unwrap();

        // åˆ›å»ºæœ‰æ•ˆçš„ vfs.db
        let vfs_conn = Connection::open(sub.join("vfs.db")).unwrap();
        vfs_conn.execute_batch("CREATE TABLE t(x)").unwrap();
        drop(vfs_conn);
        // ä¿®æ­£ vfs.db çš„ sha
        let vfs_sha = {
            use sha2::{Digest, Sha256};
            hex::encode(Sha256::digest(&std::fs::read(sub.join("vfs.db")).unwrap()))
        };
        manifest.files[0].sha256 = vfs_sha;
        manifest.files[0].size = std::fs::metadata(sub.join("vfs.db")).unwrap().len();

        // chat_v2.db å†™å…¥æŸåæ•°æ®ï¼ˆsha æ•…æ„ä¸åŒ¹é…ï¼‰
        std::fs::write(sub.join("chat_v2.db"), b"CORRUPTED DATA").unwrap();
        manifest.files[1].size = std::fs::metadata(sub.join("chat_v2.db")).unwrap().len();
        // sha ä¿æŒé”™è¯¯å€¼ â€” verify_internal ä¼šåœ¨è¿™é‡Œå¤±è´¥

        // å†™å…¥ manifest
        manifest.save_to_file(&sub.join("manifest.json")).unwrap();

        // è°ƒç”¨ restore â€” åº”è¯¥åœ¨ verify é˜¶æ®µå›  checksum ä¸åŒ¹é…è€Œå¤±è´¥
        let result = manager.restore(&manifest);

        assert!(
            result.is_err(),
            "Issue 8: restore() with corrupted backup file should fail"
        );
        let err_msg = format!("{}", result.unwrap_err());
        // éªŒè¯é”™è¯¯ä¿¡æ¯ä¸­æåˆ°æ ¡éªŒå’Œä¸åŒ¹é…
        assert!(
            err_msg.contains("æ ¡éªŒå’Œ")
                || err_msg.contains("checksum")
                || err_msg.contains("éªŒè¯å¤±è´¥"),
            "Issue 8: Error should mention checksum mismatch. Got: {}",
            err_msg
        );
        // æ³¨æ„ï¼šç”±äº verify åœ¨ pre_restore_backup ä¹‹å‰æ‰§è¡Œï¼Œ
        // å¦‚æœ verify å…ˆå¤±è´¥åˆ™ rollback_from_pre_restore æ ¹æœ¬ä¸ä¼šè¢«è°ƒç”¨ã€‚
        // è¿™æ„å‘³ç€åªæœ‰ verify é€šè¿‡ä½†ä¸ªåˆ« restore_single_database å¤±è´¥æ—¶
        // å›æ»šæ‰ä¼šè§¦å‘ â€” è¿™ç§è·¯å¾„æ›´éš¾æµ‹è¯•ï¼Œä¸”å½“å‰æ²¡æœ‰ä»»ä½•æµ‹è¯•è¦†ç›–ã€‚
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 9: VFS deleted_at åˆ—ç±»å‹ä¸ä¸€è‡´
    //
    // éªŒè¯ï¼šresources.deleted_at æ˜¯ INTEGER è€Œå…¶ä»–è¡¨æ˜¯ TEXT
    // ============================================================================

    /// æµ‹è¯•ï¼šVFS deleted_at å€¼ç±»å‹å·²ç»Ÿä¸€ä¸º TEXT
    ///
    /// Issue 9 ä¿®å¤éªŒè¯ï¼šV20260207 å°† resources.deleted_at çš„ INTEGER å€¼
    /// UPDATE ä¸º TEXTï¼ˆISO 8601ï¼‰ã€‚SQLite åŠ¨æ€ç±»å‹ä¸‹åˆ—å£°æ˜ä»ä¸º INTEGERï¼Œ
    /// ä½†å®é™…å­˜å‚¨çš„å€¼å’Œæ–°å†™å…¥çš„å€¼ç»Ÿä¸€ä¸º TEXT æ ¼å¼ã€‚
    ///
    /// éªŒè¯æ–¹å¼ï¼šæ’å…¥å¸¦ deleted_at çš„æµ‹è¯•è¡Œï¼Œç¡®è®¤å†™å…¥å’Œè¯»å–éƒ½æ˜¯ TEXTã€‚
    #[test]
    fn test_issue9_vfs_deleted_at_column_type_inconsistency() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("VFS migration should succeed");

        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");

        // æ’å…¥ä¸€æ¡å¸¦ TEXT æ ¼å¼ deleted_at çš„èµ„æºï¼ˆæ¨¡æ‹Ÿåº”ç”¨å±‚ç»Ÿä¸€å†™å…¥ TEXTï¼‰
        conn.execute(
            "INSERT INTO resources (id, hash, type, created_at, updated_at, deleted_at) \
             VALUES ('res_test_issue9', 'hash_issue9', 'note', 1000, 2000, '2026-02-07T00:00:00Z')",
            [],
        )
        .expect("Insert with TEXT deleted_at should succeed");

        // éªŒè¯å†™å…¥çš„å€¼ç¡®å®æ˜¯ TEXT ç±»å‹
        let actual_type: String = conn
            .query_row(
                "SELECT typeof(deleted_at) FROM resources WHERE id = 'res_test_issue9'",
                [],
                |row| row.get(0),
            )
            .expect("Query should succeed");

        assert_eq!(
            actual_type, "text",
            "Issue 9 FIXED: deleted_at value should be stored as text type"
        );

        // éªŒè¯å…¶ä»–è¡¨çš„ deleted_at å£°æ˜ç±»å‹ä»ä¸º TEXT
        let notes_decl_type: String = conn
            .query_row(
                "SELECT type FROM pragma_table_info('notes') WHERE name = 'deleted_at'",
                [],
                |row| row.get(0),
            )
            .expect("notes should have deleted_at");
        assert_eq!(notes_decl_type.to_uppercase(), "TEXT");

        // æ¸…ç†
        conn.execute("DELETE FROM resources WHERE id = 'res_test_issue9'", [])
            .ok();
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 10: å¹¶å‘å†™å…¥æ—¶çš„å¤‡ä»½ä¸€è‡´æ€§é—®é¢˜
    //
    // éªŒè¯ï¼šåœ¨ checkpoint å’Œ backup ä¹‹é—´æœ‰æ–°å†™å…¥æ—¶çš„è¡Œä¸º
    // ============================================================================

    /// æµ‹è¯•ï¼šcheckpoint åå†™å…¥çš„æ•°æ®å¯èƒ½ä¸è¢« Backup API æ•è·
    ///
    /// é—®é¢˜ 10: backup_single_database ä¸­ checkpoint å’Œ Backup::new ä¹‹é—´æ— é”
    #[test]
    fn test_issue10_checkpoint_then_write_then_backup_may_lose_data() {
        let temp_dir = create_test_dir();
        let src_path = temp_dir.path().join("source.db");
        let dst_path = temp_dir.path().join("backup.db");

        // æ­¥éª¤ 1: åˆ›å»ºæºæ•°æ®åº“
        let src = Connection::open(&src_path).expect("Failed to create source db");
        src.execute_batch("PRAGMA journal_mode=WAL").unwrap();
        src.execute_batch(
            "CREATE TABLE items (id INTEGER PRIMARY KEY, value TEXT);
             INSERT INTO items (value) VALUES ('row_1');
             INSERT INTO items (value) VALUES ('row_2');",
        )
        .unwrap();

        // æ­¥éª¤ 2: æ¨¡æ‹Ÿ backup_single_database çš„æ“ä½œé¡ºåºï¼ˆä¸æºç ä¸€è‡´ï¼‰
        // 2a: checkpointï¼ˆä¸ coordinator.rs ä¸€è‡´ï¼‰
        src.execute_batch("PRAGMA wal_checkpoint(TRUNCATE)")
            .unwrap();

        // 2b: åœ¨ checkpoint å’Œ Backup::new ä¹‹é—´å†™å…¥ï¼ˆæ¨¡æ‹Ÿå¹¶å‘ï¼‰
        src.execute("INSERT INTO items (value) VALUES ('after_checkpoint')", [])
            .unwrap();

        // 2c: æ‰§è¡Œ Backup APIï¼ˆä¸ coordinator.rs ä¸€è‡´ï¼‰
        {
            let mut dst = Connection::open(&dst_path).expect("Failed to create backup db");
            let backup = rusqlite::backup::Backup::new(&src, &mut dst).unwrap();
            backup
                .run_to_completion(5, std::time::Duration::from_millis(10), None)
                .unwrap();
        }

        // æ­¥éª¤ 3: æ¯”è¾ƒæºå’Œå¤‡ä»½çš„è¡Œæ•°
        let src_count: i64 = src
            .query_row("SELECT COUNT(*) FROM items", [], |row| row.get(0))
            .unwrap();
        let dst = Connection::open(&dst_path).unwrap();
        let dst_count: i64 = dst
            .query_row("SELECT COUNT(*) FROM items", [], |row| row.get(0))
            .unwrap();

        assert_eq!(src_count, 3, "Source should have 3 rows");
        // æ³¨æ„ï¼šSQLite Backup API å®é™…ä¸Šèƒ½è¯»åˆ° WAL ä¸­çš„æ•°æ®ï¼ˆå› ä¸ºå…±ç”¨åŒä¸€ä¸ªè¿æ¥çš„ cacheï¼‰ï¼Œ
        // æ‰€ä»¥ dst_count å¯èƒ½ä¹Ÿæ˜¯ 3ã€‚ä½†åœ¨å¤šè¿›ç¨‹/å¤šè¿æ¥åœºæ™¯ä¸‹ï¼Œç«æ€çª—å£ç¡®å®å­˜åœ¨ã€‚
        // é—®é¢˜ 10 çš„æ ¸å¿ƒåœ¨äºä»£ç æ²¡æœ‰ä½¿ç”¨ BEGIN IMMEDIATE æ¥é˜²æ­¢è¿™ç§ç«æ€ã€‚
        // è¿™é‡Œæˆ‘ä»¬éªŒè¯çš„æ˜¯ backup ä»£ç è·¯å¾„ä¸­ checkpoint â†’ backup ä¹‹é—´æ²¡æœ‰äº‹åŠ¡ä¿æŠ¤ã€‚
        if dst_count == src_count {
            // å•è¿æ¥åœºæ™¯ä¸‹ Backup API æ°å¥½èƒ½è¯»åˆ° WAL â€” ä½†è¿™ä¸ä»£è¡¨å¤šè¿æ¥å®‰å…¨
            assert_eq!(
                dst_count, 3,
                "Issue 10: In single-connection test, Backup API happens to see WAL data. \
                 But backup_single_database uses separate connections and no BEGIN IMMEDIATE, \
                 so multi-process races are unprotected."
            );
        } else {
            assert_eq!(
                dst_count, 2,
                "Issue 10 CONFIRMED: Backup missed the post-checkpoint write"
            );
        }
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 11: ç£ç›˜ç©ºé—´è€—å°½åœºæ™¯æ— å¤„ç†
    //
    // éªŒè¯ï¼šè¿ç§»å‰æ²¡æœ‰ç£ç›˜ç©ºé—´æ£€æŸ¥
    // ============================================================================

    /// æµ‹è¯•ï¼šrun_all äº§ç”Ÿçš„é”™è¯¯ä¸­æ²¡æœ‰ç£ç›˜ç©ºé—´ç›¸å…³çš„è¯Šæ–­ä¿¡æ¯
    ///
    /// é—®é¢˜ 11: run_all å¼€å§‹å‰ä¸æ£€æŸ¥å¯ç”¨ç£ç›˜ç©ºé—´
    /// æˆ‘ä»¬æ— æ³•åœ¨æµ‹è¯•ä¸­çœŸæ­£å¡«æ»¡ç£ç›˜ï¼Œä½†å¯ä»¥éªŒè¯ MigrationError æšä¸¾
    /// æ²¡æœ‰ç£ç›˜ç©ºé—´ä¸è¶³çš„ä¸“ç”¨å˜ä½“ã€‚
    #[test]
    fn test_issue11_migration_error_has_no_disk_space_variant() {
        // éªŒè¯ MigrationError çš„æ‰€æœ‰å˜ä½“ä¸­æ²¡æœ‰"ç£ç›˜ç©ºé—´"ç›¸å…³çš„
        // è¿™è¯æ˜äº† run_all ä¸å¯èƒ½äº§ç”Ÿ"ç£ç›˜ä¸è¶³"çš„å‹å¥½é”™è¯¯
        let io_err = MigrationError::Io(std::io::Error::new(
            std::io::ErrorKind::Other,
            "No space left on device",
        ));
        let msg = format!("{}", io_err);

        // MigrationError::Io ä¼šä¼ æ’­åŸå§‹ IO é”™è¯¯ï¼Œä½†ä¸ä¼šæ·»åŠ å¯æ“ä½œæç¤º
        assert!(
            !msg.contains("ç£ç›˜") && !msg.contains("disk space") && !msg.contains("å»ºè®®"),
            "Issue 11 would be FIXED if MigrationError included actionable disk space hints. \
             Currently the Io variant just wraps the raw error: {}",
            msg
        );

        // åŒæ—¶éªŒè¯ run_all å¯¹ç©ºç›®å½•ç›´æ¥æ‰§è¡Œï¼Œæ— ä»»ä½•é¢„æ£€æŸ¥
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);
        // å¦‚æœæœ‰é¢„æ£€æŸ¥ï¼Œå¯¹äºç©ºç›®å½•ä¹Ÿåº”è¯¥èƒ½å‘ç°"æ— æ³•ç¡®å®šç£ç›˜ç©ºé—´"ä¹‹ç±»çš„ä¿¡æ¯
        // ä½†å®é™…ä¸Šç›´æ¥æˆåŠŸäº†
        assert!(coordinator.run_all().is_ok());
    }

    // ============================================================================
    // ğŸŸ¡ é—®é¢˜ 12: Schema fingerprint è·¨ SQLite ç‰ˆæœ¬å¯èƒ½è¯¯æŠ¥
    //
    // éªŒè¯ï¼šcompute_schema_fingerprint å°†åŸå§‹ SQL æ–‡æœ¬çº³å…¥ hash
    // ============================================================================

    /// æµ‹è¯•ï¼šä¿®æ”¹ index SQL çš„ç©ºç™½å fingerprint ä¼šå˜åŒ–
    ///
    /// é—®é¢˜ 12: compute_schema_fingerprint å°†åŸå§‹ SQL çº³å…¥ hashï¼Œ
    /// å¯¼è‡´ä¸åŒ SQLite ç‰ˆæœ¬çš„æ ¼å¼å·®å¼‚å¯èƒ½è§¦å‘ fail-close
    #[test]
    fn test_issue12_schema_fingerprint_changes_with_sql_formatting() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);

        coordinator
            .migrate_single(DatabaseId::Vfs)
            .expect("VFS migration should succeed");

        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");

        // è¯»å– fingerprint è¡¨ä¸­çš„å€¼ï¼ˆè¿ç§»åè‡ªåŠ¨å†™å…¥ï¼‰
        let has_fp_table: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='__governance_schema_fingerprints')",
                [],
                |row| row.get(0),
            )
            .unwrap_or(false);

        if !has_fp_table {
            // å¦‚æœæ²¡æœ‰ fingerprint è¡¨ï¼Œè¯´æ˜åŠŸèƒ½å¯èƒ½è¿˜æ²¡å¯ç”¨ï¼Œè·³è¿‡
            return;
        }

        let original_fp: String = conn
            .query_row(
                "SELECT fingerprint FROM __governance_schema_fingerprints ORDER BY rowid DESC LIMIT 1",
                [],
                |row| row.get(0),
            )
            .expect("Failed to read fingerprint");

        // éªŒè¯ fingerprint çš„ hash è¾“å…¥ç¡®å®åŒ…å« sqlite_master çš„åŸå§‹ SQL æ–‡æœ¬
        // æ–¹æ³•ï¼šæŸ¥çœ‹ sqlite_master ä¸­çš„ç´¢å¼• SQL æ–‡æœ¬æ˜¯å¦éç©º
        let index_sql_texts: Vec<String> = conn
            .prepare(
                "SELECT IFNULL(sql, '') FROM sqlite_master
                 WHERE type='index' AND name NOT LIKE 'sqlite_autoindex%'
                 ORDER BY name",
            )
            .unwrap()
            .query_map([], |row| row.get(0))
            .unwrap()
            .filter_map(|r| r.ok())
            .collect();

        let non_empty_count = index_sql_texts.iter().filter(|s| !s.is_empty()).count();
        assert!(
            non_empty_count > 0,
            "Issue 12: There should be index SQL text in sqlite_master"
        );

        // æ ¸å¿ƒæ–­è¨€ï¼šcompute_schema_fingerprint ä½¿ç”¨çš„æ ¼å¼æ˜¯
        // "idx:{name}:{sql}\n"ï¼Œæ‰€ä»¥ä»»ä½• SQL æ–‡æœ¬æ ¼å¼å˜åŒ–éƒ½ä¼šæ”¹å˜ fingerprintã€‚
        // ä¸åŒ SQLite ç‰ˆæœ¬å¯èƒ½å¯¹åŒä¸€ CREATE INDEX è¾“å‡ºä¸åŒçš„ç©ºç™½/å¤§å°å†™ã€‚
        assert!(
            !original_fp.is_empty(),
            "Issue 12 CONFIRMED: Schema fingerprint is computed and stored. \
             Since it hashes raw SQL text from sqlite_master (idx:name:sql format), \
             SQLite version upgrades that change SQL formatting will cause fingerprint drift."
        );
    }

    // ============================================================================
    // ğŸŸ¢ é—®é¢˜ 13: tests.rs å’Œ migration_tests.rs çº¦ 60% é‡å¤
    //
    // éªŒè¯ï¼šä¸¤ä¸ªæ–‡ä»¶æœ‰é«˜åº¦ç›¸ä¼¼çš„æµ‹è¯•
    // ============================================================================

    /// æµ‹è¯•ï¼šç¡®è®¤ä¸¤ä¸ªæµ‹è¯•æ–‡ä»¶çš„æµ‹è¯•åç§°é‡å¤
    ///
    /// é—®é¢˜ 13: æµ‹è¯•æ–‡ä»¶é—´çº¦ 60% é‡å¤
    #[test]
    fn test_issue13_duplicate_test_functions_exist() {
        // é€šè¿‡æºç å†…å®¹éªŒè¯ä¸¤ä¸ªæ–‡ä»¶æœ‰ç›¸åŒçš„æµ‹è¯•å‡½æ•°å
        let tests_rs = include_str!("tests.rs");
        let migration_tests_rs = include_str!("migration_tests.rs");

        // ç»Ÿè®¡ä¸¤ä¸ªæ–‡ä»¶ä¸­éƒ½å‡ºç°çš„æµ‹è¯•é€»è¾‘æ¨¡å¼
        let shared_patterns = [
            "migrate_single(DatabaseId::Vfs)",
            "run_all()",
            "test_migration_idempotency",
            "needs_migration",
            "check_dependencies",
            "MigrationReport::default()",
            "DependencyNotSatisfied",
            "VerificationFailed",
            "pending_migrations_count",
            "migration_set_properties",
            "Schema Registry",
        ];

        let mut both_count = 0;
        for pattern in &shared_patterns {
            let in_tests = tests_rs.contains(pattern);
            let in_migration = migration_tests_rs.contains(pattern);
            if in_tests && in_migration {
                both_count += 1;
            }
        }

        assert!(
            both_count >= 8,
            "Issue 13 CONFIRMED: {} out of {} checked patterns appear in BOTH test files. \
             tests.rs and migration_tests.rs have significant duplication.",
            both_count,
            shared_patterns.len()
        );
    }

    // ============================================================================
    // ğŸŸ¢ é—®é¢˜ 14: è¿ç§»æ•°é‡ç¡¬ç¼–ç åœ¨å¸¸é‡ä¸­
    //
    // éªŒè¯ï¼šmigration_tests.rs ä¸­çš„å¸¸é‡éœ€è¦æ‰‹åŠ¨ç»´æŠ¤
    // ============================================================================

    /// æµ‹è¯•ï¼šç¡®è®¤ç¡¬ç¼–ç å¸¸é‡ä¸å®é™…è¿ç§»æ•°é‡çš„ä¸€è‡´æ€§
    ///
    /// é—®é¢˜ 14: æ¯æ¬¡æ–°å¢è¿ç§»è„šæœ¬éƒ½éœ€è¦æ‰‹åŠ¨æ›´æ–°å¸¸é‡
    #[test]
    fn test_issue14_hardcoded_migration_counts_must_match() {
        // éªŒè¯è¿ç§»æ•°é‡ >= å·²çŸ¥æœ€å°å€¼ï¼ˆé¿å…ç¡¬ç¼–ç å…·ä½“æ•°å­—ï¼Œ
        // æ–°å¢è¿ç§»æ—¶æ­¤æµ‹è¯•è‡ªåŠ¨é€šè¿‡ï¼Œåˆ é™¤è¿ç§»æ—¶æ­¤æµ‹è¯•ä¼š catchï¼‰
        let actual_vfs = VFS_MIGRATION_SET.count();
        let actual_chat_v2 = CHAT_V2_MIGRATION_SET.count();
        let actual_mistakes = MISTAKES_MIGRATIONS.count();
        let actual_llm_usage = LLM_USAGE_MIGRATION_SET.count();

        assert!(
            actual_vfs >= 9,
            "Issue 14: VFS should have at least 9 migrations, got {}",
            actual_vfs
        );
        assert!(
            actual_chat_v2 >= 7,
            "Issue 14: ChatV2 should have at least 7 migrations, got {}",
            actual_chat_v2
        );
        assert!(
            actual_mistakes >= 4,
            "Issue 14: Mistakes should have at least 4 migrations, got {}",
            actual_mistakes
        );
        assert!(
            actual_llm_usage >= 3,
            "Issue 14: LlmUsage should have at least 3 migrations, got {}",
            actual_llm_usage
        );
    }

    // ============================================================================
    // ğŸŸ¢ é—®é¢˜ 16: åŒæ­¥æ¨¡å—åŸºæœ¬ä¸ºç©ºå£³
    //
    // éªŒè¯ï¼šsync æ¨¡å—å­˜åœ¨ä½†æ— å®è´¨åŒæ­¥é€»è¾‘æµ‹è¯•
    // ============================================================================

    /// æµ‹è¯•ï¼šç¡®è®¤åŒæ­¥æ¨¡å—ç¼ºå°‘ä¸å®é™…æ•°æ®åº“äº¤äº’çš„é›†æˆæµ‹è¯•
    ///
    /// é—®é¢˜ 16: sync æ¨¡å—åªæœ‰å•å…ƒæµ‹è¯•ï¼Œæ— é›†æˆæµ‹è¯•
    #[test]
    fn test_issue16_sync_module_has_no_integration_tests_with_real_db() {
        // é—®é¢˜ 16 ç¡®è®¤ï¼šsync/mod.rs æœ‰ä»¥ä¸‹æµ‹è¯•ï¼Œä½†éƒ½æ˜¯çº¯å†…å­˜æ¨¡æ‹Ÿï¼š
        // - test_detect_no_conflicts: çº¯ SyncManifest æ¯”è¾ƒ
        // - test_detect_schema_mismatch: çº¯ SyncManifest æ¯”è¾ƒ
        // - test_sync_keep_local: ä½¿ç”¨ç©ºçš„ ConflictDetectionResult
        // - test_record_conflict_detection: ä½¿ç”¨æ„é€ çš„ RecordSnapshot
        //
        // ç¼ºå°‘ï¼š
        // - å®é™…æ‰“å¼€å·²è¿ç§»çš„æ•°æ®åº“ï¼Œè¯»å– __change_log è¡¨çš„æµ‹è¯•
        // - å®é™…æ‰§è¡Œ get_pending_changes â†’ upload â†’ download â†’ apply çš„é›†æˆæµ‹è¯•
        // - æµ‹è¯•åŒæ­¥è§¦å‘åæ•°æ®åº“ schema ä¸åŒ¹é…æ—¶çš„è¡Œä¸º

        // å°è¯•åœ¨çœŸå®è¿ç§»åçš„æ•°æ®åº“ä¸Šè·å–å¾…åŒæ­¥å˜æ›´
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);
        coordinator.run_all().expect("Migration should succeed");

        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");

        // éªŒè¯ __change_log è¡¨å­˜åœ¨
        let has_change_log: bool = conn
            .query_row(
                "SELECT EXISTS(SELECT 1 FROM sqlite_master WHERE type='table' AND name='__change_log')",
                [],
                |row| row.get(0),
            )
            .expect("Failed to check __change_log");

        assert!(
            has_change_log,
            "Issue 16: VFS database has __change_log table after migration, \
             but there are no integration tests that exercise sync operations \
             on this real table."
        );
    }

    // ============================================================================
    // é¢å¤–éªŒè¯æµ‹è¯•ï¼šä» Legacy æ¢å¤æ—¶çš„æ•°æ®å­˜æ´»
    // ============================================================================

    /// æµ‹è¯•ï¼šMistakes Legacy æ¢å¤æ—¶ä¿ç•™å·²æœ‰çš„ anki_cards æ•°æ®
    ///
    /// ç»¼åˆé—®é¢˜ 1+2: Legacy æ•°æ®åº“æ¢å¤ + æ•°æ®å­˜æ´»
    #[test]
    fn test_combined_issue1_2_legacy_mistakes_data_survives_recovery() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let mut coordinator = create_test_coordinator(&temp_dir);
        let db_path = temp_dir.path().join("mistakes.db");

        // åˆ›å»ºä¸€ä¸ª Legacy Mistakes æ•°æ®åº“ï¼ˆæœ‰æ•°æ®ï¼‰
        let conn = Connection::open(&db_path).expect("Failed to create legacy mistakes db");
        conn.execute_batch(
            "
            CREATE TABLE migration_progress (category TEXT PRIMARY KEY, status TEXT NOT NULL);
            CREATE TABLE mistakes (id TEXT PRIMARY KEY, created_at TEXT NOT NULL, status TEXT NOT NULL, question_images TEXT NOT NULL, updated_at TEXT NOT NULL DEFAULT '');
            CREATE TABLE document_tasks (
                id TEXT PRIMARY KEY,
                document_id TEXT NOT NULL DEFAULT '',
                original_document_name TEXT NOT NULL DEFAULT '',
                segment_index INTEGER NOT NULL DEFAULT 0,
                content_segment TEXT NOT NULL DEFAULT '',
                status TEXT NOT NULL DEFAULT 'Pending',
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                anki_generation_options_json TEXT NOT NULL DEFAULT '{}'
            );
            CREATE TABLE anki_cards (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                front TEXT NOT NULL,
                back TEXT NOT NULL,
                source_type TEXT NOT NULL DEFAULT '',
                source_id TEXT NOT NULL DEFAULT '',
                card_order_in_task INTEGER DEFAULT 0,
                created_at TEXT NOT NULL DEFAULT '',
                updated_at TEXT NOT NULL DEFAULT '',
                template_id TEXT,
                text TEXT
            );
            ",
        )
        .expect("Failed to build legacy schema");

        // æ’å…¥ä¸šåŠ¡æ•°æ®
        conn.execute(
            "INSERT INTO anki_cards (id, task_id, front, back, source_type, source_id)
             VALUES ('legacy-card-1', 'task-1', 'What is Rust?', 'A systems programming language', 'manual', 'src-1')",
            [],
        ).expect("Failed to insert legacy card 1");
        conn.execute(
            "INSERT INTO anki_cards (id, task_id, front, back, source_type, source_id)
             VALUES ('legacy-card-2', 'task-2', 'What is ownership?', 'Memory management without GC', 'manual', 'src-2')",
            [],
        ).expect("Failed to insert legacy card 2");
        conn.execute(
            "INSERT INTO mistakes (id, created_at, status, question_images)
             VALUES ('mistake-1', '2026-01-01T00:00:00Z', 'active', '[]')",
            [],
        )
        .expect("Failed to insert legacy mistake");

        drop(conn);

        // æ‰§è¡Œè¿ç§»ï¼ˆåº”è¯¥æ¢å¤ legacy schema åˆ°æœ€æ–°ï¼‰
        let report = coordinator
            .migrate_single(DatabaseId::Mistakes)
            .expect("Legacy mistakes recovery should succeed");
        assert!(report.success);

        // éªŒè¯æ•°æ®å­˜æ´»
        let conn = Connection::open(&db_path).expect("Failed to reopen mistakes db");

        let card_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM anki_cards", [], |row| row.get(0))
            .expect("Failed to count cards");
        assert_eq!(
            card_count, 2,
            "Combined Issue 1+2: Both legacy anki_cards should survive recovery migration"
        );

        let front: String = conn
            .query_row(
                "SELECT front FROM anki_cards WHERE id = 'legacy-card-1'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query card front");
        assert_eq!(
            front, "What is Rust?",
            "Combined Issue 1+2: Legacy card content should be preserved exactly"
        );

        let mistake_count: i64 = conn
            .query_row("SELECT COUNT(*) FROM mistakes", [], |row| row.get(0))
            .expect("Failed to count mistakes");
        assert_eq!(
            mistake_count, 1,
            "Combined Issue 1+2: Legacy mistakes data should survive recovery"
        );
    }

    // ============================================================================
    // é¢å¤–éªŒè¯ï¼šè¿ç§»åå®¡è®¡æ—¥å¿—ä¸­ repair_refinery_checksums æ“ä½œçš„å¯è¿½æº¯æ€§
    // ============================================================================

    /// æµ‹è¯•ï¼šrepair_refinery_checksums æ“ä½œæ²¡æœ‰å®¡è®¡æ—¥å¿—è®°å½•
    ///
    /// é—®é¢˜ 5 è¡¥å……: é™é»˜ä¿®å¤åº”è¯¥è¢«å®¡è®¡
    #[test]
    fn test_issue5_supplement_checksum_repair_not_audited() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);

        // åˆå§‹åŒ–å®¡è®¡æ•°æ®åº“
        let audit_db_path = init_audit_database(&temp_dir);

        let mut coordinator = create_test_coordinator_with_audit(&temp_dir);

        // æ‰§è¡Œè¿ç§»
        coordinator.run_all().expect("Migration should succeed");

        // ç¯¡æ”¹ checksum
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");
        conn.execute(
            "UPDATE refinery_schema_history SET checksum = 'TAMPERED' WHERE version = 20260130",
            [],
        )
        .expect("Failed to tamper checksum");
        drop(conn);

        // é‡æ–°è¿ç§»ï¼ˆä¼šè§¦å‘ repair_refinery_checksumsï¼‰
        let mut coordinator2 = create_test_coordinator_with_audit(&temp_dir);
        let _ = coordinator2.run_all();

        // æŸ¥çœ‹å®¡è®¡æ—¥å¿—ä¸­æ˜¯å¦æœ‰ checksum ä¿®å¤çš„è®°å½•
        let audit_conn = Connection::open(&audit_db_path).expect("Failed to open audit db");
        let all_logs = AuditRepository::query(
            &audit_conn,
            AuditFilter {
                limit: Some(100),
                ..Default::default()
            },
        )
        .expect("Query should succeed");

        // æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ—¥å¿—æåˆ° checksum repair
        let has_repair_log = all_logs.iter().any(|log| {
            let details = serde_json::to_string(&log.details).unwrap_or_default();
            let error_msg = log.error_message.as_deref().unwrap_or("");
            let target = &log.target;
            details.contains("checksum")
                || details.contains("repair")
                || error_msg.contains("checksum")
                || error_msg.contains("repair")
                || target.contains("checksum")
                || target.contains("repair")
        });

        // Issue 5 å·²ä¿®å¤ï¼šchecksum ä¿®å¤ç°åœ¨åº”è¯¥æœ‰å®¡è®¡æ—¥å¿—
        assert!(
            has_repair_log,
            "Issue 5 FIXED: repair_refinery_checksums should leave an audit trail. \
             Found {} audit logs total but none mention checksum repair. \
             Check log_checksum_repair_audit() implementation.",
            all_logs.len()
        );
    }

    // ============================================================================
    // åˆå§‹åŒ–æµç¨‹ä¸­çš„æ•°æ®å­˜æ´»æµ‹è¯•
    // ============================================================================

    /// æµ‹è¯•ï¼šå®Œæ•´çš„åˆå§‹åŒ– â†’ æ’å…¥æ•°æ® â†’ å†æ¬¡åˆå§‹åŒ– â†’ æ•°æ®å­˜æ´»
    ///
    /// ç»¼åˆéªŒè¯ï¼šåˆå§‹åŒ–æµç¨‹çš„å¹‚ç­‰æ€§å’Œæ•°æ®å®‰å…¨æ€§
    #[test]
    fn test_initialization_preserves_existing_data() {
        let temp_dir = create_test_dir();

        // ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
        let result = initialize_with_report(temp_dir.path());
        assert!(result.is_ok(), "First initialization should succeed");

        // æ’å…¥æ•°æ®åˆ° VFSï¼ˆä½¿ç”¨çœŸå® schemaï¼šresources æ—  title åˆ—ï¼‰
        let vfs_path = temp_dir.path().join("databases").join("vfs.db");
        let conn = Connection::open(&vfs_path).expect("Failed to open VFS db");
        conn.execute(
            "INSERT INTO resources (id, type, hash, storage_mode, data, ref_count, created_at, updated_at)
             VALUES ('preserved-001', 'note', 'hash_xyz_unique', 'inline', 'Must Survive Init Content', 1, 1706745600000, 1706745600000)",
            [],
        ).expect("Failed to insert data");
        drop(conn);

        // ç¬¬äºŒæ¬¡åˆå§‹åŒ–ï¼ˆåº”ä¿ç•™æ•°æ®ï¼‰
        let result2 = initialize_with_report(temp_dir.path());
        assert!(result2.is_ok(), "Second initialization should succeed");

        // éªŒè¯æ•°æ®å­˜æ´»
        let conn = Connection::open(&vfs_path).expect("Failed to reopen VFS db");
        let data_value: String = conn
            .query_row(
                "SELECT data FROM resources WHERE id = 'preserved-001'",
                [],
                |row| row.get(0),
            )
            .expect("Failed to query preserved data");
        assert_eq!(
            data_value, "Must Survive Init Content",
            "Data inserted between initializations should be preserved"
        );
    }

    // ============================================================================
    // è¾¹ç•Œæ¡ä»¶ï¼šç©ºæ•°æ®åº“çš„ needs_migration è¡Œä¸º
    // ============================================================================

    /// æµ‹è¯•ï¼šç©ºæ–‡ä»¶ä¸Šè°ƒç”¨ needs_migrationï¼Œåº”è¿”å› true æˆ–é”™è¯¯ï¼Œä¸åº” panic
    #[test]
    fn test_needs_migration_on_empty_file() {
        let temp_dir = create_test_dir();
        setup_test_directories(&temp_dir);
        let coordinator = create_test_coordinator(&temp_dir);

        // åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ï¼ˆä¸æ˜¯ SQLite æ•°æ®åº“ï¼‰
        let vfs_path = get_database_path(&temp_dir, &DatabaseId::Vfs);
        std::fs::write(&vfs_path, b"").expect("Failed to create empty file");

        // åˆ°è¿™é‡Œæ²¡æœ‰ panic å°±è¯´æ˜å‡½æ•°æ­£ç¡®å¤„ç†äº†å¼‚å¸¸è¾“å…¥
        let result = coordinator.needs_migration(&DatabaseId::Vfs);
        match result {
            Ok(needs) => {
                // ç©ºæ–‡ä»¶åº”è¯¥éœ€è¦è¿ç§»ï¼ˆæˆ–è¢«è§†ä¸ºæŸåï¼‰
                assert!(needs, "Empty file should be treated as needing migration");
            }
            Err(_) => {
                // è¿”å›é”™è¯¯ä¹Ÿæ˜¯å¯æ¥å—çš„è¡Œä¸º
            }
        }
    }
}
