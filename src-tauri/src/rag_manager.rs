use crate::models::{
    DocumentChunk, DocumentChunkWithEmbedding, RetrievedChunk, RagQueryOptions, 
    KnowledgeBaseStatusPayload, DocumentProcessingStatus, DocumentProcessingStage, 
    RagQueryResponse, AppError
};
use crate::vector_store::{SqliteVectorStore, VectorStore};
use crate::llm_manager::LLMManager;
use crate::database::Database;
use crate::file_manager::FileManager;
use std::sync::Arc;
use std::collections::HashMap;
use uuid::Uuid;
use tauri::{Window, Emitter};
use serde_json::Value;
use base64::{Engine as _, engine::general_purpose};
use regex::Regex;

type Result<T> = std::result::Result<T, AppError>;

/// æ–‡æ¡£åˆ†å—ç­–ç•¥
#[derive(Debug, Clone, PartialEq)]
pub enum ChunkingStrategy {
    FixedSize,
    Semantic,
}

/// åˆ†å—é…ç½®
#[derive(Debug, Clone)]
pub struct ChunkingConfig {
    pub strategy: ChunkingStrategy,
    pub chunk_size: usize,
    pub chunk_overlap: usize,
    pub min_chunk_size: usize,
}

impl Default for ChunkingConfig {
    fn default() -> Self {
        Self {
            strategy: ChunkingStrategy::FixedSize,
            chunk_size: 512,
            chunk_overlap: 50,
            min_chunk_size: 20,
        }
    }
}

/// RAGç®¡ç†å™¨ - åè°ƒæ•´ä¸ªRAGæµç¨‹
pub struct RagManager {
    vector_store: SqliteVectorStore,
    llm_manager: Arc<LLMManager>,
    file_manager: Arc<FileManager>,
    database: Arc<Database>,
}

impl RagManager {
    /// åˆ›å»ºæ–°çš„RAGç®¡ç†å™¨å®ä¾‹
    pub fn new(
        database: Arc<Database>, 
        llm_manager: Arc<LLMManager>, 
        file_manager: Arc<FileManager>
    ) -> Result<Self> {
        let vector_store = SqliteVectorStore::new(database.clone())?;
        
        Ok(Self {
            vector_store,
            llm_manager,
            file_manager,
            database,
        })
    }
    
    /// æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    pub async fn add_documents_to_knowledge_base(
        &self, 
        file_paths: Vec<String>, 
        window: Window
    ) -> Result<String> {
        self.add_documents_to_knowledge_base_with_library(file_paths, window, None).await
    }
    
    /// æ·»åŠ æ–‡æ¡£åˆ°æŒ‡å®šåˆ†åº“
    pub async fn add_documents_to_knowledge_base_with_library(
        &self, 
        file_paths: Vec<String>, 
        window: Window,
        sub_library_id: Option<String>
    ) -> Result<String> {
        println!("ğŸš€ å¼€å§‹å¤„ç† {} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“", file_paths.len());
        
        let mut processed_documents = Vec::new();
        let mut total_chunks = 0;
        
        for (index, file_path) in file_paths.iter().enumerate() {
            let progress = (index as f32) / (file_paths.len() as f32);
            
            // å‘é€å¤„ç†çŠ¶æ€æ›´æ–°
            self.emit_processing_status(&window, "overall", "processing", progress, 
                &format!("æ­£åœ¨å¤„ç†æ–‡æ¡£ {}/{}", index + 1, file_paths.len())).await;
            
            match self.process_single_document_with_library(file_path, &window, sub_library_id.as_deref()).await {
                Ok(chunk_count) => {
                    total_chunks += chunk_count;
                    processed_documents.push(file_path.clone());
                    println!("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {} ({} ä¸ªå—)", file_path, chunk_count);
                }
                Err(e) => {
                    println!("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {} - {}", file_path, e);
                    self.emit_processing_status(&window, "overall", "error", progress,
                        &format!("æ–‡æ¡£å¤„ç†å¤±è´¥: {}", e)).await;
                }
            }
        }
        
        // å‘é€å®ŒæˆçŠ¶æ€
        self.emit_processing_status(&window, "overall", "completed", 1.0,
            &format!("å¤„ç†å®Œæˆï¼š{} ä¸ªæ–‡æ¡£ï¼Œ{} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks)).await;
        
        Ok(format!("æˆåŠŸå¤„ç† {} ä¸ªæ–‡æ¡£ï¼Œå…± {} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks))
    }
    
    /// å¤„ç†å•ä¸ªæ–‡æ¡£
    async fn process_single_document(&self, file_path: &str, window: &Window) -> Result<usize> {
        self.process_single_document_with_library(file_path, window, None).await
    }
    
    /// å¤„ç†å•ä¸ªæ–‡æ¡£åˆ°æŒ‡å®šåˆ†åº“
    async fn process_single_document_with_library(&self, file_path: &str, window: &Window, sub_library_id: Option<&str>) -> Result<usize> {
        let document_id = Uuid::new_v4().to_string();
        let file_name = std::path::Path::new(file_path)
            .file_name()
            .and_then(|name| name.to_str())
            .unwrap_or("unknown")
            .to_string();
        
        println!("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {} (ID: {})", file_name, document_id);
        
        // 1. è¯»å–æ–‡ä»¶å†…å®¹
        self.emit_document_status(window, &document_id, &file_name, DocumentProcessingStage::Reading, 0.1).await;
        let content = self.read_file_content(file_path).await?;
        
        // 2. é¢„å¤„ç†
        self.emit_document_status(window, &document_id, &file_name, DocumentProcessingStage::Preprocessing, 0.2).await;
        let processed_content = self.preprocess_content(&content);
        
        // 3. æ–‡æœ¬åˆ†å—
        self.emit_document_status(window, &document_id, &file_name, DocumentProcessingStage::Chunking, 0.3).await;
        let chunks = self.chunk_text_with_progress(&document_id, &processed_content, &file_name, Some(window), &document_id, &file_name).await?;
        
        // 4. ç”Ÿæˆå‘é‡åµŒå…¥
        let chunks_with_embeddings = self.generate_embeddings_for_chunks_with_progress(chunks, Some(window), &document_id, &file_name).await?;
        
        // 5. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.emit_document_status(window, &document_id, &file_name, DocumentProcessingStage::Storing, 0.8).await;
        self.vector_store.add_chunks(chunks_with_embeddings.clone()).await?;
        
        // 6. æ›´æ–°æ–‡æ¡£è®°å½•  
        let target_library_id = sub_library_id.unwrap_or("default");
        self.vector_store.add_document_record_with_library(&document_id, &file_name, Some(file_path), None, target_library_id)?;
        self.vector_store.update_document_chunk_count(&document_id, chunks_with_embeddings.len())?;
        
        // 7. å®Œæˆ
        self.emit_document_status(window, &document_id, &file_name, DocumentProcessingStage::Completed, 1.0).await;
        
        Ok(chunks_with_embeddings.len())
    }
    
    /// ä»æ–‡ä»¶å†…å®¹æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    pub async fn add_documents_from_content(
        &self, 
        documents: Vec<serde_json::Value>, 
        window: Window
    ) -> Result<String> {
        println!("ğŸš€ å¼€å§‹ä»å†…å®¹å¤„ç† {} ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“", documents.len());
        
        let mut processed_documents = Vec::new();
        let mut total_chunks = 0;
        
        for (index, doc_data) in documents.iter().enumerate() {
            let progress = (index as f32) / (documents.len() as f32);
            
            // è§£ææ–‡æ¡£æ•°æ®
            let file_name = doc_data.get("fileName")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown")
                .to_string();
            
            let content = doc_data.get("content")
                .and_then(|v| v.as_str())
                .unwrap_or("");
            
            if content.is_empty() {
                println!("âš ï¸ è·³è¿‡ç©ºå†…å®¹æ–‡æ¡£: {}", file_name);
                continue;
            }
            
            // å‘é€å¤„ç†çŠ¶æ€æ›´æ–°
            self.emit_processing_status(&window, "overall", "processing", progress, 
                &format!("æ­£åœ¨å¤„ç†æ–‡æ¡£ {}/{}", index + 1, documents.len())).await;
            
            match self.process_document_content(&file_name, content, &window).await {
                Ok(chunk_count) => {
                    total_chunks += chunk_count;
                    processed_documents.push(file_name.clone());
                    println!("âœ… æ–‡æ¡£å†…å®¹å¤„ç†å®Œæˆ: {} ({} ä¸ªå—)", file_name, chunk_count);
                }
                Err(e) => {
                    println!("âŒ æ–‡æ¡£å†…å®¹å¤„ç†å¤±è´¥: {} - {}", file_name, e);
                    self.emit_processing_status(&window, "overall", "error", progress,
                        &format!("æ–‡æ¡£å¤„ç†å¤±è´¥: {}", e)).await;
                }
            }
        }
        
        // å‘é€å®ŒæˆçŠ¶æ€
        self.emit_processing_status(&window, "overall", "completed", 1.0,
            &format!("å¤„ç†å®Œæˆï¼š{} ä¸ªæ–‡æ¡£ï¼Œ{} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks)).await;
        
        Ok(format!("æˆåŠŸå¤„ç† {} ä¸ªæ–‡æ¡£ï¼Œå…± {} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks))
    }
    
    /// ä»æ–‡ä»¶å†…å®¹æ·»åŠ æ–‡æ¡£åˆ°æŒ‡å®šåˆ†åº“
    pub async fn add_documents_from_content_to_library(
        &self, 
        documents: Vec<serde_json::Value>, 
        window: Window,
        sub_library_id: Option<String>
    ) -> Result<String> {
        println!("ğŸš€ å¼€å§‹ä»å†…å®¹å¤„ç† {} ä¸ªæ–‡æ¡£åˆ°åˆ†åº“: {:?}", documents.len(), sub_library_id);
        
        let mut processed_documents = Vec::new();
        let mut total_chunks = 0;
        
        for (index, doc_data) in documents.iter().enumerate() {
            let progress = (index as f32) / (documents.len() as f32);
            
            // è§£ææ–‡æ¡£æ•°æ®
            let file_name = doc_data.get("fileName")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown")
                .to_string();
            
            let content = doc_data.get("content")
                .and_then(|v| v.as_str())
                .unwrap_or("");
            
            if content.is_empty() {
                println!("âš ï¸ è·³è¿‡ç©ºå†…å®¹æ–‡æ¡£: {}", file_name);
                continue;
            }
            
            // å‘é€å¤„ç†çŠ¶æ€æ›´æ–°
            self.emit_processing_status(&window, "overall", "processing", progress, 
                &format!("æ­£åœ¨å¤„ç†æ–‡æ¡£ {}/{}", index + 1, documents.len())).await;
            
            match self.process_document_content_with_library(&file_name, content, &window, sub_library_id.as_deref()).await {
                Ok(chunk_count) => {
                    total_chunks += chunk_count;
                    processed_documents.push(file_name.clone());
                    println!("âœ… æ–‡æ¡£å†…å®¹å¤„ç†å®Œæˆ: {} ({} ä¸ªå—)", file_name, chunk_count);
                }
                Err(e) => {
                    println!("âŒ æ–‡æ¡£å†…å®¹å¤„ç†å¤±è´¥: {} - {}", file_name, e);
                    self.emit_processing_status(&window, "overall", "error", progress,
                        &format!("æ–‡æ¡£å¤„ç†å¤±è´¥: {}", e)).await;
                }
            }
        }
        
        // å‘é€å®ŒæˆçŠ¶æ€
        self.emit_processing_status(&window, "overall", "completed", 1.0,
            &format!("å¤„ç†å®Œæˆï¼š{} ä¸ªæ–‡æ¡£ï¼Œ{} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks)).await;
        
        Ok(format!("æˆåŠŸå¤„ç† {} ä¸ªæ–‡æ¡£ï¼Œå…± {} ä¸ªæ–‡æœ¬å—", processed_documents.len(), total_chunks))
    }
    
    /// å¤„ç†æ–‡æ¡£å†…å®¹ï¼ˆä¸ä»æ–‡ä»¶è·¯å¾„è¯»å–ï¼Œç›´æ¥ä½¿ç”¨æä¾›çš„å†…å®¹ï¼‰
    async fn process_document_content(&self, file_name: &str, content: &str, window: &Window) -> Result<usize> {
        let document_id = Uuid::new_v4().to_string();
        
        println!("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£å†…å®¹: {} (ID: {})", file_name, document_id);
        
        // å‘é€æ–‡æ¡£å¼€å§‹å¤„ç†çŠ¶æ€
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Reading, 0.1).await;
        
        // æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦éœ€è¦è§£ç base64
        let processed_content = if file_name.ends_with(".txt") || file_name.ends_with(".md") {
            // æ–‡æœ¬æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å†…å®¹
            println!("ğŸ“ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {}", file_name);
            content.to_string()
        } else {
            // äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå‡è®¾æ˜¯base64ç¼–ç ï¼Œéœ€è¦è§£ç åå¤„ç†
            println!("ğŸ”„ å¼€å§‹è§£ç Base64å†…å®¹: {} å­—èŠ‚", content.len());
            match general_purpose::STANDARD.decode(content) {
                Ok(decoded_bytes) => {
                    println!("âœ… Base64è§£ç æˆåŠŸï¼Œè§£ç åå¤§å°: {} å­—èŠ‚", decoded_bytes.len());
                    // æ ¹æ®æ–‡ä»¶æ‰©å±•åå¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶
                    if file_name.ends_with(".pdf") {
                        println!("ğŸ“„ å¼€å§‹è§£æPDFæ–‡ä»¶: {}", file_name);
                        self.extract_pdf_text_from_memory(&decoded_bytes).await?
                    } else if file_name.ends_with(".docx") {
                        println!("ğŸ“„ å¼€å§‹è§£æDOCXæ–‡ä»¶: {}", file_name);
                        self.extract_docx_text_from_memory(&decoded_bytes).await?
                    } else {
                        return Err(AppError::validation(format!(
                            "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {}", 
                            file_name
                        )));
                    }
                }
                Err(_) => {
                    // ä¸æ˜¯æœ‰æ•ˆçš„base64ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†
                    println!("âš ï¸ Base64è§£ç å¤±è´¥ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†: {}", file_name);
                    content.to_string()
                }
            }
        };
        
        println!("ğŸ“Š æ–‡æ¡£è§£æå®Œæˆï¼Œæå–æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", processed_content.len());
        
        // é¢„å¤„ç†å†…å®¹
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Preprocessing, 0.2).await;
        println!("ğŸ”§ å¼€å§‹é¢„å¤„ç†æ–‡æ¡£å†…å®¹: {}", file_name);
        let preprocessed_content = self.preprocess_content(&processed_content);
        println!("âœ… é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åé•¿åº¦: {} å­—ç¬¦", preprocessed_content.len());
        
        // åˆ†å—
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Chunking, 0.3).await;
        println!("âœ‚ï¸ å¼€å§‹æ–‡æœ¬åˆ†å—: {}", file_name);
        let chunks = self.chunk_text_with_progress(&document_id, &preprocessed_content, file_name, Some(window), &document_id, file_name).await?;
        println!("âœ… åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {} ä¸ªæ–‡æœ¬å—", chunks.len());
        
        if chunks.is_empty() {
            return Err(AppError::validation("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•åˆ†å—"));
        }
        
        // ç”ŸæˆåµŒå…¥
        println!("ğŸ§  å¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥: {} ä¸ªæ–‡æœ¬å—", chunks.len());
        let chunks_with_embeddings = self.generate_embeddings_for_chunks_with_progress(chunks, Some(window), &document_id, file_name).await?;
        println!("âœ… å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆ: {} ä¸ªå‘é‡", chunks_with_embeddings.len());
        
        // è®°å½•æ–‡æ¡£ä¿¡æ¯
        let file_size = processed_content.len() as u64;
        self.vector_store.add_document_record(&document_id, file_name, None, Some(file_size))?;
        
        // å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Storing, 0.8).await;
        self.vector_store.add_chunks(chunks_with_embeddings.clone()).await?;
        
        // æ›´æ–°æ–‡æ¡£å—æ•°ç»Ÿè®¡
        self.vector_store.update_document_chunk_count(&document_id, chunks_with_embeddings.len())?;
        
        // å‘é€å®ŒæˆçŠ¶æ€
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Completed, 1.0).await;
        println!("ğŸ‰ æ–‡æ¡£å¤„ç†å®Œå…¨å®Œæˆ: {} ({} ä¸ªæ–‡æœ¬å—)", file_name, chunks_with_embeddings.len());
        
        Ok(chunks_with_embeddings.len())
    }
    
    /// å¤„ç†æ–‡æ¡£å†…å®¹åˆ°æŒ‡å®šåˆ†åº“ï¼ˆä¸ä»æ–‡ä»¶è·¯å¾„è¯»å–ï¼Œç›´æ¥ä½¿ç”¨æä¾›çš„å†…å®¹ï¼‰
    async fn process_document_content_with_library(&self, file_name: &str, content: &str, window: &Window, sub_library_id: Option<&str>) -> Result<usize> {
        let document_id = Uuid::new_v4().to_string();
        
        println!("ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£å†…å®¹åˆ°åˆ†åº“: {} (ID: {}, åˆ†åº“: {:?})", file_name, document_id, sub_library_id);
        println!("ğŸ“Š æ–‡æ¡£åŸå§‹å¤§å°: {} å­—èŠ‚", content.len());
        
        // å‘é€æ–‡æ¡£å¼€å§‹å¤„ç†çŠ¶æ€
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Reading, 0.1).await;
        
        // æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦éœ€è¦è§£ç base64
        println!("ğŸ” å¼€å§‹è§£ææ–‡æ¡£å†…å®¹: {}", file_name);
        let processed_content = if file_name.ends_with(".txt") || file_name.ends_with(".md") {
            // æ–‡æœ¬æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å†…å®¹
            println!("ğŸ“ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {}", file_name);
            content.to_string()
        } else {
            // äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå‡è®¾æ˜¯base64ç¼–ç ï¼Œéœ€è¦è§£ç åå¤„ç†
            println!("ğŸ”„ å¼€å§‹è§£ç Base64å†…å®¹: {} å­—èŠ‚", content.len());
            match general_purpose::STANDARD.decode(content) {
                Ok(decoded_bytes) => {
                    println!("âœ… Base64è§£ç æˆåŠŸï¼Œè§£ç åå¤§å°: {} å­—èŠ‚", decoded_bytes.len());
                    // æ ¹æ®æ–‡ä»¶æ‰©å±•åå¤„ç†äºŒè¿›åˆ¶æ–‡ä»¶
                    if file_name.ends_with(".pdf") {
                        println!("ğŸ“„ å¼€å§‹è§£æPDFæ–‡ä»¶: {}", file_name);
                        self.extract_pdf_text_from_memory(&decoded_bytes).await?
                    } else if file_name.ends_with(".docx") {
                        println!("ğŸ“„ å¼€å§‹è§£æDOCXæ–‡ä»¶: {}", file_name);
                        self.extract_docx_text_from_memory(&decoded_bytes).await?
                    } else {
                        return Err(AppError::validation(format!(
                            "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {}", 
                            file_name
                        )));
                    }
                }
                Err(_) => {
                    // ä¸æ˜¯æœ‰æ•ˆçš„base64ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†
                    println!("âš ï¸ Base64è§£ç å¤±è´¥ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬å¤„ç†: {}", file_name);
                    content.to_string()
                }
            }
        };
        
        println!("ğŸ“Š æ–‡æ¡£è§£æå®Œæˆï¼Œæå–æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", processed_content.len());
        
        // é¢„å¤„ç†å†…å®¹
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Preprocessing, 0.2).await;
        println!("ğŸ”§ å¼€å§‹é¢„å¤„ç†æ–‡æ¡£å†…å®¹: {}", file_name);
        let preprocessed_content = self.preprocess_content(&processed_content);
        println!("âœ… é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åé•¿åº¦: {} å­—ç¬¦", preprocessed_content.len());
        
        // åˆ†å—
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Chunking, 0.3).await;
        println!("âœ‚ï¸ å¼€å§‹æ–‡æœ¬åˆ†å—: {}", file_name);
        let chunks = self.chunk_text_with_progress(&document_id, &preprocessed_content, file_name, Some(window), &document_id, file_name).await?;
        println!("âœ… åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {} ä¸ªæ–‡æœ¬å—", chunks.len());
        
        if chunks.is_empty() {
            return Err(AppError::validation("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•åˆ†å—"));
        }
        
        // ç”ŸæˆåµŒå…¥
        println!("ğŸ§  å¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥: {} ä¸ªæ–‡æœ¬å—", chunks.len());
        let chunks_with_embeddings = self.generate_embeddings_for_chunks_with_progress(chunks, Some(window), &document_id, file_name).await?;
        println!("âœ… å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆ: {} ä¸ªå‘é‡", chunks_with_embeddings.len());
        
        // è®°å½•æ–‡æ¡£ä¿¡æ¯åˆ°æŒ‡å®šåˆ†åº“
        let file_size = processed_content.len() as u64;
        let target_library_id = sub_library_id.unwrap_or("default");
        println!("ğŸ“‹ æ·»åŠ æ–‡æ¡£è®°å½•åˆ°åˆ†åº“: {} -> {}", file_name, target_library_id);
        self.vector_store.add_document_record_with_library(&document_id, file_name, None, Some(file_size), target_library_id)?;
        
        // å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Storing, 0.8).await;
        println!("ğŸ’¾ å¼€å§‹å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“: {} ä¸ªå‘é‡", chunks_with_embeddings.len());
        self.vector_store.add_chunks(chunks_with_embeddings.clone()).await?;
        println!("âœ… å‘é‡å­˜å‚¨å®Œæˆ");
        
        // æ›´æ–°æ–‡æ¡£å—æ•°ç»Ÿè®¡
        println!("ğŸ“Š æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯");
        self.vector_store.update_document_chunk_count(&document_id, chunks_with_embeddings.len())?;
        
        // å‘é€å®ŒæˆçŠ¶æ€
        self.emit_document_status(window, &document_id, file_name, DocumentProcessingStage::Completed, 1.0).await;
        println!("ğŸ‰ æ–‡æ¡£å¤„ç†å®Œå…¨å®Œæˆ: {} ({} ä¸ªæ–‡æœ¬å—)", file_name, chunks_with_embeddings.len());
        
        Ok(chunks_with_embeddings.len())
    }
    
    /// è¯»å–æ–‡ä»¶å†…å®¹
    async fn read_file_content(&self, file_path: &str) -> Result<String> {
        let path = std::path::Path::new(file_path);
        
        // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if !path.exists() {
            return Err(AppError::file_system(format!("è¯»å–æ–‡æ¡£æ–‡ä»¶å¤±è´¥: ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„æ–‡ä»¶ã€‚æ–‡ä»¶è·¯å¾„: {}", file_path)));
        }
        
        // æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆè€Œéç›®å½•ï¼‰
        if !path.is_file() {
            return Err(AppError::file_system(format!("è¯»å–æ–‡æ¡£æ–‡ä»¶å¤±è´¥: æŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶ã€‚æ–‡ä»¶è·¯å¾„: {}", file_path)));
        }
        
        let extension = path.extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("")
            .to_lowercase();
        
        println!("ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {} (ç±»å‹: {})", file_path, extension);
        
        match extension.as_str() {
            "txt" | "md" | "markdown" => {
                std::fs::read_to_string(file_path)
                    .map_err(|e| AppError::file_system(format!("è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {} (æ–‡ä»¶è·¯å¾„: {})", e, file_path)))
            }
            "pdf" => {
                self.extract_pdf_text(file_path).await
            }
            "docx" => {
                self.extract_docx_text(file_path).await
            }
            _ => {
                Err(AppError::validation(format!("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {} (æ–‡ä»¶è·¯å¾„: {})", extension, file_path)))
            }
        }
    }
    
    /// é¢„å¤„ç†æ–‡æœ¬å†…å®¹
    fn preprocess_content(&self, content: &str) -> String {
        // åŸºç¡€æ–‡æœ¬æ¸…ç†
        content
            .lines()
            .map(|line| line.trim())
            .filter(|line| !line.is_empty())
            .collect::<Vec<_>>()
            .join("\n")
    }
    
    /// ä»æ•°æ®åº“åŠ è½½åˆ†å—é…ç½®
    fn load_chunking_config(&self) -> Result<ChunkingConfig> {
        match self.database.get_rag_configuration() {
            Ok(Some(config)) => {
                let strategy = match config.chunking_strategy.as_str() {
                    "semantic" => ChunkingStrategy::Semantic,
                    _ => ChunkingStrategy::FixedSize,
                };
                
                Ok(ChunkingConfig {
                    strategy,
                    chunk_size: config.chunk_size as usize,
                    chunk_overlap: config.chunk_overlap as usize,
                    min_chunk_size: config.min_chunk_size as usize,
                })
            }
            Ok(None) => {
                // æ²¡æœ‰é…ç½®æ—¶ä½¿ç”¨é»˜è®¤å€¼
                Ok(ChunkingConfig::default())
            }
            Err(e) => {
                println!("âš ï¸ æ— æ³•åŠ è½½RAGé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: {}", e);
                Ok(ChunkingConfig::default())
            }
        }
    }
    
    /// å°†æ–‡æœ¬åˆ†å— - æ”¯æŒä¸åŒç­–ç•¥
    fn chunk_text(&self, document_id: &str, content: &str, file_name: &str) -> Result<Vec<DocumentChunk>> {
        // ä»æ•°æ®åº“åŠ è½½åˆ†å—é…ç½®
        let config = self.load_chunking_config()?;
        
        match config.strategy {
            ChunkingStrategy::FixedSize => {
                self.chunk_text_fixed_size(document_id, content, file_name, &config)
            }
            ChunkingStrategy::Semantic => {
                self.chunk_text_semantic(document_id, content, file_name, &config)
            }
        }
    }
    
    /// å°†æ–‡æœ¬åˆ†å—å¹¶å‘é€è¿›åº¦æ›´æ–°
    async fn chunk_text_with_progress(&self, document_id: &str, content: &str, file_name: &str, window: Option<&Window>, doc_id: &str, doc_name: &str) -> Result<Vec<DocumentChunk>> {
        // ä»æ•°æ®åº“åŠ è½½åˆ†å—é…ç½®
        let config = self.load_chunking_config()?;
        
        match config.strategy {
            ChunkingStrategy::FixedSize => {
                self.chunk_text_fixed_size_with_progress(document_id, content, file_name, &config, window, doc_id, doc_name).await
            }
            ChunkingStrategy::Semantic => {
                self.chunk_text_semantic_with_progress(document_id, content, file_name, &config, window, doc_id, doc_name).await
            }
        }
    }
    
    /// å›ºå®šå¤§å°åˆ†å—ç­–ç•¥
    fn chunk_text_fixed_size(&self, document_id: &str, content: &str, file_name: &str, config: &ChunkingConfig) -> Result<Vec<DocumentChunk>> {
        let mut chunks = Vec::new();
        let chars: Vec<char> = content.chars().collect();
        let mut start = 0;
        let mut chunk_index = 0;
        
        while start < chars.len() {
            let end = std::cmp::min(start + config.chunk_size, chars.len());
            let chunk_text: String = chars[start..end].iter().collect();
            
            // è·³è¿‡è¿‡çŸ­çš„å—
            if chunk_text.trim().len() < config.min_chunk_size {
                start = end;
                continue;
            }
            
            let mut metadata = HashMap::new();
            metadata.insert("file_name".to_string(), file_name.to_string());
            metadata.insert("chunk_index".to_string(), chunk_index.to_string());
            metadata.insert("start_pos".to_string(), start.to_string());
            metadata.insert("end_pos".to_string(), end.to_string());
            metadata.insert("chunking_strategy".to_string(), "fixed_size".to_string());
            
            let chunk = DocumentChunk {
                id: Uuid::new_v4().to_string(),
                document_id: document_id.to_string(),
                chunk_index,
                text: chunk_text.trim().to_string(),
                metadata,
            };
            
            chunks.push(chunk);
            chunk_index += 1;
            
            // è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = if end == chars.len() { 
                end 
            } else { 
                std::cmp::max(start + 1, end - config.chunk_overlap) 
            };
        }
        
        println!("ğŸ“ å›ºå®šå¤§å°åˆ†å—å®Œæˆ: {} ä¸ªå—", chunks.len());
        Ok(chunks)
    }
    
    /// å›ºå®šå¤§å°åˆ†å—ç­–ç•¥ï¼ˆå¸¦è¿›åº¦æ›´æ–°ï¼‰
    async fn chunk_text_fixed_size_with_progress(&self, document_id: &str, content: &str, file_name: &str, config: &ChunkingConfig, window: Option<&Window>, doc_id: &str, doc_name: &str) -> Result<Vec<DocumentChunk>> {
        let mut chunks = Vec::new();
        let chars: Vec<char> = content.chars().collect();
        let mut start = 0;
        let mut chunk_index = 0;
        
        // ä¼°ç®—æ€»çš„åˆ†å—æ•°é‡ï¼ˆç”¨äºè¿›åº¦è®¡ç®—ï¼‰
        let estimated_total_chunks = (chars.len() / config.chunk_size) + 1;
        
        while start < chars.len() {
            let end = std::cmp::min(start + config.chunk_size, chars.len());
            let chunk_text: String = chars[start..end].iter().collect();
            
            // è·³è¿‡è¿‡çŸ­çš„å—
            if chunk_text.trim().len() < config.min_chunk_size {
                start = end;
                continue;
            }
            
            let mut metadata = HashMap::new();
            metadata.insert("file_name".to_string(), file_name.to_string());
            metadata.insert("chunk_index".to_string(), chunk_index.to_string());
            metadata.insert("start_pos".to_string(), start.to_string());
            metadata.insert("end_pos".to_string(), end.to_string());
            metadata.insert("chunking_strategy".to_string(), "fixed_size".to_string());
            
            let chunk = DocumentChunk {
                id: Uuid::new_v4().to_string(),
                document_id: document_id.to_string(),
                chunk_index,
                text: chunk_text.trim().to_string(),
                metadata,
            };
            
            chunks.push(chunk);
            chunk_index += 1;
            
            // å‘é€è¿›åº¦æ›´æ–°
            if let Some(w) = window {
                let progress = 0.3 + (chunk_index as f32 / estimated_total_chunks as f32) * 0.15; // ä»30%åˆ°45%
                self.emit_document_status_with_chunks(w, doc_id, doc_name, DocumentProcessingStage::Chunking, progress, chunk_index, estimated_total_chunks).await;
            }
            
            // è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = if end == chars.len() { 
                end 
            } else { 
                std::cmp::max(start + 1, end - config.chunk_overlap) 
            };
        }
        
        // å‘é€åˆ†å—å®ŒæˆçŠ¶æ€
        if let Some(w) = window {
            self.emit_document_status_with_chunks(w, doc_id, doc_name, DocumentProcessingStage::Chunking, 0.45, chunks.len(), chunks.len()).await;
        }
        
        println!("ğŸ“ å›ºå®šå¤§å°åˆ†å—å®Œæˆ: {} ä¸ªå—", chunks.len());
        Ok(chunks)
    }
    
    /// è¯­ä¹‰åˆ†å—ç­–ç•¥
    fn chunk_text_semantic(&self, document_id: &str, content: &str, file_name: &str, config: &ChunkingConfig) -> Result<Vec<DocumentChunk>> {
        let mut chunks = Vec::new();
        let mut chunk_index = 0;
        
        // 1. é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        let paragraphs = self.split_into_paragraphs(content);
        
        let mut current_chunk = String::new();
        let mut current_sentences = Vec::new();
        let mut paragraph_index = 0;
        
        for paragraph in paragraphs {
            if paragraph.trim().is_empty() {
                continue;
            }
            
            // 2. å°†æ®µè½æŒ‰å¥å­åˆ†å‰²
            let sentences = self.split_into_sentences(&paragraph);
            
            for sentence in sentences {
                let sentence = sentence.trim();
                if sentence.is_empty() {
                    continue;
                }
                
                // æ£€æŸ¥å½“å‰å—åŠ ä¸Šæ–°å¥å­æ˜¯å¦è¶…è¿‡ç›®æ ‡å¤§å°
                let potential_chunk = if current_chunk.is_empty() {
                    sentence.to_string()
                } else {
                    format!("{} {}", current_chunk, sentence)
                };
                
                if potential_chunk.len() <= config.chunk_size {
                    // å¯ä»¥æ·»åŠ åˆ°å½“å‰å—
                    current_chunk = potential_chunk;
                    current_sentences.push(sentence.to_string());
                } else {
                    // å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
                    if !current_chunk.is_empty() && current_chunk.len() >= config.min_chunk_size {
                        let chunk = self.create_semantic_chunk(
                            document_id,
                            &current_chunk,
                            file_name,
                            chunk_index,
                            paragraph_index,
                            &current_sentences,
                        );
                        chunks.push(chunk);
                        chunk_index += 1;
                        
                        // å®ç°é‡å ï¼šä¿ç•™æœ€å1-2ä¸ªå¥å­ä½œä¸ºæ–°å—çš„å¼€å§‹
                        let overlap_sentences = if current_sentences.len() > 2 {
                            current_sentences[current_sentences.len()-2..].to_vec()
                        } else if current_sentences.len() > 1 {
                            current_sentences[current_sentences.len()-1..].to_vec()
                        } else {
                            Vec::new()
                        };
                        
                        current_chunk = if overlap_sentences.is_empty() {
                            sentence.to_string()
                        } else {
                            format!("{} {}", overlap_sentences.join(" "), sentence)
                        };
                        current_sentences = overlap_sentences;
                        current_sentences.push(sentence.to_string());
                    } else {
                        // å¦‚æœå½“å‰å¥å­æœ¬èº«å°±å¾ˆé•¿ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
                        if sentence.len() >= config.min_chunk_size {
                            let chunk = self.create_semantic_chunk(
                                document_id,
                                sentence,
                                file_name,
                                chunk_index,
                                paragraph_index,
                                &vec![sentence.to_string()],
                            );
                            chunks.push(chunk);
                            chunk_index += 1;
                        }
                        current_chunk.clear();
                        current_sentences.clear();
                    }
                }
            }
            paragraph_index += 1;
        }
        
        // å¤„ç†æœ€åä¸€ä¸ªå—
        if !current_chunk.is_empty() && current_chunk.len() >= config.min_chunk_size {
            let chunk = self.create_semantic_chunk(
                document_id,
                &current_chunk,
                file_name,
                chunk_index,
                paragraph_index,
                &current_sentences,
            );
            chunks.push(chunk);
        }
        
        println!("ğŸ“ è¯­ä¹‰åˆ†å—å®Œæˆ: {} ä¸ªå—", chunks.len());
        Ok(chunks)
    }
    
    /// è¯­ä¹‰åˆ†å—ç­–ç•¥ï¼ˆå¸¦è¿›åº¦æ›´æ–°ï¼‰
    async fn chunk_text_semantic_with_progress(&self, document_id: &str, content: &str, file_name: &str, config: &ChunkingConfig, window: Option<&Window>, doc_id: &str, doc_name: &str) -> Result<Vec<DocumentChunk>> {
        let mut chunks = Vec::new();
        let mut chunk_index = 0;
        
        // 1. é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        let paragraphs = self.split_into_paragraphs(content);
        let total_paragraphs = paragraphs.len();
        
        let mut current_chunk = String::new();
        let mut current_sentences = Vec::new();
        let mut paragraph_index = 0;
        
        for paragraph in paragraphs {
            if paragraph.trim().is_empty() {
                paragraph_index += 1;
                continue;
            }
            
            // 2. å°†æ®µè½æŒ‰å¥å­åˆ†å‰²
            let sentences = self.split_into_sentences(&paragraph);
            
            for sentence in sentences {
                let sentence = sentence.trim();
                if sentence.is_empty() {
                    continue;
                }
                
                // æ£€æŸ¥å½“å‰å—åŠ ä¸Šæ–°å¥å­æ˜¯å¦è¶…è¿‡ç›®æ ‡å¤§å°
                let potential_chunk = if current_chunk.is_empty() {
                    sentence.to_string()
                } else {
                    format!("{} {}", current_chunk, sentence)
                };
                
                if potential_chunk.len() <= config.chunk_size {
                    // å¯ä»¥æ·»åŠ åˆ°å½“å‰å—
                    current_chunk = potential_chunk;
                    current_sentences.push(sentence.to_string());
                } else {
                    // å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
                    if !current_chunk.is_empty() && current_chunk.len() >= config.min_chunk_size {
                        let chunk = self.create_semantic_chunk(
                            document_id,
                            &current_chunk,
                            file_name,
                            chunk_index,
                            paragraph_index,
                            &current_sentences,
                        );
                        chunks.push(chunk);
                        chunk_index += 1;
                        
                        // å‘é€è¿›åº¦æ›´æ–°
                        if let Some(w) = window {
                            let progress = 0.3 + (paragraph_index as f32 / total_paragraphs as f32) * 0.15; // ä»30%åˆ°45%
                            let estimated_total_chunks = (content.len() / config.chunk_size) + 1;
                            self.emit_document_status_with_chunks(w, doc_id, doc_name, DocumentProcessingStage::Chunking, progress, chunk_index, estimated_total_chunks).await;
                        }
                        
                        // å®ç°é‡å ï¼šä¿ç•™æœ€å1-2ä¸ªå¥å­ä½œä¸ºæ–°å—çš„å¼€å§‹
                        let overlap_sentences = if current_sentences.len() > 2 {
                            current_sentences[current_sentences.len()-2..].to_vec()
                        } else if current_sentences.len() > 1 {
                            current_sentences[current_sentences.len()-1..].to_vec()
                        } else {
                            Vec::new()
                        };
                        
                        current_chunk = if overlap_sentences.is_empty() {
                            sentence.to_string()
                        } else {
                            format!("{} {}", overlap_sentences.join(" "), sentence)
                        };
                        current_sentences = overlap_sentences;
                        current_sentences.push(sentence.to_string());
                    } else {
                        // å¦‚æœå½“å‰å¥å­æœ¬èº«å°±å¾ˆé•¿ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
                        if sentence.len() >= config.min_chunk_size {
                            let chunk = self.create_semantic_chunk(
                                document_id,
                                sentence,
                                file_name,
                                chunk_index,
                                paragraph_index,
                                &vec![sentence.to_string()],
                            );
                            chunks.push(chunk);
                            chunk_index += 1;
                        }
                        current_chunk.clear();
                        current_sentences.clear();
                    }
                }
            }
            paragraph_index += 1;
        }
        
        // å¤„ç†æœ€åä¸€ä¸ªå—
        if !current_chunk.is_empty() && current_chunk.len() >= config.min_chunk_size {
            let chunk = self.create_semantic_chunk(
                document_id,
                &current_chunk,
                file_name,
                chunk_index,
                paragraph_index,
                &current_sentences,
            );
            chunks.push(chunk);
        }
        
        // å‘é€åˆ†å—å®ŒæˆçŠ¶æ€
        if let Some(w) = window {
            self.emit_document_status_with_chunks(w, doc_id, doc_name, DocumentProcessingStage::Chunking, 0.45, chunks.len(), chunks.len()).await;
        }
        
        println!("ğŸ“ è¯­ä¹‰åˆ†å—å®Œæˆ: {} ä¸ªå—", chunks.len());
        Ok(chunks)
    }
    
    /// æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
    fn split_into_paragraphs(&self, content: &str) -> Vec<String> {
        // ä½¿ç”¨åŒæ¢è¡Œç¬¦æˆ–å¤šä¸ªæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        let paragraph_regex = Regex::new(r"\n\s*\n").unwrap();
        paragraph_regex.split(content)
            .map(|p| p.trim().to_string())
            .filter(|p| !p.is_empty())
            .collect()
    }
    
    /// æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬
    fn split_into_sentences(&self, text: &str) -> Vec<String> {
        // ä½¿ç”¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²å¥å­
        let sentence_regex = Regex::new(r"[.!?ã€‚ï¼ï¼Ÿ]+").unwrap();
        let mut sentences = Vec::new();
        let mut last_end = 0;
        
        for mat in sentence_regex.find_iter(text) {
            let sentence = text[last_end..mat.end()].trim();
            if !sentence.is_empty() {
                sentences.push(sentence.to_string());
            }
            last_end = mat.end();
        }
        
        // å¤„ç†æœ€åä¸€ä¸ªå¥å­ï¼ˆå¦‚æœæ²¡æœ‰ä»¥æ ‡ç‚¹ç»“å°¾ï¼‰
        if last_end < text.len() {
            let sentence = text[last_end..].trim();
            if !sentence.is_empty() {
                sentences.push(sentence.to_string());
            }
        }
        
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥å­åˆ†éš”ç¬¦ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå¥å­
        if sentences.is_empty() {
            sentences.push(text.trim().to_string());
        }
        
        sentences
    }
    
    /// åˆ›å»ºè¯­ä¹‰å—
    fn create_semantic_chunk(
        &self,
        document_id: &str,
        content: &str,
        file_name: &str,
        chunk_index: usize,
        paragraph_index: usize,
        sentences: &[String],
    ) -> DocumentChunk {
        let mut metadata = HashMap::new();
        metadata.insert("file_name".to_string(), file_name.to_string());
        metadata.insert("chunk_index".to_string(), chunk_index.to_string());
        metadata.insert("paragraph_index".to_string(), paragraph_index.to_string());
        metadata.insert("sentence_count".to_string(), sentences.len().to_string());
        metadata.insert("chunking_strategy".to_string(), "semantic".to_string());
        
        DocumentChunk {
            id: Uuid::new_v4().to_string(),
            document_id: document_id.to_string(),
            chunk_index,
            text: content.to_string(),
            metadata,
        }
    }
    
    /// ä¸ºæ–‡æœ¬å—ç”Ÿæˆå‘é‡åµŒå…¥
    async fn generate_embeddings_for_chunks(&self, chunks: Vec<DocumentChunk>) -> Result<Vec<DocumentChunkWithEmbedding>> {
        self.generate_embeddings_for_chunks_with_progress(chunks, None, "", "").await
    }
    
    /// ä¸ºæ–‡æœ¬å—ç”Ÿæˆå‘é‡åµŒå…¥å¹¶å‘é€è¿›åº¦æ›´æ–°
    async fn generate_embeddings_for_chunks_with_progress(
        &self, 
        chunks: Vec<DocumentChunk>, 
        window: Option<&Window>, 
        document_id: &str, 
        file_name: &str
    ) -> Result<Vec<DocumentChunkWithEmbedding>> {
        println!("ğŸ§  å¼€å§‹ä¸º {} ä¸ªæ–‡æœ¬å—ç”Ÿæˆå‘é‡åµŒå…¥", chunks.len());
        let mut chunks_with_embeddings = Vec::new();
        
        let total_chunks = chunks.len();
        
        // å¦‚æœæä¾›äº†windowï¼Œå‘é€åˆå§‹çŠ¶æ€å¹¶æ˜¾ç¤ºæ€»å—æ•°
        if let Some(w) = window {
            self.emit_document_status_with_chunks(w, document_id, file_name, DocumentProcessingStage::Embedding, 0.0, 0, total_chunks).await;
        }
        
        for (index, chunk) in chunks.into_iter().enumerate() {
            if index % 10 == 0 || index == total_chunks - 1 {
                println!("ğŸ“Š å‘é‡ç”Ÿæˆè¿›åº¦: {}/{} ({:.1}%)", index + 1, total_chunks, (index + 1) as f32 / total_chunks as f32 * 100.0);
            }
            
            println!("ğŸ”¤ æ­£åœ¨ä¸ºæ–‡æœ¬å— {} ç”Ÿæˆå‘é‡ (é•¿åº¦: {} å­—ç¬¦)", index + 1, chunk.text.len());
            
            // è·å–åµŒå…¥æ¨¡å‹é…ç½®
            let model_assignments = self.llm_manager.get_model_assignments().await
                .map_err(|e| AppError::configuration(format!("è·å–æ¨¡å‹åˆ†é…å¤±è´¥: {}", e)))?;
            
            let embedding_model_id = model_assignments.embedding_model_config_id
                .ok_or_else(|| AppError::configuration("æœªé…ç½®åµŒå…¥æ¨¡å‹"))?;
            
            // è°ƒç”¨LLMç®¡ç†å™¨ç”ŸæˆåµŒå…¥
            let embeddings = self.llm_manager.call_embedding_api(vec![chunk.text.clone()], &embedding_model_id).await
                .map_err(|e| AppError::llm(format!("ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {}", e)))?;
            
            let embedding = embeddings.into_iter().next()
                .ok_or_else(|| AppError::llm("åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥"))?;
            
            if index % 10 == 0 {
                println!("âœ… æ–‡æœ¬å— {} å‘é‡ç”Ÿæˆå®Œæˆ (ç»´åº¦: {})", index + 1, embedding.len());
            }
            
            chunks_with_embeddings.push(DocumentChunkWithEmbedding {
                chunk,
                embedding,
            });
            
            // å‘é€è¿›åº¦æ›´æ–°
            if let Some(w) = window {
                let progress = 0.5 + (index + 1) as f32 / total_chunks as f32 * 0.3; // ä»50%åˆ°80%
                self.emit_document_status_with_chunks(w, document_id, file_name, DocumentProcessingStage::Embedding, progress, index + 1, total_chunks).await;
            }
        }
        
        println!("ğŸ‰ æ‰€æœ‰å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆ: {} ä¸ªå‘é‡", chunks_with_embeddings.len());
        Ok(chunks_with_embeddings)
    }
    
    /// è·å–é»˜è®¤RAGæŸ¥è¯¢é€‰é¡¹
    pub fn get_default_rag_query_options(&self) -> RagQueryOptions {
        match self.database.get_rag_configuration() {
            Ok(Some(config)) => {
                RagQueryOptions {
                    top_k: config.default_top_k as usize,
                    enable_reranking: Some(config.default_rerank_enabled),
                }
            }
            _ => {
                // ä½¿ç”¨é»˜è®¤å€¼
                RagQueryOptions {
                    top_k: 5,
                    enable_reranking: Some(false),
                }
            }
        }
    }
    
    /// æŸ¥è¯¢çŸ¥è¯†åº“
    pub async fn query_knowledge_base(&self, user_query: &str, options: RagQueryOptions) -> Result<RagQueryResponse> {
        let start_time = std::time::Instant::now();
        
        println!("ğŸ” å¼€å§‹RAGæŸ¥è¯¢: '{}' (top_k: {})", user_query, options.top_k);
        
        // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        let query_vector_start = std::time::Instant::now();
        
        let model_assignments = self.llm_manager.get_model_assignments().await
            .map_err(|e| AppError::configuration(format!("è·å–æ¨¡å‹åˆ†é…å¤±è´¥: {}", e)))?;
        
        let embedding_model_id = model_assignments.embedding_model_config_id
            .ok_or_else(|| AppError::configuration("æœªé…ç½®åµŒå…¥æ¨¡å‹"))?;
        
        let query_embeddings = self.llm_manager.call_embedding_api(vec![user_query.to_string()], &embedding_model_id).await
            .map_err(|e| AppError::llm(format!("ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {}", e)))?;
        
        let query_embedding = query_embeddings.into_iter().next()
            .ok_or_else(|| AppError::llm("æœªè·å–åˆ°æŸ¥è¯¢å‘é‡"))?;
        
        let query_vector_time = query_vector_start.elapsed();
        
        // 2. å‘é‡æœç´¢
        let search_start = std::time::Instant::now();
        let mut retrieved_chunks = self.vector_store.search_similar_chunks(query_embedding, options.top_k).await?;
        let search_time = search_start.elapsed();
        
        // 3. å¯é€‰çš„é‡æ’åº
        let reranking_time = if options.enable_reranking.unwrap_or(false) {
            let rerank_start = std::time::Instant::now();
            retrieved_chunks = self.rerank_chunks(user_query, retrieved_chunks).await?;
            Some(rerank_start.elapsed())
        } else {
            None
        };
        
        let total_time = start_time.elapsed();
        
        println!("âœ… RAGæŸ¥è¯¢å®Œæˆ: {} ä¸ªç»“æœ (æ€»è€—æ—¶: {:?})", retrieved_chunks.len(), total_time);
        
        Ok(RagQueryResponse {
            retrieved_chunks,
            query_vector_time_ms: query_vector_time.as_millis() as u64,
            search_time_ms: search_time.as_millis() as u64,
            reranking_time_ms: reranking_time.map(|t| t.as_millis() as u64),
            total_time_ms: total_time.as_millis() as u64,
        })
    }
    
    /// åœ¨æŒ‡å®šåˆ†åº“ä¸­æŸ¥è¯¢çŸ¥è¯†åº“
    pub async fn query_knowledge_base_in_libraries(&self, user_query: &str, options: RagQueryOptions, sub_library_ids: Option<Vec<String>>) -> Result<RagQueryResponse> {
        let start_time = std::time::Instant::now();
        
        let library_filter_msg = if let Some(ref lib_ids) = sub_library_ids {
            format!(" (åˆ†åº“: {:?})", lib_ids)
        } else {
            " (æ‰€æœ‰åˆ†åº“)".to_string()
        };
        
        println!("ğŸ” å¼€å§‹RAGæŸ¥è¯¢: '{}' (top_k: {}){}", user_query, options.top_k, library_filter_msg);
        
        // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        let query_vector_start = std::time::Instant::now();
        
        let model_assignments = self.llm_manager.get_model_assignments().await
            .map_err(|e| AppError::configuration(format!("è·å–æ¨¡å‹åˆ†é…å¤±è´¥: {}", e)))?;
        
        let embedding_model_id = model_assignments.embedding_model_config_id
            .ok_or_else(|| AppError::configuration("æœªé…ç½®åµŒå…¥æ¨¡å‹"))?;
        
        let query_embeddings = self.llm_manager.call_embedding_api(vec![user_query.to_string()], &embedding_model_id).await
            .map_err(|e| AppError::llm(format!("ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {}", e)))?;
        
        if query_embeddings.is_empty() {
            return Err(AppError::llm("æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥"));
        }
        
        let query_vector = query_embeddings.into_iter().next().unwrap();
        let query_vector_time = query_vector_start.elapsed();
        
        // 2. åœ¨æŒ‡å®šåˆ†åº“ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£å—
        let search_start = std::time::Instant::now();
        let mut retrieved_chunks = self.vector_store.search_similar_chunks_in_libraries(query_vector, options.top_k, sub_library_ids).await?;
        let search_time = search_start.elapsed();
        
        // 3. å¯é€‰çš„é‡æ’åº
        let reranking_time = if options.enable_reranking.unwrap_or(false) && !retrieved_chunks.is_empty() {
            let rerank_start = std::time::Instant::now();
            retrieved_chunks = self.rerank_chunks(user_query, retrieved_chunks).await?;
            Some(rerank_start.elapsed())
        } else {
            None
        };
        
        let total_time = start_time.elapsed();
        
        println!("âœ… RAGæŸ¥è¯¢å®Œæˆ: è¿”å› {} ä¸ªç»“æœ (æ€»è€—æ—¶: {}ms){}", 
                retrieved_chunks.len(), total_time.as_millis(), library_filter_msg);
        
        Ok(RagQueryResponse {
            retrieved_chunks,
            query_vector_time_ms: query_vector_time.as_millis() as u64,
            search_time_ms: search_time.as_millis() as u64,
            reranking_time_ms: reranking_time.map(|t| t.as_millis() as u64),
            total_time_ms: total_time.as_millis() as u64,
        })
    }
    
    /// é‡æ’åºæ£€ç´¢ç»“æœ
    async fn rerank_chunks(&self, query: &str, chunks: Vec<RetrievedChunk>) -> Result<Vec<RetrievedChunk>> {
        println!("ğŸ”„ å¼€å§‹é‡æ’åº {} ä¸ªæ£€ç´¢ç»“æœ", chunks.len());
        
        let model_assignments = self.llm_manager.get_model_assignments().await
            .map_err(|e| AppError::configuration(format!("è·å–æ¨¡å‹åˆ†é…å¤±è´¥: {}", e)))?;
        
        if let Some(reranker_model_id) = model_assignments.reranker_model_config_id {
            // è°ƒç”¨é‡æ’åºæ¨¡å‹
            let reranked_chunks = self.llm_manager.call_reranker_api(query.to_string(), chunks, &reranker_model_id).await
                .map_err(|e| AppError::llm(format!("é‡æ’åºå¤±è´¥: {}", e)))?;
            
            println!("âœ… é‡æ’åºå®Œæˆ");
            Ok(reranked_chunks)
        } else {
            println!("âš ï¸ æœªé…ç½®é‡æ’åºæ¨¡å‹ï¼Œè·³è¿‡é‡æ’åº");
            Ok(chunks)
        }
    }
    
    /// è·å–çŸ¥è¯†åº“çŠ¶æ€
    pub async fn get_knowledge_base_status(&self) -> Result<KnowledgeBaseStatusPayload> {
        let stats = self.vector_store.get_stats().await?;
        
        // è·å–å½“å‰åµŒå…¥æ¨¡å‹åç§°
        let embedding_model_name = match self.llm_manager.get_model_assignments().await {
            Ok(assignments) => {
                if let Some(model_id) = assignments.embedding_model_config_id {
                    match self.llm_manager.get_api_configs().await {
                        Ok(configs) => {
                            configs.iter()
                                .find(|config| config.id == model_id)
                                .map(|config| config.name.clone())
                        }
                        Err(_) => None
                    }
                } else {
                    None
                }
            }
            Err(_) => None
        };
        
        Ok(KnowledgeBaseStatusPayload {
            total_documents: stats.total_documents,
            total_chunks: stats.total_chunks,
            embedding_model_name,
            vector_store_type: "SQLite".to_string(),
        })
    }
    
    /// åˆ é™¤æ–‡æ¡£
    pub async fn delete_document_from_knowledge_base(&self, document_id: &str) -> Result<()> {
        println!("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {}", document_id);
        self.vector_store.delete_chunks_by_document_id(document_id).await
    }
    
    /// æ¸…ç©ºçŸ¥è¯†åº“
    pub async fn clear_knowledge_base(&self) -> Result<()> {
        println!("ğŸ§¹ æ¸…ç©ºçŸ¥è¯†åº“");
        self.vector_store.clear_all().await
    }
    
    /// è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨
    pub async fn get_all_documents(&self) -> Result<Vec<Value>> {
        self.vector_store.get_all_documents()
    }
    
    // è¾…åŠ©æ–¹æ³•ï¼šå‘é€å¤„ç†çŠ¶æ€æ›´æ–°
    async fn emit_processing_status(&self, window: &Window, id: &str, status: &str, progress: f32, message: &str) {
        let payload = serde_json::json!({
            "id": id,
            "status": status,
            "progress": progress,
            "message": message,
            "timestamp": chrono::Utc::now().to_rfc3339()
        });
        
        if let Err(e) = window.emit("rag_processing_status", payload) {
            println!("âš ï¸ å‘é€å¤„ç†çŠ¶æ€å¤±è´¥: {}", e);
        }
    }
    
    // è¾…åŠ©æ–¹æ³•ï¼šå‘é€æ–‡æ¡£å¤„ç†çŠ¶æ€
    async fn emit_document_status(&self, window: &Window, document_id: &str, file_name: &str, stage: DocumentProcessingStage, progress: f32) {
        let status = DocumentProcessingStatus {
            document_id: document_id.to_string(),
            file_name: file_name.to_string(),
            status: stage,
            progress,
            error_message: None,
            chunks_processed: 0,
            total_chunks: 0,
        };
        
        if let Err(e) = window.emit("rag_document_status", &status) {
            println!("âš ï¸ å‘é€æ–‡æ¡£çŠ¶æ€å¤±è´¥: {}", e);
        }
    }
    
    // è¾…åŠ©æ–¹æ³•ï¼šå‘é€å¸¦è¯¦ç»†ä¿¡æ¯çš„æ–‡æ¡£å¤„ç†çŠ¶æ€
    async fn emit_document_status_with_chunks(&self, window: &Window, document_id: &str, file_name: &str, stage: DocumentProcessingStage, progress: f32, chunks_processed: usize, total_chunks: usize) {
        let status = DocumentProcessingStatus {
            document_id: document_id.to_string(),
            file_name: file_name.to_string(),
            status: stage,
            progress,
            error_message: None,
            chunks_processed,
            total_chunks,
        };
        
        if let Err(e) = window.emit("rag_document_status", &status) {
            println!("âš ï¸ å‘é€æ–‡æ¡£çŠ¶æ€å¤±è´¥: {}", e);
        }
    }
    
    /// æå–PDFæ–‡ä»¶æ–‡æœ¬å†…å®¹
    async fn extract_pdf_text(&self, file_path: &str) -> Result<String> {
        use pdf_extract::extract_text;
        
        println!("ğŸ“• å¼€å§‹æå–PDFæ–‡æœ¬: {}", file_path);
        
        // å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        let path = std::path::Path::new(file_path);
        if !path.exists() {
            return Err(AppError::file_system(format!("PDFæ–‡ä»¶ä¸å­˜åœ¨: {}", file_path)));
        }
        
        let text = extract_text(file_path)
            .map_err(|e| AppError::validation(format!("PDFæ–‡æœ¬æå–å¤±è´¥: {} (æ–‡ä»¶è·¯å¾„: {})", e, file_path)))?;
        
        if text.trim().is_empty() {
            return Err(AppError::validation(format!("PDFæ–‡ä»¶æ²¡æœ‰å¯æå–çš„æ–‡æœ¬å†…å®¹ (æ–‡ä»¶è·¯å¾„: {})", file_path)));
        }
        
        println!("âœ… PDFæ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {} å­—ç¬¦", text.len());
        Ok(text)
    }
    
    /// æå–DOCXæ–‡ä»¶æ–‡æœ¬å†…å®¹
    async fn extract_docx_text(&self, file_path: &str) -> Result<String> {
        use docx_rs::*;
        
        println!("ğŸ“˜ å¼€å§‹æå–DOCXæ–‡æœ¬: {}", file_path);
        
        // å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        let path = std::path::Path::new(file_path);
        if !path.exists() {
            return Err(AppError::file_system(format!("DOCXæ–‡ä»¶ä¸å­˜åœ¨: {}", file_path)));
        }
        
        let bytes = std::fs::read(file_path)
            .map_err(|e| AppError::file_system(format!("è¯»å–DOCXæ–‡ä»¶å¤±è´¥: {} (æ–‡ä»¶è·¯å¾„: {})", e, file_path)))?;
        
        let docx = read_docx(&bytes)
            .map_err(|e| AppError::validation(format!("DOCXæ–‡ä»¶è§£æå¤±è´¥: {} (æ–‡ä»¶è·¯å¾„: {})", e, file_path)))?;
        
        // æå–æ–‡æ¡£ä¸­çš„æ‰€æœ‰æ–‡æœ¬
        let mut text_content = String::new();
        
        // éå†æ–‡æ¡£çš„æ‰€æœ‰æ®µè½
        for child in docx.document.children {
            match child {
                docx_rs::DocumentChild::Paragraph(paragraph) => {
                    for run in paragraph.children {
                        match run {
                            docx_rs::ParagraphChild::Run(run) => {
                                for run_child in run.children {
                                    if let docx_rs::RunChild::Text(text) = run_child {
                                        text_content.push_str(&text.text);
                                    }
                                }
                            }
                            _ => {}
                        }
                    }
                    text_content.push('\n'); // æ®µè½ç»“æŸæ·»åŠ æ¢è¡Œ
                }
                _ => {}
            }
        }
        
        if text_content.trim().is_empty() {
            return Err(AppError::validation(format!("DOCXæ–‡ä»¶æ²¡æœ‰å¯æå–çš„æ–‡æœ¬å†…å®¹ (æ–‡ä»¶è·¯å¾„: {})", file_path)));
        }
        
        println!("âœ… DOCXæ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {} å­—ç¬¦", text_content.len());
        Ok(text_content)
    }
    
    /// ä»å†…å­˜ä¸­çš„PDFå­—èŠ‚æ•°æ®æå–æ–‡æœ¬
    async fn extract_pdf_text_from_memory(&self, pdf_bytes: &[u8]) -> Result<String> {
        println!("ğŸ“„ å¼€å§‹è§£æPDFæ–‡ä»¶ (å¤§å°: {} å­—èŠ‚)", pdf_bytes.len());
        
        // ä½¿ç”¨æ–‡æ¡£è§£æå™¨å¤„ç†PDF
        let parser = crate::document_parser::DocumentParser::new();
        println!("ğŸ”§ åˆå§‹åŒ–PDFè§£æå™¨");
        
        match parser.extract_text_from_bytes("document.pdf", pdf_bytes.to_vec()) {
            Ok(text) => {
                println!("âœ… PDFè§£ææˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", text.len());
                if text.trim().is_empty() {
                    println!("âš ï¸ PDFæ–‡ä»¶è§£æç»“æœä¸ºç©º");
                    Err(AppError::validation("PDFæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ"))
                } else {
                    Ok(text)
                }
            }
            Err(e) => {
                println!("âŒ PDFè§£æå¤±è´¥: {}", e);
                Err(AppError::file_system(format!("PDFè§£æå¤±è´¥: {}", e)))
            }
        }
    }
    
    /// ä»å†…å­˜ä¸­çš„DOCXå­—èŠ‚æ•°æ®æå–æ–‡æœ¬
    async fn extract_docx_text_from_memory(&self, docx_bytes: &[u8]) -> Result<String> {
        println!("ğŸ“„ å¼€å§‹è§£æDOCXæ–‡ä»¶ (å¤§å°: {} å­—èŠ‚)", docx_bytes.len());
        
        // ä½¿ç”¨æ–‡æ¡£è§£æå™¨å¤„ç†DOCX
        let parser = crate::document_parser::DocumentParser::new();
        println!("ğŸ”§ åˆå§‹åŒ–DOCXè§£æå™¨");
        
        match parser.extract_text_from_bytes("document.docx", docx_bytes.to_vec()) {
            Ok(text) => {
                println!("âœ… DOCXè§£ææˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {} å­—ç¬¦", text.len());
                if text.trim().is_empty() {
                    println!("âš ï¸ DOCXæ–‡ä»¶è§£æç»“æœä¸ºç©º");
                    Err(AppError::validation("DOCXæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ"))
                } else {
                    Ok(text)
                }
            }
            Err(e) => {
                println!("âŒ DOCXè§£æå¤±è´¥: {}", e);
                Err(AppError::file_system(format!("DOCXè§£æå¤±è´¥: {}", e)))
            }
        }
    }
}

