use crate::models::{ChatMessage, MistakeItem};
use crate::llm_manager::LLMManager;
use crate::database::Database;
use anyhow::Result;
use serde_json::json;
use std::collections::HashMap;
use std::sync::Arc;
use tauri::Window;

// åˆ†æç»“æœç»“æ„
#[derive(Debug, Clone)]
pub struct AnalysisResult {
    pub ocr_text: String,
    pub tags: Vec<String>,
    pub mistake_type: String,
    pub first_answer: String,
}

pub struct AnalysisService {
    llm_manager: LLMManager,
    // ç§»é™¤é‡å¤çš„temp_sessionsï¼Œç»Ÿä¸€ä½¿ç”¨AppStateä¸­çš„ä¼šè¯ç®¡ç†
}

impl AnalysisService {
    pub fn new(database: Arc<Database>, file_manager: Arc<crate::file_manager::FileManager>) -> Self {
        Self {
            llm_manager: LLMManager::new(database, file_manager),
        }
    }

    // åˆ†æé”™é¢˜ï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- æµå¼ç‰ˆæœ¬
    pub async fn analyze_mistake_stream(
        &self,
        question_image_paths: &[String],
        user_question: &str,
        subject: &str,
        window: Window,
        stream_event: &str,
    ) -> Result<AnalysisResult> {
        println!("å¼€å§‹åˆ†æé”™é¢˜(æµå¼): ç§‘ç›®={}, å›¾ç‰‡æ•°é‡={}", subject, question_image_paths.len());
        
        // è°ƒç”¨ç»Ÿä¸€æ¨¡å‹ä¸€æ¥å£è¿›è¡ŒOCRå’Œåˆ†ç±»ï¼ˆç¬¬ä¸€æ¨¡å‹ä¸ä½¿ç”¨æµå¼ï¼Œå› ä¸ºéœ€è¦ç»“æ„åŒ–è¾“å‡ºï¼‰
        let model1_result = self.llm_manager.call_unified_model_1(
            question_image_paths.to_vec(),
            user_question,
            subject,
            None, // æš‚æ—¶ä¸ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡
        ).await.map_err(|e| anyhow::anyhow!("æ¨¡å‹ä¸€è°ƒç”¨å¤±è´¥: {}", e))?;

        // æ„å»ºä¸Šä¸‹æ–‡
        let mut context = HashMap::new();
        context.insert("ocr_text".to_string(), json!(model1_result.ocr_text));
        context.insert("tags".to_string(), json!(model1_result.tags));
        context.insert("mistake_type".to_string(), json!(model1_result.mistake_type));
        context.insert("user_question".to_string(), json!(user_question));

        // è·å–æ¨¡å‹é…ç½®ä»¥åˆ¤æ–­æ˜¯å¦æ˜¯æ¨ç†æ¨¡å‹
        let model_config = self.llm_manager.get_model2_config().await
            .map_err(|e| anyhow::anyhow!("è·å–æ¨¡å‹é…ç½®å¤±è´¥: {}", e))?;
        
        // æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€ç»´é“¾
        let enable_chain_of_thought = model_config.is_reasoning;

        // è°ƒç”¨ç»Ÿä¸€æ¨¡å‹äºŒæ¥å£è·å–é¦–æ¬¡è§£ç­”ï¼ˆæµå¼ï¼‰
        let model2_result = self.llm_manager.call_unified_model_2_stream(
            &context,
            &[], // ç©ºçš„èŠå¤©å†å²
            subject,
            enable_chain_of_thought, // æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€ç»´é“¾
            Some(question_image_paths.to_vec()), // ğŸ¯ ä¿®å¤ï¼šä¼ å…¥å›¾ç‰‡è·¯å¾„ç»™ç¬¬äºŒæ¨¡å‹
            None, // æš‚æ—¶ä¸ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡
            window,
            stream_event,
        ).await.map_err(|e| anyhow::anyhow!("æ¨¡å‹äºŒè°ƒç”¨å¤±è´¥: {}", e))?;

        Ok(AnalysisResult {
            ocr_text: model1_result.ocr_text,
            tags: model1_result.tags,
            mistake_type: model1_result.mistake_type,
            first_answer: model2_result.assistant_message,
        })
    }

    // åˆ†æé”™é¢˜ï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- éæµå¼ç‰ˆæœ¬ï¼ˆå·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼ï¼‰
    pub async fn analyze_mistake(
        &self,
        _question_image_paths: &[String],
        _user_question: &str,
        _subject: &str,
    ) -> Result<AnalysisResult> {
        println!("è­¦å‘Š: analyze_mistake éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ analyze_mistake_stream");
        
        // ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ Window å¯¹è±¡
        // å®é™…ä¸Šè¿™ä¸ªå‡½æ•°ä¸åº”è¯¥è¢«è°ƒç”¨
        return Err(anyhow::anyhow!("éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æµå¼ç‰ˆæœ¬"));
    }

    // ç»§ç»­å¯¹è¯ï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- æµå¼ç‰ˆæœ¬
    pub async fn continue_conversation_stream(
        &self,
        ocr_text: &str,
        tags: &[String],
        chat_history: &[ChatMessage],
        subject: &str,
        window: Window,
        stream_event: &str,
    ) -> Result<String> {
        println!("ç»§ç»­å¯¹è¯(æµå¼): ç§‘ç›®={}, èŠå¤©å†å²é•¿åº¦={}", subject, chat_history.len());
        
        // æ„å»ºä¸Šä¸‹æ–‡
        let mut context = HashMap::new();
        context.insert("ocr_text".to_string(), json!(ocr_text));
        context.insert("tags".to_string(), json!(tags));
        context.insert("subject".to_string(), json!(subject));

        // è·å–æ¨¡å‹é…ç½®ä»¥åˆ¤æ–­æ˜¯å¦æ˜¯æ¨ç†æ¨¡å‹
        let model_config = self.llm_manager.get_model2_config().await
            .map_err(|e| anyhow::anyhow!("è·å–æ¨¡å‹é…ç½®å¤±è´¥: {}", e))?;
        
        // æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€ç»´é“¾
        let enable_chain_of_thought = model_config.is_reasoning;

        // è°ƒç”¨ç»Ÿä¸€æ¨¡å‹äºŒæ¥å£ï¼ˆæµå¼ï¼‰
        let model2_result = self.llm_manager.call_unified_model_2_stream(
            &context,
            chat_history,
            subject,
            enable_chain_of_thought, // æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€ç»´é“¾
            None, // ç»§ç»­å¯¹è¯æ—¶ä¸ä¼ å…¥å›¾ç‰‡
            None, // æš‚æ—¶ä¸ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡
            window,
            stream_event,
        ).await.map_err(|e| anyhow::anyhow!("æ¨¡å‹äºŒè°ƒç”¨å¤±è´¥: {}", e))?;

        Ok(model2_result.assistant_message)
    }

    // ç»§ç»­å¯¹è¯ï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- éæµå¼ç‰ˆæœ¬ï¼ˆå·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼ï¼‰
    pub async fn continue_conversation(
        &self,
        _ocr_text: &str,
        _tags: &[String],
        _chat_history: &[ChatMessage],
        _subject: &str,
    ) -> Result<String> {
        println!("è­¦å‘Š: continue_conversation éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ continue_conversation_stream");
        
        // ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å›é”™è¯¯
        return Err(anyhow::anyhow!("éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æµå¼ç‰ˆæœ¬"));
    }

    // å›é¡¾åˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- æµå¼ç‰ˆæœ¬
    pub async fn analyze_review_session_stream(
        &self,
        mistakes: &[MistakeItem],
        subject: &str,
        window: Window,
        stream_event: &str,
    ) -> Result<String> {
        println!("å¼€å§‹å›é¡¾åˆ†æ(æµå¼): ç§‘ç›®={}, é”™é¢˜æ•°é‡={}", subject, mistakes.len());
        
        // æ„å»ºå›é¡¾åˆ†æçš„ä¸Šä¸‹æ–‡
        let mut context = HashMap::new();
        context.insert("subject".to_string(), json!(subject));
        context.insert("mistake_count".to_string(), json!(mistakes.len()));

        // æ”¶é›†æ‰€æœ‰é”™é¢˜çš„ä¿¡æ¯
        let mut mistake_summaries = Vec::new();
        for (index, mistake) in mistakes.iter().enumerate() {
            let summary = json!({
                "index": index + 1,
                "question": mistake.user_question,
                "ocr_text": mistake.ocr_text,
                "tags": mistake.tags,
                "mistake_type": mistake.mistake_type,
                "created_at": mistake.created_at.format("%Y-%m-%d").to_string()
            });
            mistake_summaries.push(summary);
        }
        context.insert("mistakes".to_string(), json!(mistake_summaries));

        // ğŸ¯ ä¿®å¤BUG-04ï¼šè·å–å›é¡¾åˆ†æä¸“ç”¨æ¨¡å‹é…ç½®
        let model_assignments = self.llm_manager.get_model_assignments().await
            .map_err(|e| anyhow::anyhow!("è·å–æ¨¡å‹åˆ†é…å¤±è´¥: {}", e))?;

        // ä¼˜å…ˆä½¿ç”¨å›é¡¾åˆ†æä¸“ç”¨æ¨¡å‹ï¼Œå¦‚æœæœªé…ç½®åˆ™å›é€€åˆ°ç¬¬äºŒæ¨¡å‹
        let target_model_id = model_assignments.review_analysis_model_config_id
            .or(model_assignments.model2_config_id)
            .ok_or_else(|| anyhow::anyhow!("æ²¡æœ‰é…ç½®å¯ç”¨çš„å›é¡¾åˆ†ææ¨¡å‹æˆ–ç¬¬äºŒæ¨¡å‹"))?;

        // è·å–ç›®æ ‡æ¨¡å‹é…ç½®
        let api_configs = self.llm_manager.get_api_configs().await
            .map_err(|e| anyhow::anyhow!("è·å–APIé…ç½®å¤±è´¥: {}", e))?;

        let model_config = api_configs.iter()
            .find(|config| config.id == target_model_id && config.enabled)
            .ok_or_else(|| anyhow::anyhow!("æ‰¾ä¸åˆ°å¯ç”¨çš„å›é¡¾åˆ†ææ¨¡å‹é…ç½®: {}", target_model_id))?;

        println!("ğŸ“‹ å›é¡¾åˆ†æä½¿ç”¨æ¨¡å‹: {} ({})", model_config.name, model_config.model);

        // æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€ç»´é“¾ï¼Œå›é¡¾åˆ†æç‰¹åˆ«éœ€è¦æ·±åº¦æ€è€ƒ
        let enable_chain_of_thought = model_config.is_reasoning || true; // å›é¡¾åˆ†ææ€»æ˜¯å¯ç”¨æ€ç»´é“¾

        // è°ƒç”¨ç»Ÿä¸€æ¨¡å‹æ¥å£è¿›è¡Œå›é¡¾åˆ†æï¼ˆæµå¼ï¼‰
        // ä½¿ç”¨é»˜è®¤çš„å›é¡¾åˆ†æä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆç§‘ç›®é…ç½®çš„æç¤ºè¯å·²ç»åœ¨ LLMManager ä¸­å¤„ç†ï¼‰
        let task_context = "å¤šé“é”™é¢˜çš„å›é¡¾åˆ†æå’Œå­¦ä¹ å»ºè®®";

        let model2_result = self.llm_manager.call_unified_model_stream_with_config(
            model_config,
            &context,
            &[], // å›é¡¾åˆ†æä¸éœ€è¦èŠå¤©å†å²
            subject,
            enable_chain_of_thought, // å›é¡¾åˆ†æå¯ç”¨æ€ç»´é“¾
            None, // å›é¡¾åˆ†æä¸ä¼ å…¥å›¾ç‰‡
            Some(task_context), // ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡
            window,
            stream_event,
        ).await.map_err(|e| anyhow::anyhow!("å›é¡¾åˆ†æå¤±è´¥: {}", e))?;

        Ok(model2_result.assistant_message)
    }

    // å›é¡¾åˆ†æï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰- éæµå¼ç‰ˆæœ¬ï¼ˆå·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼ï¼‰
    pub async fn analyze_review_session(
        &self,
        _mistakes: &[MistakeItem],
        _subject: &str,
    ) -> Result<String> {
        println!("è­¦å‘Š: analyze_review_session éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ analyze_review_session_stream");
        
        // ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å›é”™è¯¯
        return Err(anyhow::anyhow!("éæµå¼ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æµå¼ç‰ˆæœ¬"));
    }

    // æµ‹è¯•APIè¿æ¥
    pub async fn test_connection(&self, api_key: &str, api_base: &str) -> Result<bool> {
        // ä½¿ç”¨ç°æœ‰çš„LLMç®¡ç†å™¨è¿›è¡Œæµ‹è¯•
        self.llm_manager.test_connection(api_key, api_base).await
            .map_err(|e| anyhow::anyhow!("APIè¿æ¥æµ‹è¯•å¤±è´¥: {}", e))
    }
    
    // è·å–åˆå§‹è§£ç­”ï¼ˆä½¿ç”¨ç»Ÿä¸€AIæ¥å£ï¼‰
    pub async fn get_initial_answer(
        &self,
        ocr_text: &str,
        tags: &[String],
        user_question: &str,
        subject: &str,
    ) -> Result<String> {
        println!("è·å–åˆå§‹è§£ç­”: ç§‘ç›®={}", subject);
        
        // æ„å»ºä¸Šä¸‹æ–‡
        let mut context = HashMap::new();
        context.insert("ocr_text".to_string(), json!(ocr_text));
        context.insert("tags".to_string(), json!(tags));
        context.insert("user_question".to_string(), json!(user_question));

        // è°ƒç”¨ç»Ÿä¸€æ¨¡å‹äºŒæ¥å£è·å–é¦–æ¬¡è§£ç­”
        let model2_result = self.llm_manager.call_unified_model_2(
            &context,
            &[], // ç©ºçš„èŠå¤©å†å²
            subject,
            false, // åˆå§‹è§£ç­”é»˜è®¤ä¸å¯ç”¨æ€ç»´é“¾
            None, // ä¸ä¼ å…¥å›¾ç‰‡
            Some("æä¾›é¢˜ç›®çš„åˆå§‹è§£ç­”"), // ä»»åŠ¡ä¸Šä¸‹æ–‡
        ).await.map_err(|e| anyhow::anyhow!("è·å–åˆå§‹è§£ç­”å¤±è´¥: {}", e))?;

        Ok(model2_result.assistant_message)
    }
}
