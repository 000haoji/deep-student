// gemini_adapter.rs
// çº¯åº“æ¨¡å—ï¼šGoogle/Gemini APIé€‚é…å™¨ï¼Œæä¾›è¯·æ±‚æ„å»ºä¸æµå¼è§£æèƒ½åŠ›

use base64::{engine::general_purpose, Engine as _};
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use thiserror::Error;
use uuid::Uuid;

use crate::utils::fetch::fetch_binary_with_cache;

// ==================== å…¬å…±é”™è¯¯ç±»å‹ ====================

#[derive(Debug, Error)]
pub enum AdapterError {
    #[error("Invalid format: {0}")]
    InvalidFormat(String),

    #[error("Conversion failed: {0}")]
    ConversionFailed(String),

    #[error("Serialization error: {0}")]
    SerializationError(String),
}

// ==================== å…¬å…±è¿”å›ç±»å‹ ====================

#[derive(Debug, Clone)]
pub struct ProviderRequest {
    pub url: String,
    pub headers: Vec<(String, String)>,
    pub body: Value,
}

#[derive(Debug, Clone)]
pub enum StreamEvent {
    ContentChunk(String),
    ReasoningChunk(String),
    /// Gemini 3 æ€ç»´ç­¾åï¼ˆå·¥å…·è°ƒç”¨å¿…éœ€ï¼‰
    /// åœ¨å·¥å…·è°ƒç”¨åœºæ™¯ä¸‹ï¼Œéœ€è¦ç¼“å­˜æ­¤ç­¾åå¹¶åœ¨åç»­è¯·æ±‚ä¸­å›ä¼ 
    ThoughtSignature(String),
    ToolCall(Value),
    Usage(Value),
    SafetyBlocked(Value),
    Done,
}

// ==================== OpenAI æ•°æ®ç»“æ„ï¼ˆæœ€å°å­é›†ï¼‰ ====================

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIRequest {
    pub model: String,
    pub messages: Vec<OpenAIMessage>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<OpenAIStop>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<OpenAITool>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream_options: Option<Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIMessage {
    pub role: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<OpenAIMessageContent>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<OpenAIToolCall>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_call_id: Option<String>,
    /// ğŸ”§ Gemini 3 æ€ç»´ç­¾åï¼šå·¥å…·è°ƒç”¨åœºæ™¯ä¸‹å¿…é¡»åœ¨åç»­è¯·æ±‚ä¸­å›ä¼ 
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thought_signature: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum OpenAIMessageContent {
    Text(String),
    Array(Vec<OpenAIContentPart>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum OpenAIContentPart {
    #[serde(rename = "text")]
    Text { text: String },
    #[serde(rename = "image_url")]
    ImageUrl { image_url: OpenAIImageUrl },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIImageUrl {
    pub url: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub detail: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAITool {
    #[serde(rename = "type")]
    pub tool_type: String,
    pub function: OpenAIFunction,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIFunction {
    pub name: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    pub parameters: Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIToolCall {
    pub id: String,
    #[serde(rename = "type")]
    pub tool_type: String,
    pub function: OpenAIFunctionCall,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIFunctionCall {
    pub name: String,
    pub arguments: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum OpenAIStop {
    Single(String),
    Multiple(Vec<String>),
}

impl OpenAIStop {
    fn to_vec(&self) -> Vec<String> {
        match self {
            OpenAIStop::Single(value) => vec![value.clone()],
            OpenAIStop::Multiple(values) => values.clone(),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIResponse {
    pub id: String,
    pub object: String,
    pub created: i64,
    pub model: String,
    pub choices: Vec<OpenAIChoice>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage: Option<OpenAIUsage>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIChoice {
    pub index: i32,
    pub message: OpenAIMessage,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIUsage {
    pub prompt_tokens: i32,
    pub completion_tokens: i32,
    pub total_tokens: i32,
}

// ==================== Gemini æ•°æ®ç»“æ„ï¼ˆæœ€å°å­é›†ï¼‰ ====================

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiRequest {
    pub contents: Vec<GeminiContent>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_instruction: Option<GeminiContent>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub generation_config: Option<GeminiGenerationConfig>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<GeminiTool>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_config: Option<GeminiToolConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiContent {
    pub role: String,
    pub parts: Vec<GeminiPart>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiPart {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub inline_data: Option<GeminiInlineData>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_call: Option<GeminiFunctionCall>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_response: Option<GeminiFunctionResponse>,
    /// ğŸ”§ Gemini 3 æ€ç»´ç­¾åï¼šå·¥å…·è°ƒç”¨åœºæ™¯ä¸‹å¿…é¡»åœ¨åç»­è¯·æ±‚ä¸­å›ä¼ 
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thought_signature: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiInlineData {
    pub mime_type: String,
    pub data: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiFunctionCall {
    pub name: String,
    pub args: Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiFunctionResponse {
    pub name: String,
    pub response: Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiGenerationConfig {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_k: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_mime_type: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_schema: Option<Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking_config: Option<GeminiThinkingConfig>,
}

/// Gemini æ€ç»´é“¾é…ç½®
///
/// ## Gemini 2.5 vs Gemini 3 é…ç½®å·®å¼‚ï¼ˆREST API ä½¿ç”¨ camelCaseï¼‰
/// - **Gemini 2.5**: ä½¿ç”¨ `thinkingBudget`ï¼ˆtoken æ•°é‡ï¼‰
///   - 2.5 Pro: 128-32768ï¼Œä¸èƒ½ç¦ç”¨
///   - 2.5 Flash: 0-24576ï¼Œå¯è®¾ä¸º 0 ç¦ç”¨
///   - -1 è¡¨ç¤ºåŠ¨æ€åˆ†é…ï¼ˆé»˜è®¤ï¼‰
/// - **Gemini 3**: ä½¿ç”¨ `thinkingLevel`ï¼ˆé¢„è®¾çº§åˆ«ï¼‰
///   - 3 Pro: `"low"` | `"high"`ï¼ˆé»˜è®¤ highï¼Œä¸èƒ½ç¦ç”¨ï¼‰
///   - 3 Flash: `"minimal"` | `"low"` | `"medium"` | `"high"`
///
/// å‚è€ƒæ–‡æ¡£ï¼šhttps://ai.google.dev/gemini-api/docs/thinking
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiThinkingConfig {
    /// Gemini 2.5 ä½¿ç”¨ï¼šæ€ç»´é¢„ç®—ï¼ˆtoken æ•°é‡ï¼‰
    /// - 2.5 Pro: 128-32768 tokensï¼Œä¸èƒ½ç¦ç”¨
    /// - 2.5 Flash: 0-24576 tokensï¼Œå¯è®¾ä¸º 0 ç¦ç”¨
    /// - -1 è¡¨ç¤ºåŠ¨æ€åˆ†é…ï¼ˆæ¨èé»˜è®¤ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking_budget: Option<i32>,

    /// Gemini 3 ä½¿ç”¨ï¼šæ€ç»´çº§åˆ«
    /// - **Gemini 3 Pro**: `"low"` | `"high"`ï¼ˆä¸æ”¯æŒç¦ç”¨ï¼‰
    /// - **Gemini 3 Flash**: `"minimal"` | `"low"` | `"medium"` | `"high"`
    ///   - minimal: è¿‘ä¼¼ç¦ç”¨ï¼ˆå¤æ‚ä»»åŠ¡ä»å¯èƒ½æ€è€ƒï¼‰
    ///   - low: è½»åº¦æ€ç»´ï¼Œæœ€å°åŒ–å»¶è¿Ÿ
    ///   - medium: å¹³è¡¡æ¨¡å¼
    ///   - high: æ·±åº¦æ€ç»´ï¼ˆé»˜è®¤ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking_level: Option<String>,

    /// æ˜¯å¦åœ¨å“åº”ä¸­åŒ…å«æ€ç»´æ‘˜è¦
    #[serde(skip_serializing_if = "Option::is_none")]
    pub include_thoughts: Option<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiTool {
    pub function_declarations: Vec<GeminiFunctionDeclaration>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiFunctionDeclaration {
    pub name: String,
    pub description: String,
    pub parameters: Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiToolConfig {
    pub function_calling_config: GeminiFunctionCallingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiFunctionCallingConfig {
    pub mode: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub allowed_function_names: Option<Vec<String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiResponse {
    pub candidates: Vec<GeminiCandidate>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage_metadata: Option<GeminiUsageMetadata>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_feedback: Option<GeminiPromptFeedback>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiCandidate {
    pub content: GeminiContent,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,
    pub index: i32,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub safety_ratings: Option<Vec<GeminiSafetyRating>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiUsageMetadata {
    pub prompt_token_count: i32,
    pub candidates_token_count: i32,
    pub total_token_count: i32,
    /// æ€ç»´ token ç»Ÿè®¡ï¼ˆGemini 2.5/3 å¯ç”¨æ€ç»´æ—¶è¿”å›ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thoughts_token_count: Option<i32>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiSafetyRating {
    pub category: String,
    pub probability: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub blocked: Option<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GeminiPromptFeedback {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub block_reason: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub safety_ratings: Option<Vec<GeminiSafetyRating>>,
}

// ==================== æ ¸å¿ƒè½¬æ¢å‡½æ•° ====================

/// æ„å»ºGeminiè¯·æ±‚ï¼ˆä¸å‘ç½‘ç»œï¼‰
pub fn build_gemini_request(
    base_url: &str,
    api_key: &str,
    model: &str,
    openai_body: &Value,
) -> Result<ProviderRequest, AdapterError> {
    build_gemini_request_with_version(base_url, api_key, model, openai_body, None)
}

/// æ„å»ºGeminiè¯·æ±‚ï¼Œæ”¯æŒæŒ‡å®šAPIç‰ˆæœ¬
pub fn build_gemini_request_with_version(
    base_url: &str,
    api_key: &str,
    model: &str,
    openai_body: &Value,
    api_version: Option<&str>,
) -> Result<ProviderRequest, AdapterError> {
    // ååºåˆ—åŒ–OpenAIè¯·æ±‚ï¼ˆå…¼å®¹ max_total_tokens ç­‰æ‰©å±•å­—æ®µï¼‰
    let mut normalized_body = openai_body.clone();
    let mut reasoning_effort = None;
    let mut google_thinking_config: Option<Value> = None;
    let mut injected_top_k: Option<i32> = None;

    if let Value::Object(map) = &mut normalized_body {
        if let Some(value) = map.get("max_total_tokens").cloned() {
            if !map.contains_key("max_tokens") {
                map.insert("max_tokens".to_string(), value.clone());
            }
            map.remove("max_total_tokens");
        }

        if let Some(value) = map.get("max_completion_tokens").cloned() {
            if !map.contains_key("max_tokens") {
                map.insert("max_tokens".to_string(), value.clone());
            }
            map.remove("max_completion_tokens");
        }

        if let Some(value) = map.get("reasoning_effort") {
            reasoning_effort = value.as_str().map(|s| s.to_string());
        }
        map.remove("reasoning_effort");

        if let Some(value) = map.get("google_thinking_config").cloned() {
            google_thinking_config = Some(value);
        }
        map.remove("google_thinking_config");

        // ğŸ”§ ä¿®å¤ï¼šGemini adapter ä½¿ç”¨ thinkingConfig (camelCase) é”®å†™å…¥ï¼Œ
        // éœ€è¦åŒæ—¶è¯»å–æ­¤é”®ä½œä¸º fallbackï¼Œå¦åˆ™ includeThoughts ç­‰å€¼ä¼šä¸¢å¤±
        if google_thinking_config.is_none() {
            if let Some(value) = map.get("thinkingConfig").cloned() {
                google_thinking_config = Some(value);
            }
        }
        map.remove("thinkingConfig");
        // åŒæ—¶æ¸…ç† adapter å¯èƒ½æ³¨å…¥çš„ gemini_api_version
        map.remove("gemini_api_version");

        // è¯»å–é¡¶å±‚æ‰©å±•çš„ top_kï¼ˆæ¥è‡ª LLMManager.apply_reasoning_configï¼‰
        if let Some(v) = map.get("top_k").and_then(|v| v.as_i64()) {
            // clamp åˆç†èŒƒå›´ï¼ˆæœ€å°ä¸º1ï¼‰
            let clamped = v.clamp(1, 1_000_000);
            injected_top_k = Some(clamped as i32);
        }
    }

    let openai_req: OpenAIRequest = serde_json::from_value(normalized_body).map_err(|e| {
        AdapterError::SerializationError(format!("Failed to parse OpenAI request: {}", e))
    })?;

    // è½¬æ¢ä¸ºGeminiè¯·æ±‚
    let mut gemini_req = convert_openai_to_gemini(&openai_req)?;
    // è®°å½•æ˜¯å¦åŒ…å« systemInstructionï¼Œä»¥ä¾¿ç‰ˆæœ¬é€‰æ‹©ä¸å…¼å®¹é™çº§
    let system_instruction_present = gemini_req.system_instruction.is_some();

    // æ³¨å…¥ top_k åˆ° generation_configï¼ˆè‹¥å­˜åœ¨ï¼‰
    if let Some(top_k) = injected_top_k {
        let cfg = gemini_req
            .generation_config
            .get_or_insert(GeminiGenerationConfig {
                temperature: openai_req.temperature,
                top_p: openai_req.top_p,
                top_k: None,
                max_output_tokens: openai_req.max_tokens,
                stop_sequences: None,
                response_mime_type: None,
                response_schema: None,
                thinking_config: None,
            });
        cfg.top_k = Some(top_k);
    }

    // åº”ç”¨æ€ç»´é“¾/æ¨ç†é…ç½®
    let mut thinking_budget: Option<i32> = None;
    let mut thinking_level: Option<String> = None;
    let mut include_thoughts = None;

    // æ£€æµ‹æ˜¯å¦æ˜¯ Gemini 3 æ¨¡å‹ï¼ˆä½¿ç”¨ thinkingLevel è€Œé thinkingBudgetï¼‰
    let is_gemini_3 = model.to_lowercase().contains("gemini-3");

    if let Some(extra) = google_thinking_config.and_then(|v| v.as_object().cloned()) {
        // Gemini 3 ä¼˜å…ˆä½¿ç”¨ thinking_levelï¼ˆå…¼å®¹ snake_case å’Œ camelCaseï¼‰
        if let Some(level) = extra
            .get("thinking_level")
            .or_else(|| extra.get("thinkingLevel"))
            .and_then(|v| v.as_str())
        {
            thinking_level = Some(level.to_string());
        }
        // Gemini 2.5 ä½¿ç”¨ thinking_budgetï¼ˆå…¼å®¹ snake_case å’Œ camelCaseï¼‰
        if let Some(budget) = extra
            .get("thinking_budget")
            .or_else(|| extra.get("thinkingBudget"))
            .and_then(|v| v.as_i64())
        {
            let clamped = budget.clamp(-1, 2_147_483_647);
            thinking_budget = Some(clamped as i32);
        }
        // includeThoughtsï¼ˆå…¼å®¹ snake_case å’Œ camelCaseï¼‰
        if let Some(include) = extra
            .get("include_thoughts")
            .or_else(|| extra.get("includeThoughts"))
            .and_then(|v| v.as_bool())
        {
            include_thoughts = Some(include);
        }
    }

    // æ£€æµ‹æ˜¯å¦æ˜¯ Gemini 3 Flashï¼ˆæ”¯æŒæ›´å¤š thinkingLevel å€¼ï¼‰
    let is_gemini_3_flash =
        model.to_lowercase().contains("gemini-3") && model.to_lowercase().contains("flash");

    if let Some(effort) = reasoning_effort.as_deref() {
        if is_gemini_3 {
            // Gemini 3: å°† reasoning_effort æ˜ å°„åˆ° thinkingLevel
            // - Gemini 3 Pro: ä»…æ”¯æŒ "low", "high"
            // - Gemini 3 Flash: æ”¯æŒ "minimal", "low", "medium", "high"
            let level = match effort.to_ascii_lowercase().as_str() {
                "minimal" | "none" | "unset" => {
                    if is_gemini_3_flash {
                        "minimal"
                    } else {
                        "low"
                    } // Pro ä¸æ”¯æŒ minimal
                }
                "low" => "low",
                "medium" => {
                    if is_gemini_3_flash {
                        "medium"
                    } else {
                        "high"
                    } // Pro ä¸æ”¯æŒ medium
                }
                "high" => "high",
                _ => "low", // é»˜è®¤ä½¿ç”¨ low
            };
            thinking_level = Some(level.to_string());
        } else {
            // Gemini 2.5: ä½¿ç”¨ thinkingBudget
            let budget = match effort.to_ascii_lowercase().as_str() {
                "minimal" => Some(256),
                "low" => Some(1024),
                "medium" => Some(8192),
                "high" => Some(24576),
                "none" | "unset" => Some(0),
                _ => None,
            };
            if let Some(b) = budget {
                let clamped = b.clamp(-1, 2_147_483_647);
                thinking_budget = Some(clamped);
            }
        }
    }

    if thinking_budget.is_some() || thinking_level.is_some() || include_thoughts.is_some() {
        let cfg = gemini_req
            .generation_config
            .get_or_insert(GeminiGenerationConfig {
                temperature: openai_req.temperature,
                top_p: openai_req.top_p,
                top_k: None,
                max_output_tokens: openai_req.max_tokens,
                stop_sequences: None,
                response_mime_type: None,
                response_schema: None,
                thinking_config: None,
            });

        cfg.thinking_config = Some(GeminiThinkingConfig {
            thinking_budget: if is_gemini_3 { None } else { thinking_budget },
            thinking_level: if is_gemini_3 { thinking_level } else { None },
            include_thoughts,
        });
    }

    // æ„é€ URLå’Œheaders
    let is_stream = openai_req.stream.unwrap_or(false);
    let endpoint = if is_stream {
        "streamGenerateContent"
    } else {
        "generateContent"
    };

    let query = if is_stream {
        format!("alt=sse&key={}", api_key)
    } else {
        format!("key={}", api_key)
    };

    let mut resolved_version = api_version.map(|s| s.to_string());
    let thinking_config_present = gemini_req
        .generation_config
        .as_ref()
        .and_then(|cfg| cfg.thinking_config.as_ref())
        .is_some();

    // è‹¥åŒ…å«æ€ç»´é“¾é…ç½®ã€systemInstruction æˆ– Gemini 3 æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ v1beta
    // Gemini 3 æ¨¡å‹ä»…åœ¨ v1beta ä¸Šå¯ç”¨ï¼Œå³ä½¿æµ‹è¯•è¯·æ±‚ä¸å« thinkingConfig ä¹Ÿéœ€è¦ v1beta
    let require_v1beta = thinking_config_present || system_instruction_present || is_gemini_3;

    if require_v1beta {
        match resolved_version.as_deref() {
            Some("v1beta") => {}
            Some("v1") => {
                resolved_version = Some("v1beta".to_string());
            }
            Some(_) => {}
            None => {
                resolved_version = Some("v1beta".to_string());
            }
        }
    }

    let mut base_root = base_url.trim_end_matches('/').to_string();
    let mut version_in_base: Option<String> = None;

    if let Some(pos) = base_root.rfind('/') {
        let last_segment = &base_root[pos + 1..];
        let is_version_segment = last_segment.starts_with('v')
            && last_segment
                .chars()
                .nth(1)
                .map(|ch| ch.is_ascii_digit())
                .unwrap_or(false);
        if is_version_segment {
            version_in_base = Some(last_segment.to_string());
            base_root = base_root[..pos].to_string();
        }
    }

    let final_version = resolved_version
        .as_deref()
        .or_else(|| version_in_base.as_deref())
        .unwrap_or("v1");

    let base_root_trimmed = base_root.trim_end_matches('/');
    let base_with_version = if base_root_trimmed.ends_with("://") || base_root_trimmed.is_empty() {
        format!("{}{}", base_root_trimmed, final_version)
    } else {
        format!("{}/{}", base_root_trimmed, final_version)
    };

    // å…¼å®¹é™çº§ï¼šå¦‚æœæœ€ç»ˆç‰ˆæœ¬ä¸º v1ï¼Œä½†è¯·æ±‚ä½“åŒ…å« systemInstructionï¼Œåˆ™å°†å…¶åˆå¹¶è¿› contents å¹¶ç§»é™¤è¯¥å­—æ®µ
    if final_version == "v1" {
        if let Some(sys) = gemini_req.system_instruction.take() {
            // åˆå¹¶æ‰€æœ‰æ–‡æœ¬ part
            let mut merged_texts: Vec<String> = Vec::new();
            for part in sys.parts.into_iter() {
                if let Some(t) = part.text {
                    if !t.trim().is_empty() {
                        merged_texts.push(t);
                    }
                }
            }
            if !merged_texts.is_empty() {
                let merged = merged_texts.join("\n\n");
                if let Some(first) = gemini_req.contents.first_mut() {
                    // å°†ç³»ç»ŸæŒ‡ä»¤æ–‡æœ¬æ’å…¥åˆ°é¦–æ¡å†…å®¹çš„æœ€å‰é¢
                    first.parts.insert(
                        0,
                        GeminiPart {
                            text: Some(merged),
                            inline_data: None,
                            function_call: None,
                            function_response: None,
                            thought_signature: None,
                        },
                    );
                } else {
                    // å¦‚æ— å†…å®¹ï¼Œåˆ›å»ºä¸€æ¡ç”¨æˆ·å†…å®¹æ‰¿è½½ç³»ç»ŸæŒ‡ä»¤
                    gemini_req.contents.push(GeminiContent {
                        role: "user".to_string(),
                        parts: vec![GeminiPart {
                            text: Some(merged),
                            inline_data: None,
                            function_call: None,
                            function_response: None,
                            thought_signature: None,
                        }],
                    });
                }
            }
        }
    }

    let url = format!(
        "{}/models/{}:{}?{}",
        base_with_version, model, endpoint, query
    );

    let headers = vec![
        ("Content-Type".to_string(), "application/json".to_string()),
        ("x-goog-api-key".to_string(), api_key.to_string()),
    ];

    // åºåˆ—åŒ–Geminiè¯·æ±‚ä¸ºJSON
    let body = serde_json::to_value(gemini_req).map_err(|e| {
        AdapterError::SerializationError(format!("Failed to serialize Gemini request: {}", e))
    })?;

    Ok(ProviderRequest { url, headers, body })
}

/// è§£æå•è¡ŒSSEï¼ˆæµå¼ï¼‰
pub fn parse_gemini_stream_line(
    line: &str,
    pending_tool_calls: &Arc<Mutex<HashMap<i64, (String, String)>>>,
) -> Vec<StreamEvent> {
    let mut events = Vec::new();

    // æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
    if line.trim() == "data: [DONE]" {
        events.push(StreamEvent::Done);
        if let Ok(mut state) = pending_tool_calls.lock() {
            state.clear();
        }
        return events;
    }

    // æ£€æŸ¥æ˜¯å¦ä»¥"data: "å¼€å¤´
    if !line.starts_with("data: ") {
        return events;
    }

    // æå–JSONéƒ¨åˆ†
    let json_str = &line[6..]; // è·³è¿‡"data: "

    // å°è¯•è§£æJSON
    let json_value: Value = match serde_json::from_str(json_str) {
        Ok(v) => v,
        Err(_) => return events, // å¿½ç•¥éJSONè¡Œ
    };

    // æå–æ–‡æœ¬å†…å®¹
    if let Some(candidates) = json_value.get("candidates").and_then(|c| c.as_array()) {
        if let Some(candidate) = candidates.first() {
            if let Some(content) = candidate.get("content") {
                if let Some(parts) = content.get("parts").and_then(|p| p.as_array()) {
                    // å—æ§è°ƒè¯•ï¼šä»…åœ¨ debug æ„å»ºä¸­æ‰“å°ä¸€æ¬¡ part çš„å…³é”®å­—æ®µåï¼Œé¿å…æ³„æ¼æ­£æ–‡
                    if cfg!(debug_assertions) {
                        if let Some(first) = parts.first() {
                            if let Some(obj) = first.as_object() {
                                let keys: Vec<String> =
                                    obj.keys().take(12).map(|k| k.to_string()).collect();
                                println!("[Gemini][SSE][part_keys]={:?}", keys);
                            }
                        }
                    }
                    for (idx, part) in parts.iter().enumerate() {
                        let index = part
                            .get("index")
                            .and_then(|v| v.as_i64())
                            .unwrap_or(idx as i64);

                        let mut is_thought = match part.get("thought") {
                            Some(Value::Bool(b)) => *b,
                            Some(Value::String(s)) => !s.trim().is_empty(),
                            Some(Value::Object(obj)) => {
                                obj.get("value").and_then(|v| v.as_bool()).unwrap_or(true)
                            }
                            _ => false,
                        };
                        if !is_thought {
                            if let Some(metadata) = part.get("metadata") {
                                if metadata
                                    .get("type")
                                    .and_then(|v| v.as_str())
                                    .map(|s| s.eq_ignore_ascii_case("thought"))
                                    .unwrap_or(false)
                                {
                                    is_thought = true;
                                }
                                if metadata
                                    .get("isThought")
                                    .and_then(|v| v.as_bool())
                                    .unwrap_or(false)
                                {
                                    is_thought = true;
                                }
                            }
                        }
                        if !is_thought {
                            if let Some(kind) = part.get("kind").and_then(|v| v.as_str()) {
                                if kind.eq_ignore_ascii_case("thought") {
                                    is_thought = true;
                                }
                            }
                        }
                        if !is_thought {
                            if let Some(part_type) = part.get("type").and_then(|v| v.as_str()) {
                                if part_type.eq_ignore_ascii_case("thought") {
                                    is_thought = true;
                                }
                            }
                        }

                        if is_thought {
                            let extracted = extract_thought_texts(part);
                            if extracted.is_empty() {
                                if let Some(text) = part.get("text").and_then(|v| v.as_str()) {
                                    if !text.is_empty() {
                                        events.push(StreamEvent::ReasoningChunk(text.to_string()));
                                    }
                                }
                            } else {
                                for item in extracted {
                                    if !item.is_empty() {
                                        events.push(StreamEvent::ReasoningChunk(item));
                                    }
                                }
                            }
                        } else {
                            if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                                if !text.is_empty() {
                                    events.push(StreamEvent::ContentChunk(text.to_string()));
                                }
                            }
                        }

                        if let Some(thoughts) = part.get("thoughts").and_then(|t| t.as_array()) {
                            for item in thoughts {
                                for chunk in extract_thought_texts(item) {
                                    if !chunk.is_empty() {
                                        events.push(StreamEvent::ReasoningChunk(chunk));
                                    }
                                }
                            }
                        }

                        // æå– Gemini 3 thoughtSignatureï¼ˆå·¥å…·è°ƒç”¨å¿…éœ€ï¼‰
                        if let Some(signature) =
                            part.get("thoughtSignature").and_then(|v| v.as_str())
                        {
                            if !signature.is_empty() {
                                events.push(StreamEvent::ThoughtSignature(signature.to_string()));
                            }
                        }

                        // æå–å‡½æ•°è°ƒç”¨
                        if let Some(function_call) = part.get("functionCall") {
                            if let Some(name) = function_call.get("name").and_then(|n| n.as_str()) {
                                let args = function_call.get("args").cloned().unwrap_or(json!({}));
                                let args_str = serde_json::to_string(&args)
                                    .unwrap_or_else(|_| "{}".to_string());

                                let mut state = pending_tool_calls
                                    .lock()
                                    .expect("Gemini tool state poisoned");
                                let entry = state.entry(index).or_insert_with(|| {
                                    (format!("call-{}", Uuid::new_v4()), name.to_string())
                                });
                                let tool_call = json!({
                                    "id": entry.0,
                                    "type": "function",
                                    "function": {
                                        "name": entry.1,
                                        "arguments": args_str
                                    },
                                    "index": index
                                });

                                events.push(StreamEvent::ToolCall(tool_call));
                            }
                        }
                    }
                }
            }

            if let Some(thoughts) = candidate.get("thoughts").and_then(|v| v.as_array()) {
                for item in thoughts {
                    if let Some(text) = item
                        .get("text")
                        .and_then(|v| v.as_str())
                        .or_else(|| item.get("content").and_then(|v| v.as_str()))
                        .or_else(|| item.as_str())
                    {
                        if !text.is_empty() {
                            events.push(StreamEvent::ReasoningChunk(text.to_string()));
                        }
                    }
                }
            }

            // æå– candidate çº§åˆ«çš„ thoughtSignatureï¼ˆGemini 3ï¼‰
            if let Some(signature) = candidate.get("thoughtSignature").and_then(|v| v.as_str()) {
                if !signature.is_empty() {
                    events.push(StreamEvent::ThoughtSignature(signature.to_string()));
                }
            }

            // æ£€æŸ¥deltaç»“æ„ï¼ˆæŸäº›æµå¼å“åº”å¯èƒ½ä½¿ç”¨ï¼‰
            if let Some(delta) = candidate.get("delta") {
                if let Some(parts) = delta.get("parts").and_then(|p| p.as_array()) {
                    for (idx, part) in parts.iter().enumerate() {
                        let index = part
                            .get("index")
                            .and_then(|v| v.as_i64())
                            .unwrap_or(idx as i64);

                        let mut is_thought = part
                            .get("thought")
                            .and_then(|flag| flag.as_bool())
                            .unwrap_or(false);
                        if !is_thought {
                            if let Some(metadata) = part.get("metadata") {
                                if metadata
                                    .get("type")
                                    .and_then(|v| v.as_str())
                                    .map(|s| s.eq_ignore_ascii_case("thought"))
                                    .unwrap_or(false)
                                {
                                    is_thought = true;
                                }
                                if metadata
                                    .get("isThought")
                                    .and_then(|v| v.as_bool())
                                    .unwrap_or(false)
                                {
                                    is_thought = true;
                                }
                            }
                        }
                        if !is_thought {
                            if let Some(kind) = part.get("kind").and_then(|v| v.as_str()) {
                                if kind.eq_ignore_ascii_case("thought") {
                                    is_thought = true;
                                }
                            }
                        }
                        if !is_thought {
                            if let Some(part_type) = part.get("type").and_then(|v| v.as_str()) {
                                if part_type.eq_ignore_ascii_case("thought") {
                                    is_thought = true;
                                }
                            }
                        }

                        if is_thought {
                            let extracted = extract_thought_texts(part);
                            if extracted.is_empty() {
                                if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                                    if !text.is_empty() {
                                        events.push(StreamEvent::ReasoningChunk(text.to_string()));
                                    }
                                }
                            } else {
                                for item in extracted {
                                    if !item.is_empty() {
                                        events.push(StreamEvent::ReasoningChunk(item));
                                    }
                                }
                            }
                            continue;
                        }

                        if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                            if !text.is_empty() {
                                events.push(StreamEvent::ContentChunk(text.to_string()));
                            }
                        }
                        if let Some(thoughts) = part.get("thoughts").and_then(|t| t.as_array()) {
                            for item in thoughts {
                                let extracted = extract_thought_texts(item);
                                if extracted.is_empty() {
                                    if let Some(text) = item
                                        .get("text")
                                        .and_then(|v| v.as_str())
                                        .or_else(|| item.get("content").and_then(|v| v.as_str()))
                                    {
                                        if !text.is_empty() {
                                            events.push(StreamEvent::ReasoningChunk(
                                                text.to_string(),
                                            ));
                                        }
                                    }
                                } else {
                                    for entry in extracted {
                                        if !entry.is_empty() {
                                            events.push(StreamEvent::ReasoningChunk(entry));
                                        }
                                    }
                                }
                            }
                        }
                        if let Some(function_call) = part.get("functionCall") {
                            if let Some(name) = function_call.get("name").and_then(|n| n.as_str()) {
                                let args = function_call.get("args").cloned().unwrap_or(json!({}));
                                let args_str = serde_json::to_string(&args)
                                    .unwrap_or_else(|_| "{}".to_string());

                                let mut state = pending_tool_calls
                                    .lock()
                                    .expect("Gemini tool state poisoned");
                                let entry = state.entry(index).or_insert_with(|| {
                                    (format!("call-{}", Uuid::new_v4()), name.to_string())
                                });

                                let tool_call = json!({
                                    "id": entry.0,
                                    "type": "function",
                                    "function": {
                                        "name": entry.1,
                                        "arguments": args_str
                                    },
                                    "index": index
                                });

                                events.push(StreamEvent::ToolCall(tool_call));
                            }
                        }
                    }
                }
            }
        }
    }

    if let Some(thoughts) = json_value.get("thoughts").and_then(|v| v.as_array()) {
        for item in thoughts {
            if let Some(text) = item
                .get("text")
                .and_then(|v| v.as_str())
                .or_else(|| item.get("content").and_then(|v| v.as_str()))
                .or_else(|| item.as_str())
            {
                if !text.is_empty() {
                    events.push(StreamEvent::ReasoningChunk(text.to_string()));
                }
            }
        }
    }

    // æå–ç”¨é‡ä¿¡æ¯ï¼ˆå¥å£®æ€§ï¼šç¡®ä¿å­—æ®µå®Œæ•´æ€§ï¼‰
    if let Some(usage_metadata) = json_value.get("usageMetadata") {
        // æä¾›å¥å£®çš„ç”¨é‡ä¿¡æ¯ï¼Œç¼ºå¤±å­—æ®µæ—¶ä½¿ç”¨é›¶å€¼
        let prompt_tokens = usage_metadata
            .get("promptTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(0);
        let completion_tokens = usage_metadata
            .get("candidatesTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(0);
        let total_tokens = usage_metadata
            .get("totalTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(prompt_tokens + completion_tokens);
        // P2 ä¿®å¤ï¼šè§£æ thoughtsTokenCountï¼ˆGemini 2.5/3 å¯ç”¨æ€ç»´æ—¶è¿”å›ï¼‰
        let thoughts_tokens = usage_metadata
            .get("thoughtsTokenCount")
            .and_then(|t| t.as_i64());

        let prompt_tokens = prompt_tokens as i32;
        let completion_tokens = completion_tokens as i32;
        let total_tokens = total_tokens as i32;

        let mut robust_usage = json!({
            "promptTokenCount": prompt_tokens,
            "candidatesTokenCount": completion_tokens,
            "totalTokenCount": total_tokens,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            // ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡éœ€è¦
            "original": usage_metadata
        });

        // æ·»åŠ æ€ç»´ token ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if let Some(thoughts) = thoughts_tokens {
            robust_usage["thoughtsTokenCount"] = json!(thoughts);
            robust_usage["reasoning_tokens"] = json!(thoughts);
        }

        events.push(StreamEvent::Usage(robust_usage));
    }

    // æ£€æŸ¥å®‰å…¨é˜»æ–­
    if let Some(prompt_feedback) = json_value.get("promptFeedback") {
        if let Some(block_reason) = prompt_feedback.get("blockReason") {
            let safety_info = json!({
                "type": "prompt_blocked",
                "reason": block_reason,
                "details": prompt_feedback
            });
            events.push(StreamEvent::SafetyBlocked(safety_info));
        }
    }

    // æ£€æŸ¥å€™é€‰é¡¹å®‰å…¨é˜»æ–­
    if let Some(candidates) = json_value.get("candidates").and_then(|c| c.as_array()) {
        for candidate in candidates {
            if let Some(finish_reason) = candidate.get("finishReason").and_then(|f| f.as_str()) {
                if finish_reason == "SAFETY" {
                    let safety_info = json!({
                        "type": "content_blocked",
                        "reason": "SAFETY",
                        "safetyRatings": candidate.get("safetyRatings").cloned(),
                        "details": candidate
                    });
                    events.push(StreamEvent::SafetyBlocked(safety_info));
                }
            }
        }
    }

    if !events.is_empty() {
        if let StreamEvent::Done = events.last().unwrap() {
            if let Ok(mut state) = pending_tool_calls.lock() {
                state.clear();
            }
        }
    }

    events
}

/// éæµå¼å“åº”è½¬æ¢ï¼ˆGemini -> OpenAIï¼‰
fn extract_thought_texts(value: &Value) -> Vec<String> {
    let mut out = Vec::new();
    match value {
        Value::String(s) => out.push(s.to_string()),
        Value::Object(obj) => {
            if let Some(text) = obj.get("text").and_then(|v| v.as_str()) {
                out.push(text.to_string());
            }
            if let Some(content) = obj.get("content") {
                out.extend(extract_thought_texts(content));
            }
            if let Some(parts) = obj.get("parts").and_then(|v| v.as_array()) {
                for part in parts {
                    out.extend(extract_thought_texts(part));
                }
            }
            if let Some(data) = obj.get("data") {
                out.extend(extract_thought_texts(data));
            }
        }
        Value::Array(arr) => {
            for item in arr {
                out.extend(extract_thought_texts(item));
            }
        }
        _ => {}
    }
    out
}

pub fn convert_gemini_nonstream_response_to_openai(
    gemini_json: &Value,
    model: &str,
) -> Result<Value, AdapterError> {
    // é¦–å…ˆæ£€æŸ¥å®‰å…¨é˜»æ–­
    if let Some(prompt_feedback) = gemini_json.get("promptFeedback") {
        if let Some(block_reason) = prompt_feedback.get("blockReason") {
            let error_msg = format!("Request blocked due to safety reasons: {}", block_reason);
            return Err(AdapterError::InvalidFormat(error_msg));
        }
    }

    // æå–candidates
    let candidates = gemini_json
        .get("candidates")
        .and_then(|c| c.as_array())
        .ok_or_else(|| {
            AdapterError::InvalidFormat("Missing candidates in Gemini response".to_string())
        })?;

    if candidates.is_empty() {
        return Err(AdapterError::InvalidFormat(
            "Empty candidates array".to_string(),
        ));
    }

    let mut choices = Vec::new();

    for (index, candidate) in candidates.iter().enumerate() {
        let mut text_parts = Vec::new();
        let mut reasoning_parts = Vec::new();
        let mut tool_calls = Vec::new();

        // æå–å†…å®¹
        if let Some(content) = candidate.get("content") {
            if let Some(parts) = content.get("parts").and_then(|p| p.as_array()) {
                for part in parts {
                    if cfg!(debug_assertions) {
                        if let Ok(debug_part) = serde_json::to_string(part) {
                            println!("[Gemini][convert_nonstream] part: {}", debug_part);
                        }
                    }
                    // æå–æ–‡æœ¬
                    let is_thought = part
                        .get("thought")
                        .and_then(|flag| flag.as_bool())
                        .unwrap_or(false);

                    if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                        if is_thought {
                            reasoning_parts.push(text.to_string());
                        } else {
                            text_parts.push(text.to_string());
                        }
                    }

                    // æå–å‡½æ•°è°ƒç”¨
                    if let Some(function_call) = part.get("functionCall") {
                        if let Some(name) = function_call.get("name").and_then(|n| n.as_str()) {
                            let args = function_call.get("args").cloned().unwrap_or(json!({}));
                            let args_str =
                                serde_json::to_string(&args).unwrap_or_else(|_| "{}".to_string());

                            tool_calls.push(json!({
                                "id": format!("call-{}", Uuid::new_v4()),
                                "type": "function",
                                "function": {
                                    "name": name,
                                    "arguments": args_str
                                }
                            }));
                        }
                    }
                }
            }
        }

        // åˆå¹¶æ–‡æœ¬
        let mut main_text = text_parts.join("\n");
        let mut reasoning_texts = reasoning_parts;

        if reasoning_texts.is_empty() {
            let mut fallback_reasoning = Vec::new();
            if let Some(candidate_thoughts) = candidate.get("thoughts") {
                fallback_reasoning.extend(extract_thought_texts(candidate_thoughts));
            }
            if fallback_reasoning.is_empty() {
                if let Some(global_thoughts) = gemini_json.get("thoughts") {
                    fallback_reasoning.extend(extract_thought_texts(global_thoughts));
                }
            }
            reasoning_texts = fallback_reasoning;
        }

        for snippet in &reasoning_texts {
            let snippet_trim = snippet.trim();
            if snippet_trim.is_empty() {
                continue;
            }
            if main_text.contains(snippet_trim) {
                main_text = main_text.replacen(snippet_trim, "", 1);
            }
        }
        main_text = main_text.trim().to_string();
        let combined_reasoning = reasoning_texts
            .iter()
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            .collect::<Vec<_>>()
            .join("\n\n");

        // æ„é€ message
        let mut message = json!({
            "role": "assistant",
            "content": main_text
        });

        if !combined_reasoning.is_empty() {
            message["thinking_content"] = json!(combined_reasoning.clone());
        }

        // æ·»åŠ tool_callså¦‚æœå­˜åœ¨
        if !tool_calls.is_empty() {
            message["tool_calls"] = json!(tool_calls);
        }

        // è·å–finish_reason
        let finish_reason = candidate
            .get("finishReason")
            .and_then(|f| f.as_str())
            .map(|s| s.to_string());

        let finish_reason_str = finish_reason
            .as_deref()
            .map(map_gemini_finish_reason)
            .unwrap_or("stop");

        let mut choice = json!({
            "index": index as i32,
            "message": message,
            "finish_reason": finish_reason_str
        });

        if !combined_reasoning.is_empty() {
            choice["message"] = message.clone();
        }

        choices.push(choice);
    }

    // è½¬æ¢ç”¨é‡ä¿¡æ¯ï¼ˆå¥å£®æ€§ï¼šç¼ºå¤±å­—æ®µæ—¶ç»™å‡ºé›¶å€¼é»˜è®¤ï¼‰
    let usage = if let Some(usage_metadata) = gemini_json.get("usageMetadata") {
        let prompt_tokens = usage_metadata
            .get("promptTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(0) as i32;
        let completion_tokens = usage_metadata
            .get("candidatesTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(0) as i32;
        let total_tokens = usage_metadata
            .get("totalTokenCount")
            .and_then(|t| t.as_i64())
            .unwrap_or(prompt_tokens as i64 + completion_tokens as i64)
            as i32;
        // P2 ä¿®å¤ï¼šè§£æ thoughtsTokenCountï¼ˆGemini 2.5/3 å¯ç”¨æ€ç»´æ—¶è¿”å›ï¼‰
        let thoughts_tokens = usage_metadata
            .get("thoughtsTokenCount")
            .and_then(|t| t.as_i64())
            .map(|t| t as i32);

        let mut usage_obj = json!({
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens
        });

        // æ·»åŠ æ€ç»´ token ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if let Some(thoughts) = thoughts_tokens {
            usage_obj["reasoning_tokens"] = json!(thoughts);
        }

        Some(usage_obj)
    } else {
        // å³ä½¿æ²¡æœ‰usageMetadataï¼Œä¹Ÿæä¾›é»˜è®¤çš„é›¶å€¼ç”¨é‡ä¿¡æ¯
        Some(json!({
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }))
    };

    // æ„é€ OpenAIå“åº”
    let mut response = json!({
        "id": format!("chatcmpl-{}", Uuid::new_v4()),
        "object": "chat.completion",
        "created": std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs() as i64,
        "model": model,
        "choices": choices
    });

    if let Some(usage_value) = usage {
        response["usage"] = usage_value;
    }

    Ok(response)
}

// ==================== å†…éƒ¨è¾…åŠ©å‡½æ•° ====================

fn convert_openai_to_gemini(openai_req: &OpenAIRequest) -> Result<GeminiRequest, AdapterError> {
    let mut contents = Vec::new();
    let mut system_instruction = None;

    // å¤„ç†æ¶ˆæ¯
    for message in &openai_req.messages {
        match message.role.as_str() {
            "system" => {
                // ç³»ç»Ÿæ¶ˆæ¯è½¬ä¸ºsystem_instruction
                if let Some(content) = &message.content {
                    let text = extract_text_from_openai_content(content);
                    if !text.is_empty() {
                        system_instruction = Some(GeminiContent {
                            role: "user".to_string(),
                            parts: vec![GeminiPart {
                                text: Some(text),
                                inline_data: None,
                                function_call: None,
                                function_response: None,
                                thought_signature: None,
                            }],
                        });
                    }
                }
            }
            "user" => {
                // ç”¨æˆ·æ¶ˆæ¯
                if let Some(content) = &message.content {
                    let parts = convert_openai_content_to_gemini_parts(content)?;
                    if !parts.is_empty() {
                        contents.push(GeminiContent {
                            role: "user".to_string(),
                            parts,
                        });
                    }
                }
            }
            "assistant" => {
                // åŠ©æ‰‹æ¶ˆæ¯
                let mut parts = Vec::new();

                // å¤„ç†æ–‡æœ¬å†…å®¹
                if let Some(content) = &message.content {
                    let text = extract_text_from_openai_content(content);
                    if !text.is_empty() {
                        parts.push(GeminiPart {
                            text: Some(text),
                            inline_data: None,
                            function_call: None,
                            function_response: None,
                            thought_signature: None,
                        });
                    }
                }

                // å¤„ç†å·¥å…·è°ƒç”¨
                // ğŸ”§ Gemini 3ï¼šthoughtSignature å¿…é¡»å’Œ functionCall åœ¨åŒä¸€ä¸ª part ä¸­
                // æµå¼å“åº”ä¸­ [part_keys]=["functionCall", "thoughtSignature"] è¡¨æ˜å®ƒä»¬æ˜¯åŒä¸€ä¸ª part
                if let Some(tool_calls) = &message.tool_calls {
                    let sig = message.thought_signature.clone();
                    for (i, tool_call) in tool_calls.iter().enumerate() {
                        if tool_call.tool_type == "function" {
                            let args: Value = serde_json::from_str(&tool_call.function.arguments)
                                .unwrap_or(json!({}));
                            parts.push(GeminiPart {
                                text: None,
                                inline_data: None,
                                function_call: Some(GeminiFunctionCall {
                                    name: tool_call.function.name.clone(),
                                    args,
                                }),
                                function_response: None,
                                // ç¬¬ä¸€ä¸ª functionCall part æºå¸¦ thoughtSignature
                                thought_signature: if i == 0 { sig.clone() } else { None },
                            });
                        }
                    }
                }

                if !parts.is_empty() {
                    contents.push(GeminiContent {
                        role: "model".to_string(),
                        parts,
                    });
                }
            }
            "function" | "tool" => {
                // å‡½æ•°/å·¥å…·å“åº”
                // ğŸ”§ ä¿®å¤ï¼šOpenAI tool æ¶ˆæ¯å¯èƒ½æ²¡æœ‰ name å­—æ®µï¼Œéœ€è¦ä» tool_call_id
                // æŸ¥æ‰¾å‰é¢ assistant æ¶ˆæ¯çš„ tool_calls æ¥è·å–å‡½æ•°å
                let resolved_name = message.name.clone().or_else(|| {
                    if let Some(tool_call_id) = &message.tool_call_id {
                        // ä»å‰é¢çš„æ¶ˆæ¯ä¸­æŸ¥æ‰¾å¯¹åº”çš„ tool_call
                        for prev_msg in &openai_req.messages {
                            if prev_msg.role == "assistant" {
                                if let Some(tool_calls) = &prev_msg.tool_calls {
                                    for tc in tool_calls {
                                        if tc.id == *tool_call_id {
                                            return Some(tc.function.name.clone());
                                        }
                                    }
                                }
                            }
                        }
                    }
                    None
                });

                if let Some(name) = resolved_name {
                    if let Some(content) = &message.content {
                        let response_text = extract_text_from_openai_content(content);
                        let response_value: Value = serde_json::from_str(&response_text)
                            .unwrap_or_else(|_| json!({"result": response_text}));

                        let new_part = GeminiPart {
                            text: None,
                            inline_data: None,
                            function_call: None,
                            function_response: Some(GeminiFunctionResponse {
                                name: name.clone(),
                                response: response_value,
                            }),
                            thought_signature: None,
                        };

                        // ğŸ”§ Gemini è¦æ±‚è§’è‰²äº¤æ›¿ï¼šå¤šä¸ª functionResponse å¿…é¡»åˆå¹¶åˆ°åŒä¸€ä¸ª user content å—
                        // å®˜æ–¹æ–‡æ¡£ç¤ºä¾‹ï¼šå¹¶è¡Œå·¥å…·è°ƒç”¨çš„ç»“æœåœ¨ä¸€ä¸ª user æ¶ˆæ¯ä¸­åŒ…å«å¤šä¸ª functionResponse parts
                        let should_merge = contents.last().map_or(false, |last: &GeminiContent| {
                            last.role == "user"
                                && last.parts.iter().all(|p| p.function_response.is_some())
                        });

                        if should_merge {
                            contents.last_mut().unwrap().parts.push(new_part);
                        } else {
                            contents.push(GeminiContent {
                                role: "user".to_string(),
                                parts: vec![new_part],
                            });
                        }
                    }
                }
            }
            _ => {
                // å¿½ç•¥æœªçŸ¥è§’è‰²
            }
        }
    }

    // ğŸ”§ é˜²å¾¡æ€§åå¤„ç†ï¼šåˆå¹¶è¿ç»­åŒè§’è‰² contentï¼Œç¡®ä¿ Gemini è§’è‰²äº¤æ›¿è¦æ±‚
    // Gemini API è¦æ±‚ contents ä¸­ user å’Œ model è§’è‰²ä¸¥æ ¼äº¤æ›¿
    // å¦‚æœ OpenAI æ¶ˆæ¯è½¬æ¢åäº§ç”Ÿè¿ç»­åŒè§’è‰² turnï¼ˆå¦‚ä¸¤ä¸ªè¿ç»­ assistant/modelï¼‰ï¼Œä¼šè§¦å‘ 400 é”™è¯¯
    if contents.len() >= 2 {
        let mut merged_contents: Vec<GeminiContent> = Vec::with_capacity(contents.len());
        for content in contents.drain(..) {
            let should_merge = merged_contents
                .last()
                .map_or(false, |last| last.role == content.role);
            if should_merge {
                let last = merged_contents.last_mut().unwrap();
                let merged_count = content.parts.len();
                last.parts.extend(content.parts);
                log::warn!(
                    "[GeminiConverter] Merged consecutive '{}' turns ({} parts appended)",
                    last.role,
                    merged_count
                );
            } else {
                merged_contents.push(content);
            }
        }
        contents = merged_contents;
    }

    // ğŸ”§ Gemini 3+ é˜²æŠ¤ï¼šå°†æ²¡æœ‰ thoughtSignature çš„ functionCall é™çº§ä¸ºæ–‡æœ¬
    // åˆæˆçš„ load_skills ç­‰å·¥å…·è°ƒç”¨æ²¡æœ‰çœŸå®çš„ thoughtSignatureï¼Œ
    // Gemini 3+ ä¼šæ‹’ç»æ­¤ç±»è¯·æ±‚ï¼ˆ400: "Function call is missing a thought_signature"ï¼‰ã€‚
    // å°†å®ƒä»¬åŠå¯¹åº”çš„ functionResponse è½¬æ¢ä¸ºç­‰ä»·çš„æ–‡æœ¬æ¶ˆæ¯ã€‚
    {
        let mut i = 0;
        while i < contents.len() {
            let has_unprotected_fc = contents[i].role == "model"
                && contents[i]
                    .parts
                    .iter()
                    .any(|p| p.function_call.is_some() && p.thought_signature.is_none());

            if has_unprotected_fc {
                // å°† functionCall parts è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
                for part in &mut contents[i].parts {
                    if part.function_call.is_some() && part.thought_signature.is_none() {
                        if let Some(fc) = part.function_call.take() {
                            let args_str =
                                serde_json::to_string(&fc.args).unwrap_or_else(|_| "{}".into());
                            part.text =
                                Some(format!("[Tool call: {}({})]", fc.name, args_str));
                        }
                    }
                }

                // å°†ç´§éšå…¶åçš„ user content ä¸­çš„ functionResponse parts ä¹Ÿè½¬æ¢ä¸ºæ–‡æœ¬
                if i + 1 < contents.len() && contents[i + 1].role == "user" {
                    for part in &mut contents[i + 1].parts {
                        if part.function_response.is_some() {
                            if let Some(fr) = part.function_response.take() {
                                let resp_str = serde_json::to_string(&fr.response)
                                    .unwrap_or_else(|_| "{}".into());
                                part.text = Some(format!(
                                    "[Tool result for {}: {}]",
                                    fr.name, resp_str
                                ));
                            }
                        }
                    }
                }

                log::warn!(
                    "[GeminiConverter] Converted functionCall without thoughtSignature to text at content index {}",
                    i
                );
            }
            i += 1;
        }
    }

    // ç¡®ä¿ç¬¬ä¸€ä¸ª content æ˜¯ user è§’è‰²ï¼ˆGemini è¦æ±‚ï¼‰
    if let Some(first) = contents.first() {
        if first.role == "model" {
            log::warn!(
                "[GeminiConverter] First content is 'model' role, inserting dummy user turn"
            );
            contents.insert(
                0,
                GeminiContent {
                    role: "user".to_string(),
                    parts: vec![GeminiPart {
                        text: Some(".".to_string()),
                        inline_data: None,
                        function_call: None,
                        function_response: None,
                        thought_signature: None,
                    }],
                },
            );
        }
    }

    // ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå†…å®¹
    if contents.is_empty() && system_instruction.is_none() {
        return Err(AdapterError::InvalidFormat(
            "No valid content to convert".to_string(),
        ));
    }

    // è½¬æ¢ç”Ÿæˆé…ç½®
    let stop_sequences = openai_req.stop.as_ref().map(|stop| stop.to_vec());

    let mut generation_config = GeminiGenerationConfig {
        temperature: openai_req.temperature,
        top_p: openai_req.top_p,
        top_k: None,
        max_output_tokens: openai_req.max_tokens,
        stop_sequences,
        response_mime_type: None,
        response_schema: None,
        thinking_config: None,
    };

    if let Some(format_value) = &openai_req.response_format {
        if let Some(format_obj) = format_value.as_object() {
            if let Some(format_type) = format_obj.get("type").and_then(|v| v.as_str()) {
                match format_type {
                    "json_object" => {
                        generation_config.response_mime_type = Some("application/json".to_string());
                    }
                    "json_schema" => {
                        generation_config.response_mime_type = Some("application/json".to_string());
                        if let Some(schema_holder) = format_obj.get("json_schema") {
                            if let Some(schema_value) = schema_holder.get("schema") {
                                generation_config.response_schema = Some(schema_value.clone());
                            } else {
                                generation_config.response_schema = Some(schema_holder.clone());
                            }
                        }
                    }
                    _ => {}
                }
            }
        }
    }

    let generation_config = if generation_config.temperature.is_some()
        || generation_config.top_p.is_some()
        || generation_config.top_k.is_some()
        || generation_config.max_output_tokens.is_some()
        || generation_config.stop_sequences.is_some()
        || generation_config.response_mime_type.is_some()
        || generation_config.response_schema.is_some()
    {
        Some(generation_config)
    } else {
        None
    };

    /// é€’å½’ä¿®è¡¥ JSON Schemaï¼Œç¡®ä¿ç¬¦åˆ Gemini åŸç”Ÿ API çš„ä¸¥æ ¼è¦æ±‚ï¼š
    /// - `type: "array"` æ—¶å¿…é¡»æœ‰ `items` å­—æ®µ
    /// - æ¯ä¸ª `items` å¿…é¡»åŒ…å« `type` å­—æ®µ
    fn fix_schema_for_gemini(value: &mut Value) {
        match value {
            Value::Object(map) => {
                // å¦‚æœ type=array ä½†ç¼ºå°‘ itemsï¼Œè¡¥å……é»˜è®¤ items
                if map.get("type").and_then(|v| v.as_str()) == Some("array") {
                    if !map.contains_key("items") {
                        map.insert("items".to_string(), json!({"type": "string"}));
                    }
                }
                // å¦‚æœæœ‰ items ä½† items ç¼ºå°‘ typeï¼Œè¡¥å……é»˜è®¤ type
                if let Some(items) = map.get_mut("items") {
                    if let Value::Object(items_map) = items {
                        if !items_map.contains_key("type") {
                            if items_map.contains_key("properties") {
                                items_map.insert("type".to_string(), json!("object"));
                            } else {
                                items_map.insert("type".to_string(), json!("string"));
                            }
                        }
                    }
                }
                // é€’å½’å¤„ç†æ‰€æœ‰å­å€¼
                for v in map.values_mut() {
                    fix_schema_for_gemini(v);
                }
            }
            Value::Array(arr) => {
                for v in arr.iter_mut() {
                    fix_schema_for_gemini(v);
                }
            }
            _ => {}
        }
    }

    // è½¬æ¢å·¥å…·
    let tools = if let Some(openai_tools) = &openai_req.tools {
        let mut function_declarations = Vec::new();
        for tool in openai_tools {
            if tool.tool_type == "function" {
                let mut params = tool.function.parameters.clone();
                // Gemini åŸç”Ÿ API è¦æ±‚æ‰€æœ‰ schema èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ itemsï¼‰éƒ½å¿…é¡»æœ‰ type å­—æ®µ
                fix_schema_for_gemini(&mut params);
                function_declarations.push(GeminiFunctionDeclaration {
                    name: tool.function.name.clone(),
                    description: tool.function.description.clone().unwrap_or_default(),
                    parameters: params,
                });
            }
        }
        if !function_declarations.is_empty() {
            Some(vec![GeminiTool {
                function_declarations,
            }])
        } else {
            None
        }
    } else {
        None
    };

    // è½¬æ¢tool_choiceåˆ°tool_config
    let tool_config = if let Some(tool_choice) = &openai_req.tool_choice {
        convert_tool_choice_to_tool_config(tool_choice)?
    } else {
        None
    };

    Ok(GeminiRequest {
        contents,
        system_instruction,
        generation_config,
        tools,
        tool_config,
    })
}

fn extract_text_from_openai_content(content: &OpenAIMessageContent) -> String {
    match content {
        OpenAIMessageContent::Text(text) => text.clone(),
        OpenAIMessageContent::Array(parts) => {
            let texts: Vec<String> = parts
                .iter()
                .filter_map(|part| match part {
                    OpenAIContentPart::Text { text } => Some(text.clone()),
                    _ => None,
                })
                .collect();
            texts.join("\n")
        }
    }
}

fn convert_openai_content_to_gemini_parts(
    content: &OpenAIMessageContent,
) -> Result<Vec<GeminiPart>, AdapterError> {
    let mut parts = Vec::new();

    match content {
        OpenAIMessageContent::Text(text) => {
            if !text.is_empty() {
                parts.push(GeminiPart {
                    text: Some(text.clone()),
                    inline_data: None,
                    function_call: None,
                    function_response: None,
                    thought_signature: None,
                });
            }
        }
        OpenAIMessageContent::Array(content_parts) => {
            for part in content_parts {
                match part {
                    OpenAIContentPart::Text { text } => {
                        if !text.is_empty() {
                            parts.push(GeminiPart {
                                text: Some(text.clone()),
                                inline_data: None,
                                function_call: None,
                                function_response: None,
                                thought_signature: None,
                            });
                        }
                    }
                    OpenAIContentPart::ImageUrl { image_url } => {
                        if let Some(inline_data) = image_url_to_inline_data(image_url) {
                            parts.push(GeminiPart {
                                text: None,
                                inline_data: Some(inline_data),
                                function_call: None,
                                function_response: None,
                                thought_signature: None,
                            });
                        }
                    }
                }
            }
        }
    }

    Ok(parts)
}

fn image_url_to_inline_data(image_url: &OpenAIImageUrl) -> Option<GeminiInlineData> {
    if image_url.url.starts_with("data:") {
        let parts_split: Vec<&str> = image_url.url.splitn(2, ',').collect();
        if parts_split.len() == 2 {
            let header = parts_split[0];
            let data = parts_split[1];
            let mime_type = header
                .trim_start_matches("data:")
                .trim_end_matches(";base64")
                .to_string();
            return Some(GeminiInlineData {
                mime_type,
                data: data.to_string(),
            });
        }
    }

    if image_url.url.starts_with("http://") || image_url.url.starts_with("https://") {
        if let Some((bytes, mime_hint)) = fetch_binary_with_cache(&image_url.url) {
            let mime_type = mime_hint.unwrap_or_else(|| "application/octet-stream".to_string());
            let data = general_purpose::STANDARD.encode(bytes);
            return Some(GeminiInlineData { mime_type, data });
        }
    }

    None
}

fn map_gemini_finish_reason(reason: &str) -> &'static str {
    match reason {
        "STOP" | "STOP_REASON_UNSPECIFIED" | "FINISH_REASON_UNSPECIFIED" | "OTHER" => "stop",
        "MAX_TOKENS" => "length",
        "SAFETY" | "RECITATION" | "BLOCKLIST" | "PROHIBITED_CONTENT" | "SPII" => "content_filter",
        "MALFORMED_FUNCTION_CALL" | "TOOL_CALL_REQUIRED" => "tool_calls",
        _ => "stop",
    }
}

/// è½¬æ¢OpenAI tool_choiceåˆ°Gemini tool_config
fn convert_tool_choice_to_tool_config(
    tool_choice: &Value,
) -> Result<Option<GeminiToolConfig>, AdapterError> {
    // å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„tool_choice
    if let Some(choice_str) = tool_choice.as_str() {
        match choice_str {
            "auto" => {
                return Ok(Some(GeminiToolConfig {
                    function_calling_config: GeminiFunctionCallingConfig {
                        mode: "AUTO".to_string(),
                        allowed_function_names: None,
                    },
                }));
            }
            "none" => {
                return Ok(Some(GeminiToolConfig {
                    function_calling_config: GeminiFunctionCallingConfig {
                        mode: "NONE".to_string(),
                        allowed_function_names: None,
                    },
                }));
            }
            _ => {
                // å¿½ç•¥æœªçŸ¥çš„å­—ç¬¦ä¸²å€¼
                return Ok(None);
            }
        }
    }

    // å¤„ç†å¯¹è±¡å½¢å¼çš„tool_choice
    if let Some(choice_obj) = tool_choice.as_object() {
        if let Some(choice_type) = choice_obj.get("type").and_then(|t| t.as_str()) {
            if choice_type == "function" {
                if let Some(function_obj) = choice_obj.get("function").and_then(|f| f.as_object()) {
                    if let Some(function_name) = function_obj.get("name").and_then(|n| n.as_str()) {
                        return Ok(Some(GeminiToolConfig {
                            function_calling_config: GeminiFunctionCallingConfig {
                                mode: "ANY".to_string(),
                                allowed_function_names: Some(vec![function_name.to_string()]),
                            },
                        }));
                    }
                }
            }
        }

        // å¤„ç†ç›´æ¥æŒ‡å®šå‡½æ•°åçš„æƒ…å†µï¼ˆæ‰©å±•æ”¯æŒï¼‰
        if let Some(function_name) = choice_obj.get("function").and_then(|f| f.as_str()) {
            return Ok(Some(GeminiToolConfig {
                function_calling_config: GeminiFunctionCallingConfig {
                    mode: "ANY".to_string(),
                    allowed_function_names: Some(vec![function_name.to_string()]),
                },
            }));
        }
    }

    // é»˜è®¤æƒ…å†µï¼šè¿”å›Noneï¼ˆä½¿ç”¨Geminié»˜è®¤è¡Œä¸ºï¼‰
    Ok(None)
}

// ==================== æµ‹è¯•æ¨¡å— ====================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_build_gemini_request_nonstream() {
        let openai_body = json!({
            "model": "gpt-4",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What's in this image?"},
                        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQ..."}}
                    ]
                }
            ],
            "temperature": 0.7,
            "max_tokens": 100
        });

        let result = build_gemini_request(
            "https://generativelanguage.googleapis.com",
            "test-api-key",
            "gemini-pro-vision",
            &openai_body,
        );

        assert!(result.is_ok());
        let request = result.unwrap();

        // éªŒè¯URL
        assert!(request.url.contains(":generateContent?"));
        assert!(request.url.contains("key=test-api-key"));

        // éªŒè¯headersï¼ˆContent-Type å’Œ x-goog-api-keyï¼‰
        assert_eq!(request.headers.len(), 2);
        assert!(request.headers.iter().any(|(k, _)| k == "Content-Type"));
        assert!(request.headers.iter().any(|(k, _)| k == "x-goog-api-key"));

        // éªŒè¯bodyç»“æ„
        assert!(request.body.get("contents").is_some());
        let contents = request.body.get("contents").unwrap().as_array().unwrap();
        assert_eq!(contents.len(), 1);

        let parts = contents[0].get("parts").unwrap().as_array().unwrap();
        assert_eq!(parts.len(), 2); // textå’Œimage
        assert!(parts[0].get("text").is_some());
        assert!(parts[1].get("inlineData").is_some());
    }

    #[test]
    fn test_build_gemini_request_stream() {
        let openai_body = json!({
            "model": "gpt-4",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello"}
            ],
            "stream": true
        });

        let result = build_gemini_request(
            "https://generativelanguage.googleapis.com",
            "test-api-key",
            "gemini-pro",
            &openai_body,
        );

        assert!(result.is_ok());
        let request = result.unwrap();

        // éªŒè¯æµå¼URL
        assert!(request.url.contains(":streamGenerateContent?"));

        // éªŒè¯system_instruction
        assert!(request.body.get("systemInstruction").is_some());
    }

    #[test]
    fn test_parse_gemini_stream_line_content() {
        let line = r#"data: {"candidates":[{"content":{"parts":[{"text":"Hello"}]}}]}"#;
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);

        assert_eq!(events.len(), 1);
        match &events[0] {
            StreamEvent::ContentChunk(text) => assert_eq!(text, "Hello"),
            _ => panic!("Expected ContentChunk"),
        }
    }

    #[test]
    fn test_parse_gemini_stream_line_done() {
        let line = "data: [DONE]";
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);

        assert_eq!(events.len(), 1);
        assert!(matches!(events[0], StreamEvent::Done));
    }

    #[test]
    fn test_parse_gemini_stream_line_function_call() {
        let line = r#"data: {"candidates":[{"content":{"parts":[{"functionCall":{"name":"get_weather","args":{"location":"Tokyo"}}}]}}]}"#;
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);

        assert_eq!(events.len(), 1);
        match &events[0] {
            StreamEvent::ToolCall(value) => {
                assert_eq!(value.get("type").unwrap(), "function");
                assert_eq!(
                    value.get("function").unwrap().get("name").unwrap(),
                    "get_weather"
                );
            }
            _ => panic!("Expected ToolCall"),
        }
    }

    #[test]
    fn test_parse_gemini_stream_line_usage() {
        let line = r#"data: {"usageMetadata":{"promptTokenCount":10,"candidatesTokenCount":20,"totalTokenCount":30}}"#;
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);

        assert_eq!(events.len(), 1);
        match &events[0] {
            StreamEvent::Usage(value) => {
                assert_eq!(value.get("promptTokenCount").unwrap(), 10);
                assert_eq!(value.get("candidatesTokenCount").unwrap(), 20);
            }
            _ => panic!("Expected Usage"),
        }
    }

    #[test]
    fn test_convert_gemini_nonstream_response() {
        let gemini_response = json!({
            "candidates": [{
                "content": {
                    "parts": [
                        {"text": "Hello"},
                        {"text": " world!"}
                    ]
                },
                "finishReason": "STOP",
                "index": 0
            }],
            "usageMetadata": {
                "promptTokenCount": 5,
                "candidatesTokenCount": 2,
                "totalTokenCount": 7
            }
        });

        let result = convert_gemini_nonstream_response_to_openai(&gemini_response, "gemini-pro");
        assert!(result.is_ok());

        let openai_response = result.unwrap();

        // éªŒè¯choices
        let choices = openai_response.get("choices").unwrap().as_array().unwrap();
        assert_eq!(choices.len(), 1);

        // éªŒè¯åˆå¹¶çš„æ–‡æœ¬
        let message = &choices[0].get("message").unwrap();
        let content = message.get("content").unwrap().as_str().unwrap();
        assert_eq!(content, "Hello\n world!");

        // éªŒè¯usageï¼ˆå¥å£®æ€§æµ‹è¯•ï¼‰
        let usage = openai_response.get("usage").unwrap();
        assert_eq!(usage.get("prompt_tokens").unwrap(), 5);
        assert_eq!(usage.get("completion_tokens").unwrap(), 2);
        assert_eq!(usage.get("total_tokens").unwrap(), 7);
    }

    #[test]
    fn test_convert_with_tools() {
        let openai_body = json!({
            "model": "gpt-4",
            "messages": [{"role": "user", "content": "What's the weather?"}],
            "tools": [{
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather information",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string"}
                        }
                    }
                }
            }]
        });

        let result = build_gemini_request(
            "https://generativelanguage.googleapis.com",
            "test-api-key",
            "gemini-pro",
            &openai_body,
        );

        assert!(result.is_ok());
        let request = result.unwrap();

        // éªŒè¯toolsè½¬æ¢
        assert!(request.body.get("tools").is_some());
        let tools = request.body.get("tools").unwrap().as_array().unwrap();
        assert_eq!(tools.len(), 1);

        let declarations = tools[0]
            .get("functionDeclarations")
            .unwrap()
            .as_array()
            .unwrap();
        assert_eq!(declarations.len(), 1);
        assert_eq!(declarations[0].get("name").unwrap(), "get_weather");
    }

    #[test]
    fn test_error_handling_invalid_json() {
        let invalid_body = json!({"invalid": "structure"});

        let result = build_gemini_request(
            "https://generativelanguage.googleapis.com",
            "test-api-key",
            "gemini-pro",
            &invalid_body,
        );

        assert!(result.is_err());
        match result.unwrap_err() {
            AdapterError::SerializationError(msg) => {
                assert!(msg.contains("Failed to parse OpenAI request"));
            }
            _ => panic!("Expected SerializationError"),
        }
    }

    #[test]
    fn test_parse_invalid_stream_line() {
        let line = "not a data line";
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);
        assert_eq!(events.len(), 0);

        let line = "data: not json";
        let state = Arc::new(Mutex::new(HashMap::new()));
        let events = parse_gemini_stream_line(line, &state);
        assert_eq!(events.len(), 0);
    }

    #[test]
    fn test_usage_robustness() {
        // æµ‹è¯•ç¼ºå¤±usageMetadataçš„æƒ…å†µ
        let gemini_response_no_usage = json!({
            "candidates": [{
                "content": {
                    "parts": [{"text": "Hello"}]
                },
                "index": 0
            }]
        });

        let result =
            convert_gemini_nonstream_response_to_openai(&gemini_response_no_usage, "gemini-pro");
        assert!(result.is_ok());

        let openai_response = result.unwrap();
        let usage = openai_response.get("usage").unwrap();
        assert_eq!(usage.get("prompt_tokens").unwrap(), 0);
        assert_eq!(usage.get("completion_tokens").unwrap(), 0);
        assert_eq!(usage.get("total_tokens").unwrap(), 0);

        // æµ‹è¯•éƒ¨åˆ†ç¼ºå¤±å­—æ®µçš„æƒ…å†µ
        let gemini_response_partial_usage = json!({
            "candidates": [{
                "content": {
                    "parts": [{"text": "Hello"}]
                },
                "index": 0
            }],
            "usageMetadata": {
                "promptTokenCount": 10
                // ç¼ºå¤±candidatesTokenCountå’ŒtotalTokenCount
            }
        });

        let result2 = convert_gemini_nonstream_response_to_openai(
            &gemini_response_partial_usage,
            "gemini-pro",
        );
        assert!(result2.is_ok());

        let openai_response2 = result2.unwrap();
        let usage2 = openai_response2.get("usage").unwrap();
        assert_eq!(usage2.get("prompt_tokens").unwrap(), 10);
        assert_eq!(usage2.get("completion_tokens").unwrap(), 0);
        assert_eq!(usage2.get("total_tokens").unwrap(), 10);
    }

    #[test]
    fn test_tool_response_mapping() {
        // æµ‹è¯•å·¥å…·å“åº”æ¶ˆæ¯çš„æ­£ç¡®æ˜ å°„
        let openai_body = json!({
            "model": "gpt-4",
            "messages": [
                {"role": "user", "content": "What's the weather like?"},
                {"role": "assistant", "tool_calls": [
                    {
                        "id": "call_123",
                        "type": "function",
                        "function": {"name": "get_weather", "arguments": "{\"location\": \"Tokyo\"}"}
                    }
                ]},
                {"role": "tool", "tool_call_id": "call_123", "name": "get_weather", "content": "{\"temperature\": 25, \"condition\": \"sunny\"}"}
            ]
        });

        let result = build_gemini_request(
            "https://generativelanguage.googleapis.com",
            "test-api-key",
            "gemini-pro",
            &openai_body,
        );

        assert!(result.is_ok());
        let request = result.unwrap();
        let contents = request.body.get("contents").unwrap().as_array().unwrap();

        // åº”è¯¥æœ‰3ä¸ªcontentï¼šuser + assistant (functionCall) + user (functionResponse)
        assert_eq!(contents.len(), 3);

        // éªŒè¯ç¬¬ä¸€ä¸ªæ˜¯ç”¨æˆ·æ¶ˆæ¯
        assert_eq!(contents[0].get("role").unwrap(), "user");

        // éªŒè¯ç¬¬äºŒä¸ªæ˜¯åŠ©æ‰‹çš„å‡½æ•°è°ƒç”¨
        assert_eq!(contents[1].get("role").unwrap(), "model");
        let parts1 = contents[1].get("parts").unwrap().as_array().unwrap();
        assert!(parts1[0].get("functionCall").is_some());

        // éªŒè¯ç¬¬ä¸‰ä¸ªæ˜¯å·¥å…·å“åº”ï¼Œè§’è‰²åº”è¯¥æ˜¯"user"è€Œä¸æ˜¯"model"
        assert_eq!(contents[2].get("role").unwrap(), "user");
        let parts2 = contents[2].get("parts").unwrap().as_array().unwrap();
        assert!(parts2[0].get("functionResponse").is_some());

        // éªŒè¯functionResponseçš„å†…å®¹
        let function_response = parts2[0].get("functionResponse").unwrap();
        assert_eq!(function_response.get("name").unwrap(), "get_weather");
        assert!(function_response.get("response").is_some());
    }

    #[test]
    fn test_multiple_content_parts() {
        let gemini_response = json!({
            "candidates": [{
                "content": {
                    "parts": [
                        {"text": "Part 1"},
                        {"text": "Part 2"},
                        {"functionCall": {"name": "func", "args": {"key": "value"}}}
                    ]
                },
                "index": 0
            }]
        });

        let result = convert_gemini_nonstream_response_to_openai(&gemini_response, "gemini-pro");
        assert!(result.is_ok());

        let openai_response = result.unwrap();
        let choices = openai_response.get("choices").unwrap().as_array().unwrap();
        let message = &choices[0].get("message").unwrap();

        // éªŒè¯æ–‡æœ¬åˆå¹¶
        let content = message.get("content").unwrap().as_str().unwrap();
        assert_eq!(content, "Part 1\nPart 2");

        // éªŒè¯tool_calls
        assert!(message.get("tool_calls").is_some());
        let tool_calls = message.get("tool_calls").unwrap().as_array().unwrap();
        assert_eq!(tool_calls.len(), 1);
    }

    #[test]
    fn test_topk_injection_from_openai_body() {
        // é¡¶å±‚ top_k æ‰©å±•åº”æ³¨å…¥åˆ° generationConfig.topK
        let openai_body = json!({
            "model": "gemini-2.5-pro",
            "messages": [ {"role": "user", "content": "hi"} ],
            "top_k": 33
        });

        let req = build_gemini_request_with_version(
            "https://generativelanguage.googleapis.com",
            "k",
            "gemini-2.5-pro",
            &openai_body,
            Some("v1"),
        )
        .expect("req");

        let gen = req.body.get("generationConfig").unwrap();
        assert_eq!(gen.get("topK").and_then(|v| v.as_i64()), Some(33));
    }

    #[test]
    fn test_thinking_forces_v1beta_when_no_version_specified() {
        // å½“å¯ç”¨ include_thoughts ä¸”æœªæŒ‡å®šç‰ˆæœ¬æ—¶ï¼Œåº”å¼ºåˆ¶ä½¿ç”¨ v1beta
        let openai_body = json!({
            "model": "gemini-2.5-flash",
            "messages": [ {"role": "user", "content": "hi"} ],
            "google_thinking_config": {"include_thoughts": true}
        });

        let req = build_gemini_request_with_version(
            "https://generativelanguage.googleapis.com",
            "k",
            "gemini-2.5-flash",
            &openai_body,
            None,
        )
        .expect("req");

        assert!(req.url.contains("/v1beta/"));
    }
}

// ==================== Cargo.toml æœ€å°ä¾èµ– ====================
/*
[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "1.0"
uuid = { version = "1.6", features = ["v4"] }

[dev-dependencies]
# æµ‹è¯•æ—¶å¯é€‰
*/
